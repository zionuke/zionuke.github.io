<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux定制文件系统(二)]]></title>
    <url>%2F2019%2F09%2F09%2FLinux%E5%AE%9A%E5%88%B6%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[Linux定制文件系统(二)这是种子班Linux初步第二个实验，具体要求如下 完成v0.55版本，可以从你自己的文件系统看到硬盘和完整文件系统，记得做记录，因为都是在内存中操作，关机就没有了。要能使用cp,ls,cat,ps,grep,mount等指令 实验环境 VMware虚拟机 Centos6.5 minimal版本镜像 实验具体流程1.在实验一的基础上，先把需要的命令和依赖全部cp到自己的文件系统内，我写了一个脚本，可以任意输入要cp的命令，记得填绝对地址.bash自带cd,要添加的命令有bash,insmod,ls,mkdir,mknod,mount,rm,cat 1234567#!/bin/bashINS=$1J=lib64cp $&#123;INS&#125; binlist="$(ldd $&#123;INS&#125; | egrep -o '/lib.*\.[0-9]')"for i in $list;do cp -v "$i" "$&#123;J&#125;";done#这个脚本可以把任意命令cp到bin,且其依赖放到lib64 2.之后就是要在test文件夹内创建dev,module,proc,sysfs,etc文件夹 3.修改init脚本内容如下 12345678910111213141516171819202122232425#!/bin/bashexport PATH=/sbin:/bin:/usr/bin:/usr/sbin:$PATHinsmod /module/scsi_transport_spi.koinsmod /module/mptbase.koinsmod /module/mptscsih.koinsmod /module/mptspi.koinsmod /module/crc-t10dif.koinsmod /module/sd_mod.koinsmod /module/mbcache.koinsmod /module/jbd2.koinsmod /module/ext4.komount -t proc proc /proc/mount -t sysfs sysfs /sysfs/mkdir /root#这里要注意以下，用df -h看一下你的根目录"/"在哪个sda下，就把x换为几，比如发现在sda3,则所有x替换为3即可mknod /dev/sdax b 8 xmount -t ext4 /dev/sdax /root/./bin/bash 4.之后照例打包整个test文件夹,会自动覆盖同名文件 1find . | cpio -H newc -o | gzip &gt; /boot/initrd.img 5.然后重启选择自己的文件系统，进入bash后直接cd进root看看是不是自己的完整文件系统，我的如下，这样即说明成功]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux定制文件系统(一)]]></title>
    <url>%2F2019%2F09%2F08%2FLinux%E5%AE%9A%E5%88%B6%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[Linux定制文件系统(一)这是种子班Linux初步第一个实验，具体要求如下 利用initrd.img机制，建立一个简单文件系 统（v0.5版本），使得内核用该文件系统启动后可以直接获得一个shell 在grub启动配置文件当中增加一个入口用 于测试新建的initrd.img 整个文件系统在启动后运行在内存中，不 需要调用硬盘资源。 实验环境 VMware虚拟机 Centos6.5 minimal版本镜像 实验技巧 在虚拟机中不能copy,推荐用putty或powershell，cmder等工具远程连接后操作，更方便，putty填入虚拟机ip地址即可使用，powershell，cmder等用ssh远程登陆，输入密码后即可正常使用 123ssh root@your ip address#ip地址可用ifconfig命令查看ifconfig 实验具体流程 首先创建一个文件夹作为虚拟的根文件系统,并在这个文件夹中创建一个bin文件夹用于存放/bin/bash相关文件，一个文件夹lib64用于存放bash的依赖文件 1234mkdir testcd testmkdir binmkdir lib64 将/bin/bash内容拷贝至bin文件夹，将其依赖拷贝至lib64文件夹,ldd命令可用于查询依赖 12345678910111213cp /bin/bash binldd /bin/bash#查询依赖所得结果如下linux-vdso.so.1 =&gt; (0x00007fff611ff000) libtinfo.so.5 =&gt; /lib64/libtinfo.so.5 (0x00007f6431a56000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f6431852000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f64314bd000) /lib64/ld-linux-x86-64.so.2 (0x00007f6431c7c000)#之后依次将依赖拷贝至lib64文件夹(第一个应该是内核内部的，不用拷贝)cp /lib64/libtinfo.so.5 lib64cp /lib64/libdl.so.2 lib64cp /lib64/libc.so.6 lib64cp /lib64/ld-linux-x86-64.so.2 lib64 之后创建一个init文件,并将其中内容修改为/bin/bash,之后将其权限改为可执行 12345vi initchmod +x init#修改完init之后用chroo命令辅助验证新环境下是否完备，即依赖是否齐全chroot ./#若执行该命令之后进入一个新的终端且ctrl+z退不出去，说明没问题 打包第一步创建的整个test文件夹，作为Kernel的临时根文件系统 1find . | cpio -H newc -o | gzip &gt; /boot/initrd.img 之后修改grub.conf内容 123456789101112131415161718192021222324252627cd /boot/grubvi grub.conf#打开之后可得内容如下# grub.conf generated by anaconda## Note that you do not have to rerun grub after making changes to this file# NOTICE: You have a /boot partition. This means that# all kernel and initrd paths are relative to /boot/, eg.# root (hd0,0)# kernel /vmlinuz-version ro root=/dev/sda5# initrd /initrd-[generic-]version.img#boot=/dev/sdadefault=0timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (2.6.32-431.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-431.el6.x86_64 ro root=UUID=f1a163bd-6d42-4438-8498-1e7c1d057739 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet initrd /initramfs-2.6.32-431.el6.x86_64.img #在最后一行添加如下内容title v0.5 #v0.5为启动项选择的名字 root (hd0,0) kernel /vmlinuz-2.6.32-431.el6.x86_64 ro root=UUID=f1a163bd-6d42-4438-8498-1e7c1d057739 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet initrd /initrd.img #这里注意不要写绝对路径/boot/initrd.img,因为加载启动项之后是默认以/boot为根目录/,所以只需 要填/initrd.img 修改完grub.conf之后保存退出，reboot重启，开机启动项选v0.5即可得到只有一个shell的终端tty]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux启动过程]]></title>
    <url>%2F2019%2F09%2F04%2FLinux%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Linux启动过程17级种子班Linux初步课程笔记 大体启动流程 具体流程MBR引导加载(Boot Loader) boot loader将控制权交给对应操作系统的loader，让它负责去启动操作系统(OS Loader) grub(Linux的Loader使用的就是grub) Linux所在分区的boot sector就是存放着stage1文件的内容，同时默认Linux启动的话，也需要把stage1中的引导代码安装到MBR中的boot loader中 stage1完成了主程序的引导后，主引导程序开始加载配置文件，但是加载这些配置文件之前需要有文件系统的支持，GRUB的内置文件系统其实是依靠stage1_5那些文件定义的，而且有不同文件系统的stage1_5(/boot/grub) 而后开始读取stage2开始真正地读取配置文件grub.conf,解析/boot/grub/grub.conf文件 加载系统内核kernel MBR将内核文件（代码）载入物理内存中执行，内核就是/boot/vmlinuz.x86_64，观察该文件，发现这是一个压缩镜像文件,控制权转交给内核后，内核重新检测各种硬件信息 这时候内核还没有文件系统的概念，没有文件系统就没办法挂载根目录，想要挂载根目录就需要相应的模块支持，内核需要文件系统来加载提供这些程序功能的模块(鸡与蛋的问题) 所以先采用载入临时根文件系统(/boot/initramfs.img)把其解压成根目录，然后内核就可以在这个虚拟的根文件系统上加载驱动程序，之后释放根文件系统，最后开始正常的启动过程。 initramfs.img解压 1234567891011# file initramfs-2.6.32-431.el6.x86_64.img //检查initramfs文件类型//initramfs-2.6.32-431.el6.x86_64.img: gzip compressed data, from Unix, last modified: Wed Sep 4 01:05:16 2019, max compression//所以需要先用gzip解压# mv initramfs-2.6.32-431.el6.x86_64.img initramfs-2.6.32-431.el6.x86_64.img.gz //gzip解压文件必须以.gz后缀# gzip -d initramfs-2.6.32-431.el6.x86_64.img.gz# file initramfs-2.6.32-431.el6.x86_64.img //再次检查文件类型//initramfs-2.6.32-431.el6.x86_64.img: ASCII cpio archive (SVR4 with no CRC)//查看需要借助cpio命令# mkdir init# cd init# cpio -id &lt; initramfs-2.6.32-431.el6.x86_64.img//解压到init目录下 解压之后的内容类似于真正/目录下内容，这是因为这是一个最小化的Linux根文件系统。内核就是先把这个文件展开，形成一个虚拟文件系统(在/目录下)，内核借虚拟文件系统装载必要的模块，完成后释放该虚拟文件系统并挂载真正的根目录 启动init 内核完成硬件检测和加载模块后，内核会呼叫第一个进程，就是/sbin/init，至此内核把控制权交给init进程 初始化配置文件/etc/inittab决定操作系统的runlevel 系统初始化脚本/etc/rc.d/rc.sysinit设置主机名，挂载/etc/fstab中的文件系统，修改/etc/sysctl.conf 的内核参数等各项系统环境 开机启动信息存放在/var/log/dmesg中 /etc/rc.d/rc脚本 总结一下启动流程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux_day1]]></title>
    <url>%2F2019%2F09%2F02%2FLinux_day1%2F</url>
    <content type="text"><![CDATA[Linux初步这是种子班Linux初步课程第一天的课程，主要内容如下 1.程序要能运行是要经过编译和链接两步的，这样做是很明智的，可以避免重复劳动。 2.要学会偷懒，比如编写makefile，以及tab,和利用cltr+r可以查找之前用过的命令 3.要搞清楚VMW的3种网络模型 bridge 1.实验时间(1/4) 目标：在VMWare当中安装Linux操作系统 缺省字体建议用英文(否则崩溃之后调试英文环境中文全乱码) 使用ssh客户端(putty)完成Linux远程登录 了解并熟悉Linux基本命令行操作命令 检视如下内容：1.OS Loader位置、配置2.内核vmlinuz的位置3.应用程序管理器init的位置、配置 实验时间(2/4)• 常用Linux命令行操作命令 – 文件文本进程：ls：列出目录内容（List Directory Contents）的意思。运行它就是列出文件夹里的内容，可能是文件也可能是文件夹。 ls -a:隐藏文件也列出 ls - l:列出文件属性能详细信息，ll等效 cat：打印文件内容 cp：主要用于复制文件或目录 rm：用于删除一个文件或者目录 ps：显示当前进程 (process) 的状态，ps -A列出所有进程 grep：用于查找文件里符合条件的字符串,grep 指令用于查找内容包含指定的范本样式的文件 mkdir:建立名称为 dirName 之子目录 mv:用来为文件或目录改名、或将文件或目录移入其它位置。 mv 文件名 文件名:改名 mv 文件名 目录名:移动文件 less:less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 vi:用vim打开文件,一开始默认为命令模式。按i变为输入模式，按Esc退出输入模式，输入模式下按:进入底线命令模式，此模式下输入wq即可保存退出 cpio:用于备份文件。 tar :用来压缩和解压文件。tar本身不具有压缩功能。他是调用压缩功能实现的 – 网络：ifconfig：用于显示或设置网络设备 ip：用来配置网卡ip信息的命令，且是未来的趋势，重启网卡后IP失效 ssh： 1.SSH是安全的加密协议，用于远程连接Linux服务器 2.SSH的默认端口是22，安全协议版本是SSH2 3.SSH服务器端主要包含2个服务功能SSH连接和SFTP服务器 4.SSH客户端包含ssh连接命令和远程拷贝scp命令等 telnet：用于远端登入，执行telnet指令开启终端机阶段作业，并登入远端主机。 ftp ：连接ftp服务器 – 关机启动： reboot：用于用来重新启动计算机。 shutdown：可以用来进行关机程序，并且在关机以前传送讯息给所有使用者正在执行的程序，shutdown 也可以用来重开机。但使用权限是系统管理者 init： init 0 就是关机init 3 就是切换到多用户-命令行模式init 5 就是切换到图形化界面init 6 就是重启 – 帮助：man – 编程： gcc：gcc编译的四个步骤 预处理：gcc -E Test.c -o Test.i 编译： gcc -S Test.i -o Test.s 汇编： gcc -c Test.s -o Test.o 链接生成可执行文件： gcc Test.o -o Test make：这个详见自己笔记 gdb ：能熟练用这个之后不需要依赖IDE了 – 软件安装升级： yum：基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 课程内容（3/4）• 从源码开始安装软件 – 准备工作：必备的工具包 • gcc、make、automake、autoconf…… – 获得源码包并展开 • wget、ftp、mount等命令获得源码包 • tar、bzip2、gunzip等命令展开源码 – 配置、编译软件 • configuration、make – 安装软件 • make install、make strip_install 课程内容（4/4）• 配置、编译软件 – 从hello.c说开去 • gcc用法 • make用法和依赖关系 – 自动配置脚本 ./configure • 生成Makefile – make • 不同的软件包可能有不同的目标方式 目标：在VMWare当中安装Linux操作系统 缺省字体建议用英文(否则若崩溃之后调试全乱码) 使用ssh客户端(putty)完成Linux远程登录– 了解并熟悉Linux基本命令行操作命令– 检视如下内容：• OS Loader位置、配置• 内核vmlinuz的位置• 应用程序管理器init的位置、配置]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[添加百度统计，站点运行时间统计和搜索功能------系列第七篇]]></title>
    <url>%2F2019%2F08%2F10%2Fblog7%2F</url>
    <content type="text"><![CDATA[1.添加百度统计1、注册百度统计账号，https://tongji.baidu.com/web/welcome/login2、配置参数 A、获取baidu_analytics的key B、在themes/next/_confing.xml中设置key 12#Baidu Analytics IDbaidu_analytics: 填写代码段中hmjs?后的那一段key C、检测是否成功 2.增加站点运行时间统计1.首先按类似地址”E:\blog\themes\next\layout_partials\footer.swig”找到这个文件，在里面添加如下代码即可 12345678910111213141516171819&lt;!-- 网站运行时间的设置 --&gt;&lt;span id="timeDate"&gt;载入天数...&lt;/span&gt;&lt;span id="times"&gt;载入时分秒...&lt;/span&gt; &lt;script&gt; var now = new Date(); function createtime() &#123; var grt= new Date("03/09/2019 13:14:21");//此处修改你的建站时间或者网站上线时间 now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 )&#123;hnum = "0" + hnum;&#125; minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 )&#123;mnum = "0" + mnum;&#125; seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 )&#123;snum = "0" + snum;&#125; document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; &#125;setInterval("createtime()",250);&lt;/script&gt; 3.添加搜索功能1.在hexo的根目录下执行命令： 1npm install hexo-generator-searchdb --save 2.在blog下的_config.yml文件中添加配置(注意是站点配置文件不是主题配置文件)： 12345search: path: search.xml field: post format: html limit: 10000 3.在根目录下的/theme/next/_config.yml文件中搜索local_search，将enable改为true： 12local_search: enable: true 配置完搜索之后可能会出现点击搜索一直loading转圈的问题 解决方法如下: 主要看看跳转后的url是什么，如果url异常，就需要在站点配置文件（注意不是主题配置文件）下面看看你的url和永久链接设置的是否正确。如下所示： 12345678#URL##If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://dragon-liu.github.ioroot: //permalink: z_post/:title/ #这里一定要注意，z_post的前面绝对不能加 / ，否则就会挑战异常permalink_defaults: 一直处于加载状态首先看是不是有哪些文件的title命名方式有问题, 已知的问题有: 不能有包含半角的冒号(全角可以), 这个可以用hexo g得知, 如果有问题, 则hexo g会报错(大概率是命名规范问题) 方法一: hexo clean + hexo g 重新生成search.xml文件 方法二: 更新searchdb插件: npm install hexo-generator-searchdb —save 方法三: 文章太多时, 尝试将_comfig.yml中Local Search下的limits设置的更高一些]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github在git push之后不记录Contributions]]></title>
    <url>%2F2019%2F08%2F09%2Fgithub%E5%9C%A8git%20push%E4%B9%8B%E5%90%8E%E4%B8%8D%E8%AE%B0%E5%BD%95Contributions%2F</url>
    <content type="text"><![CDATA[github在git push之后不记录Contributions​ 在开始使用github之后一直很疑惑，为什么push之后Contributions一直没有相应更新，明明在github上Repositories都有了相应更新，查了很久才发现，是我自己git的email设置有问题。 ​ 首先要清楚一点，你push的代码要在contributions上有有显示，有关邮箱有一点很重要 你的github账户要添加与你git config相同的邮箱才行，也就是说你git config设置的邮箱必须add到你github的邮箱中才行 其余要求见github官方文档： https://help.github.com/en/articles/why-are-my-contributions-not-showing-up-on-my-profile ​ 我就是因为本地git设置瞎写的，我的config里面email设置是dragon-liu,甚至都不是一个正确的邮箱，如果这时候你修改了config为正确邮箱，之前提交的还是不会显示，那么怎么补救呢？ ​ Github官方同样给出了解决方案： https://help.github.com/en/articles/changing-author-info ​ 如果你看不懂英文，具体步骤不是很清楚，我已经把这些写成了一个脚本，你只需要做相应替换即可 123456789101112131415161718192021222324252627#!/bin/shgit clone --bare https://github.com/user/repo.gitcd repo.gitgit filter-branch --env-filter 'OLD_EMAIL="your-old-email@example.com"CORRECT_NAME="Your Correct Name"CORRECT_EMAIL="your-correct-email@example.com"if [ "$GIT_COMMITTER_EMAIL" = "$OLD_EMAIL" ]then export GIT_COMMITTER_NAME="$CORRECT_NAME" export GIT_COMMITTER_EMAIL="$CORRECT_EMAIL"fiif [ "$GIT_AUTHOR_EMAIL" = "$OLD_EMAIL" ]then export GIT_AUTHOR_NAME="$CORRECT_NAME" export GIT_AUTHOR_EMAIL="$CORRECT_EMAIL"fi' --tag-name-filter cat -- --branches --tagsgit push --force --tags origin 'refs/heads/*'cd ..rm -rf repo.git 具体解释以下替换什么和怎么用 1.首先就是你随便找一个你想运行这个脚本的地方，桌面也行。创建一个文本文件，把这一段代码粘贴到里面去，并保存把后缀改为sh即可。 2.怎么替换呢，首先要做如下替换(这个替换做一次即可) your-old-email@example.com改为你的老的不能用的邮箱，比如我的老的不能用的dragon-liu Your Correct Name替换为你之后本地git config里面想用的username即可 your-correct-email@example.com替换为你之后本地git config里面想用的email 之后就是你每次修改一个Repositories都要做的修改如下 1234561.git clone --bare https://github.com/user/repo.git中的Repositories地址更换为你想修改的那个2.cd repo.gitrm -rf repo.git上面的repo.git更改为https://github.com/user/repo.git地址最后你要修改的.git即可 ​ 在所有都更改完之后，你再更改git全局config,运行如下代码 12git config --global user.name "your-username"git config --global user.email "your-correct-email@example.com" 这样的话，之后即可正常commit和push,在github上也会有contributions更新]]></content>
      <categories>
        <category>错误总结</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nrmake,makefile,autorun如何配合以简化编译运行]]></title>
    <url>%2F2019%2F08%2F08%2Fnrmake%2Cmakefile%2Cautorun%E5%A6%82%E4%BD%95%E9%85%8D%E5%90%88%E4%BB%A5%E7%AE%80%E5%8C%96%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[nrmake,makefile,autorun如何配合以简化编译运行首先要搞清楚，nrmake和autorun都是shell脚本 1.如何配合​ 通常是通过一个nrmake脚本(名字随便取)比如allmake.sh来通过传入参数控制如何make,因为可以在make中定义一些宏和变量来配合allmake.sh达到不同的make效果，这样省去了调用make来编译的过程，之后运行一般是通过一个makefile脚本autorun.sh来运行，在这个脚本中向main函数传递参数。 首先来看一个nrmake脚本allmake.sh 12345678910#! /bin/bashif [ $# -eq 0 ]then make dos2unix autorun.sh #在windows下写的，直接在Linux环境下运行会报错syntax error:unexpected end of fileelse make clean make dos2unix autorun.shfi 上面的代码会判断输入参数个数，$#代表输入参数个数，你在命令行输入几个参数这个就是几。所以上面的代码会判断，如果输入参数为0，即没有输入参数，就默认make;而若不是0，则先清理掉之前make生成的目标文件和可执行文件。 再来看看makefile 123456789101112objects = main.o thread.occ = gccedit : $(objects) cc -o edit $(objects) -lpthreadmain.o: main.c thread.h cc -c main.cthread.o: thread.c thread.h cc -c thread.c.PHONY : cleanclean: rm edit $(objects) 我这里没有体现出allmake.sh传给makefile的参数，具体参看项目代码的nrmake和makefile怎么配合的 最后看一下autorun.sh 123456789#!/bin/bashif test $# -ne 2then echo "Invalid parameter, the parameters are end,num_of_thread." exitfi./edit $1 $2 上面的代码会判断输入参数，不是2个就报错不执行，否则会执行可执行文件edit,向其传入两个参数， $1 $2表示传入的第一个和第二个参数(不包括文件名) 2.编译+运行12sh allmake.shsh autorun.sh 100 10]]></content>
      <categories>
        <category>编码心得</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell常用基本知识]]></title>
    <url>%2F2019%2F08%2F08%2FShell%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Shell1.基本概念​ Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。 ​ Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。 ​ 打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了。 12#！/bin/bashecho "Hello world!" #! 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。 echo 命令用于向窗口输出文本。 2.如何运行shell脚本1.作为可执行程序将上面的代码保存为 test.sh，并 cd 到相应目录： 12chmod +x ./test.sh #使脚本具有执行权限./test.sh #执行脚本 注意，一定要写成 ./test.sh，而不是 test.sh，运行其它二进制的程序也一样，直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。 2.作为解释器参数这种运行方式是，直接运行解释器，其参数就是 shell 脚本的文件名，如： 12/bin/sh test.sh/bin/php test.php 这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。 3.Shell变量定义变量时，变量名不加美元符号（$，PHP语言中变量需要），如： 1a="nb" #注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。 使用变量使用一个定义过的变量，只要在变量名前面加美元符号即可，如： 123your_name="nb"echo $your_nameecho $&#123;your_name&#125; 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况： 123for skill in Ada Coffe Action Java; do echo "I am good at $&#123;skill&#125;Script"done 如果不给skill变量加花括号，写成echo “I am good at $skillScript”，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。 推荐给所有变量加上花括号，这是个好的编程习惯。 已定义的变量，可以被重新定义，如： 1234your_name="tom"echo $your_nameyour_name="alibaba"echo $your_name 这样写是合法的，但注意，第二次赋值的时候不能写$your_name=”alibaba”，使用变量的时候才加美元符（$）。 Shell 字符串字符串可以用单引号，也可以用双引号，也可以不用引号。 单引号1str=&apos;this is a string&apos; 单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。 双引号123your_name=&apos;runoob&apos;str=&quot;Hello, I know you are \&quot;$your_name\&quot;! \n&quot;echo -e $str 输出结果为： 1Hello, I know you are &quot;runoob&quot;! 双引号的优点： 双引号里可以有变量 双引号里可以出现转义字符 4.Shell传递参数我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：$n。n 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推…… 实例以下实例我们向脚本传递三个参数，并分别输出，其中 $0 为执行的文件名： 1234567#!/bin/bashecho "Shell 传递参数实例！";echo "执行的文件名：$0";echo "第一个参数为：$1";echo "第二个参数为：$2";echo "第三个参数为：$3"; 为脚本设置可执行权限，并执行脚本，输出结果如下所示： 1234567$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！执行的文件名：./test.sh第一个参数为：1第二个参数为：2第三个参数为：3 要注意$#这个参数表示传递到脚本的参数个数(不包括文件名) 5.Shell test命令Shell中的 test 命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试。 数值测试 参数 说明 -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 实例演示： 12345678num1=100num2=100if test $[num1] -eq $[num2]then echo '两个数相等！'else echo '两个数不相等！'fi 输出结果： 1两个数相等！ 代码中的 [] 执行基本的算数运算，如： 1234567#!/bin/basha=5b=6result=$[a+b] # 注意等号两边不能有空格echo "result 为： $result" 结果为: 1result 为： 11 字符串测试 参数 说明 = 等于则为真 != 不相等则为真 -z 字符串 字符串的长度为零则为真 -n 字符串 字符串的长度不为零则为真 实例演示： 12345678num1="ru1noob"num2="runoob"if test $num1 = $num2then echo '两个字符串相等!'else echo '两个字符串不相等!'fi 输出结果： 1两个字符串不相等! 文件测试 参数 说明 -e 文件名 如果文件存在则为真 -r 文件名 如果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 实例演示： 1234567cd /binif test -e ./bashthen echo '文件已存在!'else echo '文件不存在!'fi 输出结果： 1文件已存在! 另外，Shell还提供了与( -a )、或( -o )、非( ! )三个逻辑操作符用于将测试条件连接起来，其优先级为：”!”最高，”-a”次之，”-o”最低。例如： 1234567cd /binif test -e ./notFile -o -e ./bashthen echo '至少有一个文件存在!'else echo '两个文件都不存在'fi 输出结果： 1至少有一个文件存在! 6.Shell流程控制和Java、PHP等语言不一样，sh的流程控制不可为空，如(以下为PHP流程控制写法)： 1234567&lt;?phpif (isset($_GET["q"])) &#123; search(q);&#125;else &#123; // 不做任何事情&#125; 在sh/bash里可不能这么写，如果else分支没有语句执行，就不要写这个else。 if elseifif 语句语法格式： 1234567if conditionthen command1 command2 ... commandN fi 写成一行（适用于终端命令提示符）： 1if [ $(ps -ef | grep -c &quot;ssh&quot;) -gt 1 ]; then echo &quot;true&quot;; fi 末尾的fi就是if倒过来拼写，后面还会遇到类似的。 if elseif else 语法格式： 123456789if conditionthen command1 command2 ... commandNelse commandfi if else-if elseif else-if else 语法格式： 123456789if condition1then command1elif condition2 then command2else commandNfi 以下实例判断两个变量是否相等： 1234567891011121314a=10b=20if [ $a == $b ]then echo "a 等于 b"elif [ $a -gt $b ]then echo "a 大于 b"elif [ $a -lt $b ]then echo "a 小于 b"else echo "没有符合的条件"fi 输出结果： 1a 小于 b if else语句经常与test命令结合使用，如下所示： 12345678num1=$[2*3]num2=$[1+5]if test $[num1] -eq $[num2]then echo '两个数字相等!'else echo '两个数字不相等!'fi 输出结果： 1两个数字相等! 7.Shell 输入/输出重定向大多数 UNIX 系统命令从你的终端接受输入并将所产生的输出发送回到您的终端。一个命令通常从一个叫标准输入的地方读取输入，默认情况下，这恰好是你的终端。同样，一个命令通常将其输出写入到标准输出，默认情况下，这也是你的终端。 重定向命令列表如下： 命令 说明 command &gt; file 将输出重定向到 file。 command &lt; file 将输入重定向到 file。 command &gt;&gt; file 将输出以追加的方式重定向到 file。 n &gt; file 将文件描述符为 n 的文件重定向到 file。 n &gt;&gt; file 将文件描述符为 n 的文件以追加的方式重定向到 file。 n &gt;&amp; m 将输出文件 m 和 n 合并。 n &lt;&amp; m 将输入文件 m 和 n 合并。 &lt;&lt; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SHELL脚本“SYNTAX ERROR:UNEXPECTED END OF FILE”解决方案]]></title>
    <url>%2F2019%2F08%2F08%2FSHELL%E8%84%9A%E6%9C%AC%E2%80%9CSYNTAX%20ERRORUNEXPECTED%20END%20OF%20FILE%E2%80%9D%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[SHELL脚本“SYNTAX ERROR:UNEXPECTED END OF FILE”解决方案1.问题今天在windows环境下写了一个shell脚本(用vscode)，结果发现直接传到Linux服务器上运行会报错如下 1syntax error:unexpected end of file 查了很久才明白，原因如下: ​ dos文件传输到unix系统时,会在每行的结尾多一个^M,在vim的时候,当你用如下命令查看：cat -A SCRIPT示例：[root@localhost~]#cat -A installhttp.sh 1234567#！/bin/bash^M$#The script is autoinstallhttp.sh^M$#Author Joah^M$#Version 1.0.0^M$version=`grep -o " [0-9]\&gt;" /etc/centos-release`^M$if [ $version -eq 7 ];do^M$echo "We will install newversion.Please wait!"^M$ 2、解决方法使用命令dos2unix命令将SCRIPT脚本格式 12[root@localhost~]# dos2unix allmake.sh dos2unix: converting file installhttp.sh to UNIX format ... 或者： 123[root@localhost/app]# vim allmake.sh :set fileformat=unix :wq 12345678910[root@localhost/app]#cat -A installhttp.sh#此时再查看结果如下$$#/bin/bash$#$#The script is autoinstallhttp.sh$#Author Joah$#Version 1.0.0$version=`grep -o " [0-9]\&gt;" /etc/centos-release$]]></content>
      <categories>
        <category>错误总结</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码心得(一)]]></title>
    <url>%2F2019%2F08%2F08%2F%E7%BC%96%E7%A0%81%E5%BF%83%E5%BE%97(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[编码心得(一)1.注意数组指针对其进行[]操作取得某个元素之后，得到的是整个数组，而不是像指针数组那样[]后得到的是一个指针，也就是说 123int (*a)[2];a[0] = (int*)malloc(2*sizeof(int));//这是一个典型错误，a[0]是int[2],而不是int* 2.想通过二重指针来创建二维动态数组，要注意，要动态分配2次空间，具体如下 1234567891011121314int **a;int i;for(i = 0; i &lt; 10 ;i++)&#123; a[i] = (int*)malloc(2*sizeof(int));//会报错，原因很简单，a还没分配空间，访问a[]有问题&#125;//正确操作如下a = (int**)malloc(2*sizeof(int*));for(i = 0; i &lt; 10 ;i++)&#123; a[i] = (int*)malloc(2*sizeof(int));//这里不会报错，因为a的空间已经分配了&#125; 3.强调几次，malloc之后一定要free 4.在父进程创建了多个子进程之后，要记得在子进程处理完了相应的任务之后exit(0),否则，因为你创建了很多进程，最后输出的时候会同时有多个输出，这个一定要注意。尤其是每个父子进程之间都开一个管道之后，子进程负责写入，父进程负责读，那么子进程记得exit(0). 5.注意画流程图时要尽量用自然语言或者伪代码，不要直接用代码中的函数接口，否则之后不好会议也不好懂，也只有开发人员能看懂 6.Linux下命令行编译c语言程序 12gcc -o name main.c #name是可执行文件的名字./name #这样来运行可执行程序 7.多线程的程序记得加锁，不然很容易输出混乱，这个要记得 8.关于程序的编译和连接 ​ 源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。你需要指定函数的ObjectFile. 9.c语言中要在一个源文件中引用另一个源文件中的变量时，只需在相应变量之前加extern即可 10.在一个大的软件工程里面，可能会有多个文件同时包含一个头文件，当这些文件编译链接成一个可执行文件时，就会出现大量重定义的错误。在头文件中实用#ifndef #define #endif能避免头文件的重定义。方法：例如要编写头文件test.h在头文件开头写上两行： 12#ifndef _TEST_H#define _TEST_H//一般是文件名的大写 头文件结尾写上一行：#endif这样一个工程文件里同时包含两个test.h时，就不会出现重定义的错误了。 头文件结尾写上一行：#endif这样一个工程文件里同时包含两个test.h时，就不会出现重定义的错误了。 分析：当第一次包含test.h时，由于没有定义_TEST_H，条件为真，这样就会包含（执行）#ifndef _TEST_H和#endif之间的代码，当第二次包含test.h时前面一次已经定义了_TEST_H，条件为假，#ifndef _TEST_H和#endif之间的代码也就不会再次被包含，这样就避免了重定义了。主要用于防止重复定义宏和重复包含头文件]]></content>
      <categories>
        <category>编码心得</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程知识点概要(三)]]></title>
    <url>%2F2019%2F08%2F07%2F%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[进程间通信(InterProcess Communication)1.管道​ 这是unix系统IPC的最古老形式，由以下2个局限性 它们是半双工的，即数据只能在一个方向上流动 管道只能在具有公共祖先的两个进程之间使用，比如父进程创建了一个管道，然后fork了一个子进程，则父子进程之间可以使用这个管道了(为什么子进程也会有一个管道呢，因为子进程是父进程的副本。会获得父进程数据空间，堆和栈的副本，但注意这两者并不是共用同一数据空间) 管道是通过调用pipe函数创建的 12#include&lt;unistd.h&gt;int pipe(int fd[2]); //成功返回0，出错返回-1 ​ 经由参数fd会返回2个文件描述符，fd[0]为读而打开，fd[1]为写而打开。fd[1]输出是fd[0]的输入。要注意通常是管道的写端写完了读端才能读。 ​ 单个进程中的管道几乎没有任何用处，进程会先调用pipe,接着调用fork,从而创建从父进程到子进程的IPC通道。 ​ Fork之后做什么取决于我们想要的数据流的方向。对于从父进程到子进程的管道，父进程关闭读端，子进程关闭写段。 ​ 当管道的一端被关闭后，下列两条规则会起作用 当读(read)一个写端已被关闭的管道时，在所有数据都被读取后，read返回0，表示文件结束。 如果写一个读端已被关闭的管道，则产生信号SIGPIPE 在写管道或(FIFO)时，常量PIPE_BUF规定了内核的管道缓冲区大小。若对管道调用write,且要求写的字节数小于PIPE_BUF，则此操作不会与其他进程对同一管道(或FIFO)的write操作交叉进行。否则可能会交叉进行。 ​ 下面看一个从父进程到子进程的管道(父进程向子进程传递数据) 12345678910111213141516171819202122232425262728#include&lt;stdio.h&gt;#include&lt;unistd.h&gt;int main()&#123; int fd[2]; // 两个文件描述符 pid_t pid; char buff[20]; if(pipe(fd) &lt; 0) // 创建管道 printf("Create Pipe Error!\n"); if((pid = fork()) &lt; 0) // 创建子进程 printf("Fork Error!\n"); else if(pid &gt; 0) // 父进程 &#123; close(fd[0]); // 关闭读端 write(fd[1], "hello world\n", 12); &#125; else &#123; close(fd[1]); // 关闭写端 read(fd[0], buff, 20); printf("%s", buff); &#125; return 0;&#125; 2.函数popen和pclose​ 函数popen和pclose实现的操作是:创建一个管道，fork一个子进程，关闭未使用的管道端，执行一个shell命令，然后等待命令终止。 123#include&lt;stdio.h&gt;FILE *popen(const char *cmdstring, const char *type);//成功返回文件指针，出错返回NULLint pclose(FILE *fp); //成功返回cmdstring的终止状态，出错返回-1 ​ 函数popen先执行fork,然后调用exec执行cmdstring，并且返回一个标准I/O文件指针.若type是”r”,则文件指针连接到cmdstring的标准输出，即返回的文件指针是可读的;若type是”w”,则文件指针连接到cmdstring的标准输入，即文件指针是可写的。 ​ pclose函数关闭标准I/O流，等待命令终止，然后返回shell的终止状态。 3.协同进程​ UNIX系统过滤程序从标准输入读取数据，向标准输出写数据。几个过滤程序通常在shell管道中线性连接，当一个过滤程序既产生某个过滤程序的输入，又读取该过滤程序的输出时，它就变成了协同进程。 ​ popen只提供连接到另一个进程的标准输入或标准输出的一个单向管道，而协同进程则有效连接到另一个进程的两个单向管道;一个连接到其标准输入，一个连接到其标准输出。我们想将数据写到其标准输入，经其处理后，再从其标准输出读取数据。 4.FIFO​ FIFO有时被称为命名管道，它是一种文件类型。未命名的管道只能在两个相关的进程之间使用，而且这两个相关的进程还要有一个共同的创建了它们的祖先进程。但通过FIFO，不相关的进程也能交换数据。 ​ 创建FIFO类似于创建文件，FIFO有路径名与之相关联，它以一种特殊设备文件形式存在于文件系统中。 1234#include &lt;sys/stat.h&gt;// 返回值：成功返回0，出错返回-1int mkfifo(const char* path, mode_t mode);int mkfifoat(int fd, const char *path, mode_t mode); ​ mkfifo函数中mode参数的规格说明与open函数中mode相同；mkfifoat函数与mkfifo函数类似，但mkfifoat函数可以被用来在fd文件描述符表示的目录相关的位置创建一个FIFO。这里有3种情形: 若path参数指定的是绝对路径名，则fd参数会被忽略掉，并且mkfifoat函数的行为与mkfifo类似 若path参数指定的是相对路径名，则fd参数是一个打开目录的有效文件描述符，路径名和目录有关 若path参数指定的是相对路径名，且fd参数有一个特殊值AT_FDCWD，则路径名以当前目录开始，mkfifoat与mkfifo类似。 当我们用mkfifohuomkfifoat创建FIFO时，要用open(文件I/O函数)打开它。 当 open 一个FIFO时，是否设置非阻塞标志（O_NONBLOCK）的区别： 若没有指定O_NONBLOCK（默认），只读 open 要阻塞到某个其他进程为写而打开此 FIFO。类似的，只写 open 要阻塞到某个其他进程为读而打开它。 若指定了O_NONBLOCK，则只读 open 立即返回。而只写 open 将出错返回 -1 如果没有进程已经为读而打开该 FIFO，其errno置ENXIO。 5.消息队列消息队列，是消息的链接表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标识。 1、特点 消息队列是面向记录的，其中的消息具有特定的格式以及特定的优先级。 消息队列独立于发送与接收进程。进程终止时，消息队列及其内容并不会被删除。 消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取。 2、原型1234567891 #include &lt;sys/msg.h&gt;2 // 创建或打开消息队列：成功返回队列ID，失败返回-13 int msgget(key_t key, int flag);4 // 添加消息：成功返回0，失败返回-15 int msgsnd(int msqid, const void *ptr, size_t size, int flag);6 // 读取消息：成功返回消息数据的长度，失败返回-17 int msgrcv(int msqid, void *ptr, size_t size, long type,int flag);8 // 控制消息队列：成功返回0，失败返回-19 int msgctl(int msqid, int cmd, struct msqid_ds *buf); 在以下两种情况下，msgget将创建一个新的消息队列： 如果没有与键值key相对应的消息队列，并且flag中包含了IPC_CREAT标志位。 key参数为IPC_PRIVATE。 函数msgrcv在读取消息队列时，type参数有下面几种情况： type == 0，返回队列中的第一个消息； type &gt; 0，返回队列中消息类型为 type 的第一个消息； type &lt; 0，返回队列中消息类型值小于或等于 type 绝对值的消息，如果有多个，则取类型值最小的消息。 可以看出，type值非 0 时用于以非先进先出次序读消息。也可以把 type 看做优先级的权值。 六.信号量信号量（semaphore）与已经介绍过的 IPC 结构不同，它是一个计数器。信号量用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。 1、特点​ 1.信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。 ​ 2.信号量基于操作系统的 PV 操作，程序对信号量的操作都是原子操作。 3.每次对信号量的 PV 操作不仅限于对信号量值加 1 或减 1，而且可以加减任意正整数。 ​ 4.支持信号量组。 2、原型最简单的信号量是只能取 0 和 1 的变量，这也是信号量最常见的一种形式，叫做二值信号量（Binary Semaphore）。而可以取多个正整数的信号量被称为通用信号量。 Linux 下的信号量函数都是在通用的信号量数组上进行操作，而不是在一个单一的二值信号量上进行操作。 12345671 #include &lt;sys/sem.h&gt;2 // 创建或获取一个信号量组：若成功返回信号量集ID，失败返回-13 int semget(key_t key, int num_sems, int sem_flags);4 // 对信号量组进行操作，改变信号量的值：成功返回0，失败返回-15 int semop(int semid, struct sembuf semoparray[], size_t numops); 6 // 控制信号量的相关信息7 int semctl(int semid, int sem_num, int cmd, ...); 当semget创建新的信号量集合时，必须指定集合中信号量的个数（即num_sems），通常为1； 如果是引用一个现有的集合，则将num_sems指定为 0 。 在semop函数中，sembuf结构的定义如下： 1234561 struct sembuf 2 &#123;3 short sem_num; // 信号量组中对应的序号，0～sem_nums-14 short sem_op; // 信号量值在一次操作中的改变量5 short sem_flg; // IPC_NOWAIT, SEM_UNDO6 &#125; 还有共享内存和信号量的一部分待补充]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程知识点概要(二)]]></title>
    <url>%2F2019%2F08%2F06%2F%E8%BF%9B%E7%A8%8B(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[进程(二)1.进程标识​ 每个进程都有一个非负整形表示的唯一进程ID。虽然是唯一的，但进程ID是可复用的。当一个进程终止后，其进程ID就成为复用的候选者。 ​ 系统中有一些专用进程，但具体细节随实现而不同。ID为0的通常为调度进程，常被称为交换进程，这个是内核的一部分，不执行磁盘上任何程序，因此被称为系统进程。进程ID 1通常是Init进程，在自举过程结束后由内核调用。init通常读取与系统有关的初始化文件，并将系统引导到一个状态。init进程决不会终止，它是所有孤儿进程的父进程。 2.函数fork​ 一个现有的进程可以调用fork函数创建一个新进程 12#include &lt;unistd.h&gt;pid_t fork(void); //子进程返回0，父进程返回子进程ID,出错返回-1 ​ 由fork创建的新进程称为子进程。fork函数被调用一次，但返回两次，子进程的返回值是0，而父进程的返回值则是新建子进程的进程ID。 ​ 子进程和父进程继续执行fork调用之后的指令。子进程是父进程的副本，例如，子进程获得父进程数据空间，堆和栈的副本。注意，这是子进程所拥有的副本，父进程和子进程并不共享这些存储空间部分。 ​ 下面举一个例子 12345678910111213141516171819202122232425262728293031323334#include "apue.h"int globvar = 6; /*external variable in initialized data*/char buf[] = "a write to stdout\n";intmain(void)&#123; int var; pid_t pid; var = 88; if(write(STDOUT_FILENO, buf, sizeof(buf)-1) != sizeof(buf) - 1) err_sys("write error"); printf("before fork\n"); if((pid = fork()) &lt; 0)&#123; err_sys("fork error"); &#125; else if (pid == 0) &#123; /*child*/ globvar++; /*modify variables*/ var++; &#125; else &#123; sleep(2); /*parent*/ &#125; printf("pid = %ld, glob = %d, var = %d\n", (long)getpid(), globvar,var); exit(0);&#125;//注意这个程序输出有2个，因为有2个进程./a.outa write to stdoutbefore forkpid = 430, glob = 7, var = 89 子进程的变量值改变了pid = 429, glob = 6, var = 88 父进程的变量值没有改变 ​ 一般来说，在fork之后是父进程先执行还是子进程先执行是不确定的，这取决于内核所使用的调度算法。 ​ 还有一点需要注意:在重定向父进程的标准输出时，子进程的标准输出也被重定向。实际上，fork的一个特性是父进程的所有打开文件描述符都被复制到子进程中。比如上述程序，因父进程标准输出已重定向，那么子进程写到该标准输出时，它将更新与父进程共享的该文件的偏移量。也就是说，在父进程等待子进程时，子进程写到标准输出;而在子进程终止后，父进程也写到标准输出上，并且其输出会追加在子进程所写数据之后。 ​ 在fork后处理文件描述符有以下2种常见情况 (1)父进程等待子进程完成。这种情况下父进程无需对描述符做任何处理，因子进程终止后，它曾进行过读，写操作的任一共享描述符的文件偏移量已做了相应更新。 (2)父进程和子进程各自执行不同的程序段。 ​ fork有以下2种用法 (1)一个父进程希望复制自己，使父进程和子进程同时执行不同的代码段，这在网络进程中常见 (2)一个进程要执行一个不同的程序，这对shell很常见。 3.函数exit​ 在说明fork函数时，显然子进程是在父进程调用fork后生成。且子进程将其终止状态返回给父进程。但若父进程在子进程之前终止，子进程的父进程都改变为init进程，我们称这些进程由init进程收养。 ​ 另一个关心的情况是，如果子进程在父进程之前终止，那么父进程如何能在做相应检查时得到子进程的终止状态呢？因内核为每个终止子进程保存了一定量的信息，所以当终止进程的父进程调用wait或waitpid时，可以得到这些信息。在UNIX术语中，一个已经终止，但是其父进程尚未对其进行善后处理的进程称为僵死进程(zombie)。 ​ 一个由init进程收养的进程终止时不会变成僵死进程，因init被编写成无论何时只要有一个子进程终止，init就会调用一个wait函数取得其终止状态。 4.函数wait和waitpid​ 调用wait或waitpid的进程会发生如下情况: 如果其所有子进程都还在运行，则阻塞 如果一个子进程已终止，正等待父进程获取其终止状态，则取得该子进程的终止状态立即返回 如果它没有任何子进程，则立即出错返回 5.函数exec​ 在用fork函数创建新的子进程后，子进程往往要调用一种exec函数以执行另一个程序。当进程调用一种exec函数时，该进程执行的程序完全替换为新程序，而新程序则从其main函数开始执行。因调用exec并不创建新进程，所以前后进程ID不改变。exec只是用磁盘上的一个新程序替换了当前进程的正文段，数据段，堆段和栈段。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程知识点概要(一)]]></title>
    <url>%2F2019%2F08%2F06%2F%E8%BF%9B%E7%A8%8B(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[进程(一)1.程序和进程1.程序​ 程序是一个存储在磁盘上某个目录中的可执行文件。内核使用exec函数(7个exec函数之一)，将程序读入内存，并执行程序。 2.进程和进程ID​ 程序的执行实例被称为进程(process).某些操作系统用任务(task)表示正在被执行的程序。UNIX系统确保每个进程都有一个唯一的数字标识符，称为进程ID，是一个非负整数。 3.进程控制​ 有3个用于进程控制的主要函数:fork,exec和waitpid。 ​ 下面看一个示例 1234567891011121314151617181920212223242526#include &lt;sys/wait.h&gt;#define MAXLINE 1000int main(void)&#123; char buf[MAXLINE]; pid_t pid; int status; while(fgets(buf, MAXLINE, stdin) != NULL)&#123; if(buf[strlen(buf) - 1]=='\n') buf[strlen(buf) - 1] = 0; if((pid = fork()) &lt; 0)&#123; err_sys("fork error"); &#125;else if (pid == 0)&#123; /*child*/ execlp(buf, buf, (char*)0); err_ret("couldn't execute: %s", buf); exit(127); &#125; /* parent */ if((pid = waitpid(pid, &amp;status, 0)) &lt; 0) err_sys("waitpid error"); &#125; exit(0);&#125; 上述程序很有参考价值 调用fork创建一个新进程。新进程是调用进程的一个副本，我们称调用进程为父进程，新创建的进程是子进程。fork对父进程返回新的子进程的进程ID(一个非负整数)，对于子进程则返回0.因为fork创建一个新进程，所以说它被调用一次(由父进程)，但返回两次(分别在父进程和子进程中). 在子进程中，调用execlp以执行从标准输入读入的命令。这就用新的程序文件替换了子进程原先执行的程序文件。fork和跟随其后的exec两者的组合就是某些操作系统所称的产生(spawn)一个新进程。在UNIX系统中，这两部分分离成2个独立的函数。 子进程调用execlp执行新程序文件，而父进程希望等待子进程终止，这是通过调用waitpid实现的，其参数指定要等待的进程(即pid参数是子进程的ID)。waitpid函数返回子进程的终止状态(status变量) 4.信号​ 信号用于通知进程发生了某种情况。例如，若某一进程执行除法操作，其除数为0，则将名为SIGFPE(浮点异常)的信号发送给该进程，进程有以下3种处理信号的方式 忽略信号 按系统默认方式处理 提供一个函数，信号发生时调用该函数，这被称为捕捉该信号 2.进程环境1.退出函数12345#include&lt;stdlib.h&gt;void exit(int status);//先执行一些清理，再返回内核void _Exit(int status);#include&lt;unistd.h&gt;void _exit(int status);//上2个直接进入内核 main函数返回一个整型值与用该值调用exit等价。即main函数中 1exit(0);等价于return(0);]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程知识点概要]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[线程1.概念​ 典型的unix进程可以看成只有一个控制线程，一个进程在某一时刻只能做一件事情。有了多个控制线程以后，程序设计时就可以把进程设计成在某一时刻能够做不止一件事，每个线程处理各自独立的任务。 ​ 多个线程自动地可以访问相同的存储地址空间和文件描述符。 ​ 不管处理器的个数多少，程序都可以通过使用线程得以简化。 ​ 一个进程的所有信息对该进程的所有线程都是共享的，包括可执行程序的代码，程序的全局内存和堆内存，栈以及文件描述符。 2.线程标识​ 每个线程都有一个线程ID,不像进程ID在整个系统唯一，线程ID只有在它所属的进程上下文才有意义。线程ID使用pthread_t数据类型来表示的。 1int pthread_equal(pthread_t tid1, pthread_t tid2); #若相等，返回非0数值；否则返回0 3.线程创建123#include &lt;pthread.h&gt;int pthread_create(pthread_t * restrict tidp, const pthread_attr_t *restrict attr,void *(*start_rtn)(void*), void *restrict arg);#成功返回0，否则返回错误编号 ​ 当pthread_create成功返回时，新创建线程的线程ID会被设置成tidp指向的内存单元。attr参数用于定制各种不同的线程属性，设置为NULL表示创建一个默认属性的线程。 ​ 新创建的线程从start_rtn函数的地址开始运行，该函数只有一个无类型指针参数arg，若向start_rtn函数传递的参数有一个以上，则需把这些参数放到一个结构中，再把其地址作为arg参数传入。 ​ 线程创建时不能保证哪个线程会先运行：是新创建的线程还是调用线程。 4.线程终止单个线程可以通过3种方式退出，可以在不终止整个进程的情况下，停下其控制流 线程可以简单地从启动例程中返回，返回值是线程的退出码 线程可以被统一进程中的其他线程取消 线程调用pthread_exit。 123int pthread_exit(void *rval_ptr);int pthread_join(pthread_t thread, void **rval_ptr);//调用线程将一直阻塞，直到指定的线程上述3种方式之一终止。若线程简单地从启动例程返回，rval_ptr包含返回码 线程可以通过调用pthread_cancel函数来请求取消同一进程中的其它线程。 1int pthread_cancel(pthread_t tid); 注意pthread_cancel并不等待线程终止，它仅仅提出请求。 线程被分离之后，我们不能用pthread_join函数等待它的终止状态，因为对分离状态的线程调用pthread_join会产生未定义行为。可用pthread_detach分离线程 1int pthread_detach(pthread_t tid); 5.线程同步​ 当多个线程共享相同内存时，需确保每个线程看到一致的数据视图。当一个线程可以修改的变量，其他线程可以读取或者修改的时候，我们就需要对这些线程进行同步，确保它们在访问变量的存储内容时不会访问到无效的值。 ​ 如果修改操作是原子操作，那么就不存在竞争。 5.1 互斥量​ 可使用pthread的互斥接口来保护数据，确保同一时间只有一个线程访问数据。互斥量(mutex)从本质上说是一把锁，在访问共享资源前对互斥量进行设置(加锁)，在访问完成后释放(解锁)互斥量。对互斥量进行加锁之后，任何其他试图再次对互斥量加锁的线程都会被阻塞，直到当前线程释放该互斥锁。 ​ 互斥变量是用pthread_mutex_t数据类型表示的。在使用互斥变量以前，必须首先进行初始化，可设置为常量PTHREAD_MUTEX_INITIALIZER(只适用于静态分配的互斥量)，也可以通过调用pthread_mutex_init进行初始化。且若是malloc分配互斥量，释放前要调用pthread_mutex_desroy。 123int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr);int pthread_mutex_destroy(pthread_mutex_t *mutex); 对互斥量加锁和解锁调用如下函数 123int pthread_mutex_lock(pthread_mutex_t *mutex);int pthread_mutex_unlock(pthread_mutex_t *mutex);int pthread_mutex_trylock(pthread_mutex_t *mutex); 若互斥量不希望被阻塞，可使用pthread_mutex_trylock尝试对互斥量进行加锁。若调用trylock时互斥量并未被上锁，则会锁住互斥量，但不会出现阻塞直接返回0，否则trylock会失败，不能锁住互斥量。 5.2 避免死锁​ 若线程试图对同一个互斥量加锁两次，那么它自身就会陷入死锁状态；或2个线程各自占有一个互斥量，但2个线程彼此都在请求另一个线程的资源，导致两个线程都无法向前运行，于是产生死锁。 ​ 可通过仔细控制互斥量加锁的顺序来避免死锁的发生。比如所有线程总是在对互斥量B加锁之前锁住互斥量A，则使用这2个互斥量就不会产生死锁。即总是以相同的顺序加锁可以避免多个互斥量时死锁。 5.3 函数pthread_mutex_timedlock​ 当线程试图获取一个已加锁的互斥量时，pthread_mutex_timedlock互斥量原语允许绑定线程阻塞时间。即到达超时时间值时，pthread_mutex_timedlock不会对互斥量加锁，而是返回错误码。 123#include&lt;time.h&gt;int pthread_mutex_timelock(pthread_mutex_t *restrict mutex, const struct timespec *restrict tsptr); 5.4 读写锁​ 读写锁允许更高的并行性，有3种状态：读模式下加锁状态，写模式下加锁状态，不加锁状态。一次只有一个线程可以占用写模式下的读写锁，但多个线程可以同时占有写模式的读写锁。 ​ 读写锁非常适合于对数据结构读的次数远大于写的情况。 ​ 与互斥量相比，读写锁在使用之前必须初始化，在释放它们底层的内存之前必须销毁。 123int pthread_rwlock_init(pthread_rwlock_t *restrict rwlock, const pthread_rwlockattr_t *restrict attr);int pthread_rwlock_destroy(pthread_rwlock_t *rwlock); 读写模式下锁定锁调用函数不同 123int pthread_rwlock_rdlock(pthread_rwlock_t *rwlock); int pthread_rwlock_wdlock(pthread_rwlock_t *rwlock); int pthread_rwlock_unlock(pthread_rwlock_t *rwlock); 类似之前互斥量，读写锁也可以有带有超时的速写锁。 下面3个是进阶概念，感兴趣的可以自己去了解。 5.5 条件变量5.6 自旋锁(这个很重要)5.7 屏障]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[makefile笔记(四)]]></title>
    <url>%2F2019%2F08%2F02%2Fmakefile%E7%AC%94%E8%AE%B0(%E5%9B%9B)%2F</url>
    <content type="text"><![CDATA[makefile笔记(四)4.makefile 书写命令​ 每条规则中的命令和操作系统Shell的命令行是一致的。make会按顺序一条一条的执行命令，每条命令的开头必须以[Tab]键开头，除非，命令是紧跟在依赖规则后面的分号后的。在命令行之间中的空格或是空行会被忽略，但是如果该空格或空行是以Tab键开头的，那么make会认为其是一个空命令。 ​ 我们在UNIX下可能会使用不同的Shell，但是make的命令默认是被“/bin/sh”——UNIX的标准Shell解释执行的。除非你特别指定一个其它的Shell。Makefile中，“#”是注释符，很像C/C++中的“//”，其后的本行字符都被注释。 4.1 显示命令通常，make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前，那么，这个命令将不被make显示出来，最具代表性的例子是，我们用这个功能来像屏幕显示一些信息。如： @echo 正在编译XXX模块…… 当make执行时，会输出“正在编译XXX模块……”字串，但不会输出命令，如果没有“@”，那么，make将输出： echo 正在编译XXX模块…… 正在编译XXX模块…… 如果make执行时，带入make参数“-n”或“—just-print”，那么其只是显示命令，但不会执行命令，这个功能很有利于我们调试我们的Makefile，看看我们书写的命令是执行起来是什么样子的或是什么顺序的。 而make参数“-s”或“—slient”则是全面禁止命令的显示。 4.2 命令执行当依赖目标新于目标时，也就是当规则的目标需要被更新时，make会一条一条的执行其后的命令。需要注意的是，如果你要让上一条命令的结果应用在下一条命令时，你应该使用分号分隔这两条命令。比如你的第一条命令是cd命令，你希望第二条命令得在cd之后的基础上运行，那么你就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。如： 示例一： ​ exec: ​ cd /home/hchen ​ pwd 示例二： ​ exec: ​ cd /home/hchen; pwd 当我们执行“make exec”时，第一个例子中的cd没有作用，pwd会打印出当前的Makefile目录，而第二个例子中，cd就起作用了，pwd会打印出“/home/hchen”。 make一般是使用环境变量SHELL中所定义的系统Shell来执行命令，默认情况下使用UNIX的标准Shell——/bin/sh来执行命令。但在MS-DOS下有点特殊，因为MS-DOS下没有SHELL环境变量，当然你也可以指定。如果你指定了UNIX风格的目录形式，首先，make会在SHELL所指定的路径中找寻命令解释器，如果找不到，其会在当前盘符中的当前目录中寻找，如果再找不到，其会在PATH环境变量中所定义的所有路径中寻找。MS-DOS中，如果你定义的命令解释器没有找到，其会给你的命令解释器加上诸如“.exe”、“.com”、“.bat”、“.sh”等后缀。 4.3 命令出错​ 每当命令运行完后，make会检测每个命令的返回码，如果命令返回成功，那么make会执行下一条命令，当规则中所有的命令成功返回后，这个规则就算是成功完成了。如果一个规则中的某个命令出错了（命令退出码非零），那么make就会终止执行当前规则，这将有可能终止所有规则的执行。 有些时候，命令的出错并不表示就是错误的。例如mkdir命令，我们一定需要建立一个目录，如果目录不存在，那么mkdir就成功执行，万事大吉，如果目录存在，那么就出错了。我们之所以使用mkdir的意思就是一定要有这样的一个目录，于是我们就不希望mkdir出错而终止规则的运行。 为了做到这一点，忽略命令的出错，我们可以在Makefile的命令行前加一个减号“-”（在Tab键之后），标记为不管命令出不出错都认为是成功的。如： clean: ​ -rm -f *.o 还有一个全局的办法是，给make加上“-i”或是“—ignore-errors”参数，那么，Makefile中所有命令都会忽略错误。而如果一个规则是以“.IGNORE”作为目标的，那么这个规则中的所有命令将会忽略错误。这些是不同级别的防止命令出错的方法，你可以根据你的不同喜欢设置。 还有一个要提一下的make的参数的是“-k”或是“—keep-going”，这个参数的意思是，如果某规则中的命令出错了，那么就终止该规则的执行，但继续执行其它规则。 4.4 嵌套执行make **在一些大的工程中，我们会把我们不同模块或是不同功能的源文件放在不同的目录中，我们可以在每个目录中都书写一个该目录的Makefile，这有利于让我们的Makefile变得更加地简洁，而不至于把所有的东西全部写在一个Makefile中，这样会很难维护我们的Makefile，这个技术对于我们模块编译和分段编译有着非常大的好处。** 例如，我们有一个子目录叫subdir，这个目录下有个Makefile文件，来指明了这个目录下文件的编译规则。那么我们总控的Makefile可以这样书写： subsystem: ​ cd subdir &amp;&amp; $(MAKE) 其等价于： ​ subsystem: ​ $(MAKE) -C subdir 定义$(MAKE)宏变量的意思是，也许我们的make需要一些参数，所以定义成一个变量比较利于维护。这两个例子的意思都是先进入“subdir”目录，然后执行make命令。 我们把这个Makefile叫做“总控Makefile”，总控Makefile的变量可以传递到下级的Makefile中（如果你显示的声明），但是不会覆盖下层的Makefile中所定义的变量，除非指定了“-e”参数。 如果你要传递变量到下级Makefile中，那么你可以使用这样的声明： export 如果你不想让某些变量传递到下级Makefile中，那么你可以这样声明： unexport 如： ​ 示例一： ​ export variable = value ​ 其等价于： ​ variable = value ​ export variable ​ 其等价于： ​ export variable := value ​ 其等价于： ​ variable := value ​ export variable 示例二： ​ export variable += value ​ 其等价于： ​ variable += value ​ export variable ​ 如果你要传递所有的变量，那么，只要一个export就行了。后面什么也不用跟，表示传递所有的变量。 需要注意的是，有两个变量，一个是SHELL，一个是MAKEFLAGS，这两个变量不管你是否export，其总是要传递到下层Makefile中，特别是MAKEFILES变量，其中包含了make的参数信息，如果我们执行“总控Makefile”时有make参数或是在上层Makefile中定义了这个变量，那么MAKEFILES变量将会是这些参数，并会传递到下层Makefile中，这是一个系统级的环境变量。 但是make命令中的有几个参数并不往下传递，它们是“-C”,“-f”,“-h”“-o”和“-W”（有关Makefile参数的细节将在后面说明），如果你不想往下层传递参数，那么，你可以这样来： subsystem: ​ cd subdir &amp;&amp; $(MAKE) MAKEFLAGS= 如果你定义了环境变量MAKEFLAGS，那么你得确信其中的选项是大家都会用到的，如果其中有“-t”,“-n”,和“-q”参数，那么将会有让你意想不到的结果，或许会让你异常地恐慌。 还有一个在“嵌套执行”中比较有用的参数，“-w”或是“—print-directory”会在make的过程中输出一些信息，让你看到目前的工作目录。比如，如果我们的下级make目录是“/home/hchen/gnu/make”，如果我们使用“make -w”来执行，那么当进入该目录时，我们会看到： ​ make: Entering directory `/home/hchen/gnu/make’. 而在完成下层make后离开目录时，我们会看到： make: Leaving directory `/home/hchen/gnu/make’ 当你使用“-C”参数来指定make下层Makefile时，“-w”会被自动打开的。如果参数中有“-s”（“—slient”）或是“—no-print-directory”，那么，“-w”总是失效的。 4.5 定义命令包​ 如果Makefile中出现一些相同命令序列，那么我们可以为这些相同的命令序列定义一个变量。定义这种命令序列的语法以“define”开始，以“endef”结束，如： define run-yacc yacc $(firstword $^) mv y.tab.c $@ endef 这里，“run-yacc”是这个命令包的名字，其不要和Makefile中的变量重名。在“define”和“endef”中的两行就是命令序列。这个命令包中的第一个命令是运行Yacc程序，因为Yacc程序总是生成“y.tab.c”的文件，所以第二行的命令就是把这个文件改改名字。还是把这个命令包放到一个示例中来看看吧。 foo.c : foo.y ​ $(run-yacc) 我们可以看见，要使用这个命令包，我们就好像使用变量一样。在这个命令包的使用中，命令包“run-yacc”中的“$^”就是“foo.y”，“$@”就是“foo.c”（有关这种以“$”开头的特殊变量，我们会在后面介绍），make在执行命令包时，命令包中的每个命令会被依次独立执行。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[makefile笔记(三)]]></title>
    <url>%2F2019%2F08%2F02%2Fmakefile%E7%AC%94%E8%AE%B0(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[makefile笔记(三)3 Makefile书写规则规则包含两个部分，一个是依赖关系，一个是生成目标的方法。 在Makefile中，规则的顺序是很重要的，因为，Makefile中只应该有一个最终目标，其它的目标都是被这个目标所连带出来的，所以一定要让make知道你的最终目标是什么。一般来说，定义在Makefile中的目标可能会有很多，但是第一条规则中的目标将被确立为最终的目标。如果第一条规则中的目标有很多个，那么，第一个目标会成为最终的目标。make所完成的也就是这个目标。 3.1 规则举例foo.o: foo.c defs.h # foo模块 ​ cc -c -g foo.c 看到这个例子，各位应该不是很陌生了，前面也已说过，foo.o是我们的目标，foo.c和defs.h是目标所依赖的源文件，而只有一个命令“cc -c -g foo.c”（以Tab键开头）。这个规则告诉我们两件事： \1. 文件的依赖关系，foo.o依赖于foo.c和defs.h的文件，如果foo.c和defs.h的文件日期要比foo.o文件日期要新，或是foo.o不存在，那么依赖关系发生。 \2. 如果生成（或更新）foo.o文件。也就是那个cc命令，其说明了，如何生成foo.o这个文件。（当然foo.c文件include了defs.h文件） 3.2 规则的语法targets : prerequisites ​ command ​ … 或是这样： ​ targets : prerequisites ; command ​ command ​ … targets是文件名，以空格分开，可以使用通配符。一般来说，我们的目标基本上是一个文件，但也有可能是多个文件。 command是命令行，如果其不与“target:prerequisites”在一行，那么，必须以[Tab键]开头，如果和prerequisites在一行，那么可以用分号做为分隔。（见上） prerequisites也就是目标所依赖的文件（或依赖目标）。如果其中的某个文件要比目标文件要新，那么，目标就被认为是“过时的”，被认为是需要重生成的。这个在前面已经讲过了。 如果命令太长，你可以使用反斜框（‘\’）作为换行符。make对一行上有多少个字符没有限制。规则告诉make两件事，文件的依赖关系和如何成成目标文件。 一般来说，make会以UNIX的标准Shell，也就是/bin/sh来执行命令。 3.3 在规则中使用通配符如果我们想定义一系列比较类似的文件，我们很自然地就想起使用通配符。make支持三各通配符：“*”，“?”和“[…]”。这是和Unix的B-Shell是相同的。 “~” 波浪号（“~”）字符在文件名中也有比较特殊的用途。如果是“~/test”，这就表示当前用户的$HOME目录下的test目录。而“~hchen/test”则表示用户hchen的宿主目录下的test目录。（这些都是Unix下的小知识了，make也支持）而在Windows或是MS-DOS下，用户没有宿主目录，那么波浪号所指的目录则根据环境变量“HOME”而定。 “*“通配符代替了你一系列的文件，如“*.c”表示所以后缀为c的文件。一个需要我们注意的是，如果我们的文件名中有通配符，如：“\”，那么可以用转义字符“\”，如“*”来表示真实的“*“字符，而不是任意长度的字符串。 好吧，还是先来看几个例子吧： clean: ​ rm -f *.o 上面这个例子我不不多说了，这是操作系统Shell所支持的通配符。这是在命令中的通配符。 print: *.c ​ lpr -p $? ​ touch print 上面这个例子说明了通配符也可以在我们的规则中，目标print依赖于所有的[.c]文件。其中的“$?”是一个自动化变量，我会在后面给你讲述。 objects = *.o 上面这个例子，表示了，通符同样可以用在变量中。并不是说[.o]会展开，不！objects的值就是“\.o”。Makefile中的变量其实就是C/C++中的宏。如果你要让通配符在变量中展开，也就是让objects的值是所有[.o]的文件名的集合，那么，你可以这样： objects := $(wildcard *.o) 这种用法由关键字“wildcard”指出，关于Makefile的关键字，我们将在后面讨论。 3.4 文件搜寻在一些大的工程中，有大量的源文件，我们通常的做法是把这许多的源文件分类，并存放在不同的目录中。所以，当make需要去找寻文件的依赖关系时，你可以在文件前加上路径，但最好的方法是把一个路径告诉make，让make在自动去找。 Makefile文件中的特殊变量“VPATH”就是完成这个功能的，如果没有指明这个变量，make只会在当前的目录中去找寻依赖文件和目标文件。如果定义了这个变量，那么，make就会在当当前目录找不到的情况下，到所指定的目录中去找寻文件了。 VPATH = src:../headers 上面的的定义指定两个目录，“src”和“../headers”，make会按照这个顺序进行搜索。目录由“冒号”分隔。（当然，当前目录永远是最高优先搜索的地方） 另一个设置文件搜索路径的方法是使用make的“vpath”关键字（注意，它是全小写的），这不是变量，这是一个make的关键字，这和上面提到的那个VPATH变量很类似，但是它更为灵活。它可以指定不同的文件在不同的搜索目录中。这是一个很灵活的功能。它的使用方法有三种： \1. vpath &lt; pattern&gt; &lt; directories&gt; 为符合模式&lt; pattern&gt;的文件指定搜索目录。 \2. vpath &lt; pattern&gt; 清除符合模式&lt; pattern&gt;的文件的搜索目录。 \3. vpath 清除所有已被设置好了的文件搜索目录。 vapth使用方法中的&lt; pattern&gt;需要包含“%”字符。“%”的意思是匹配零或若干字符，例如，“%.h”表示所有以“.h”结尾的文件。&lt; pattern&gt;指定了要搜索的文件集，而&lt; directories&gt;则指定了的文件集的搜索的目录。例如： vpath %.h ../headers 该语句表示，要求make在“../headers”目录下搜索所有以“.h”结尾的文件。（如果某文件在当前目录没有找到的话） 我们可以连续地使用vpath语句，以指定不同搜索策略。如果连续的vpath语句中出现了相同的&lt; pattern&gt;，或是被重复了的&lt; pattern&gt;，那么，make会按照vpath语句的先后顺序来执行搜索。如： vpath %.c foo vpath % blish vpath %.c bar 其表示“.c”结尾的文件，先在“foo”目录，然后是“blish”，最后是“bar”目录。 vpath %.c foo:bar vpath % blish 而上面的语句则表示“.c”结尾的文件，先在“foo”目录，然后是“bar”目录，最后才是“blish”目录。 3.5 伪目标最早先的一个例子中，我们提到过一个“clean”的目标，这是一个“伪目标”， clean: ​ rm *.o temp 正像我们前面例子中的“clean”一样，即然我们生成了许多文件编译文件，我们也应该提供一个清除它们的“目标”以备完整地重编译而用。 （以“make clean”来使用该目标） 因为，我们并不生成“clean”这个文件。“伪目标”并不是一个文件，只是一个标签，由于“伪目标”不是文件，所以make无法生成它的依赖关系和决定它是否要执行。我们只有通过显示地指明这个“目标”才能让其生效。当然，“伪目标”的取名不能和文件名重名，不然其就失去了“伪目标”的意义了。 当然，为了避免和文件重名的这种情况，我们可以使用一个特殊的标记“.PHONY”来显示地指明一个目标是“伪目标”，向make说明，不管是否有这个文件，这个目标就是“伪目标”。 .PHONY : clean 只要有这个声明，不管是否有“clean”文件，要运行“clean”这个目标，只有“make clean”这样。于是整个过程可以这样写： ​ .PHONY: clean clean: ​ rm *.o temp 伪目标一般没有依赖的文件。但是，我们也可以为伪目标指定所依赖的文件。伪目标同样可以作为“默认目标”，只要将其放在第一个。一个示例就是，如果你的Makefile需要一口气生成若干个可执行文件，但你只想简单地敲一个make完事，并且，所有的目标文件都写在一个Makefile中，那么你可以使用“伪目标”这个特性： all : prog1 prog2 prog3 .PHONY : all prog1 : prog1.o utils.o ​ cc -o prog1 prog1.o utils.o prog2 : prog2.o ​ cc -o prog2 prog2.o prog3 : prog3.o sort.o utils.o ​ cc -o prog3 prog3.o sort.o utils.o 我们知道，Makefile中的第一个目标会被作为其默认目标。我们声明了一个“all”的伪目标，其依赖于其它三个目标。由于伪目标的特性是，总是被执行的，所以其依赖的那三个目标就总是不如“all”这个目标新。所以，其它三个目标的规则总是会被决议。也就达到了我们一口气生成多个目标的目的。“.PHONY : all”声明了“all”这个目标为“伪目标”。 随便提一句，从上面的例子我们可以看出，目标也可以成为依赖。所以，伪目标同样也可成为依赖。看下面的例子： .PHONY: cleanall cleanobj cleandiff cleanall : cleanobj cleandiff ​ rm program cleanobj : ​ rm *.o cleandiff : ​ rm *.diff “makeclean”将清除所有要被清除的文件。“cleanobj”和“cleandiff”这两个伪目标有点像“子程序”的意思。我们可以输入“makecleanall”和“make cleanobj”和“makecleandiff”命令来达到清除不同种类文件的目的 3.6 多目标Makefile的规则中的目标可以不止一个，其支持多目标，有可能我们的多个目标同时依赖于一个文件，并且其生成的命令大体类似。于是我们就能把其合并起来。当然，多个目标的生成规则的执行命令是同一个，这可能会给我们带来麻烦，不过好在我们的可以使用一个自动化变量“$@”（关于自动化变量，将在后面讲述），这个变量表示着目前规则中所有的目标的集合，这样说可能很抽象，还是看一个例子吧。 bigoutput littleoutput : text.g ​ generate text.g -$(subst output,,$@) &gt; $@ 上述规则等价于： bigoutput : text.g ​ generate text.g -big &gt; bigoutput littleoutput : text.g ​ generate text.g -little &gt; littleoutput 其中，-$(subst output,,$@)中的“$”表示执行一个Makefile的函数，函数名为subst，后面的为参数。关于函数，将在后面讲述。这里的这个函数是截取字符串的意思，“$@”表示目标的集合，就像一个数组，“$@”依次取出目标，并执于命令。 3.7 静态模式​ 静态模式可以更加容易地定义多目标的规则，可以让我们的规则变得更加的有弹性和灵活。先来看一下语法： : : … targets定义了一系列的目标文件，可以有通配符。是目标的一个集合。 target-pattern是指明了targets的模式，也就是目标集的模式。 prereq-patterns是目标的依赖模式，它对target-pattern形成的模式再进行一次依赖目标的定义。 这样描述这三个东西，可能还是没有说清楚，还是举个例子来说明一下吧。如果我们的定义成“%.o”，意思是我们的集合中都是以“.o”结尾的，而如果我们的定义成“%.c”，意思是对所形成的目标集进行二次定义，其计算方法是，取模式中的“%”（也就是去掉了[.o]这个结尾），并为其加上[.c]这个结尾，形成的新集合。 所以，我们的“目标模式”或是“依赖模式”中都应该有“%”这个字符，如果你的文件名中有“%”那么你可以使用反斜杠“\”进行转义，来标明真实的“%”字符。 看一个例子： objects = foo.o bar.o all: $(objects) $(objects): %.o: %.c ​ $(CC) -c $(CFLAGS) $&lt; -o $@ 上面的例子中，指明了我们的目标从$object中获取，“%.o”表明要所有以“.o”结尾的目标，也就是“foo.o bar.o”，也就是变量$object集合的模式，而依赖模式“%.c”则取模式“%.o”的“%”，也就是“foobar”，并为其加下“.c”的后缀，于是，我们的依赖目标就是“foo.cbar.c”。而命令中的“$&lt;”和“$@”则是自动化变量，“$&lt;”表示所有的依赖目标集（也就是“foo.c bar.c”），“$@”表示目标集（也就是foo.o bar.o”）。于是，上面的规则展开后等价于下面的规则： foo.o : foo.c ​ $(CC) -c $(CFLAGS) foo.c -o foo.o bar.o : bar.c ​ $(CC) -c $(CFLAGS) bar.c -o bar.o 试想，如果我们的“%.o”有几百个，那种我们只要用这种很简单的“静态模式规则”就可以写完一堆规则，实在是太有效率了。“静态模式规则”的用法很灵活，如果用得好，那会一个很强大的功能。再看一个例子： files = foo.elc bar.o lose.o $(filter %.o,$(files)): %.o: %.c ​ $(CC) -c $(CFLAGS) $&lt; -o $@ $(filter %.elc,$(files)): %.elc: %.el ​ emacs -f batch-byte-compile $&lt; $(filter%.o,$(files))表示调用Makefile的filter函数，过滤“$filter”集，只要其中模式为“%.o”的内容。其的它内容，我就不用多说了吧。这个例字展示了Makefile中更大的弹性。 3.8 自动生成依赖性在Makefile中，我们的依赖关系可能会需要包含一系列的头文件，比如，如果我们的main.c中有一句“#include “defs.h””，那么我们的依赖关系应该是： main.o : main.c defs.h 但是，如果是一个比较大型的工程，你必需清楚哪些C文件包含了哪些头文件，并且，你在加入或删除头文件时，也需要小心地修改Makefile，这是一个很没有维护性的工作。为了避免这种繁重而又容易出错的事情，我们可以使用C/C++编译的一个功能。大多数的C/C++编译器都支持一个“-M”的选项，即自动找寻源文件中包含的头文件，并生成一个依赖关系。例如，如果我们执行下面的命令： cc -M main.c 其输出是： main.o : main.c defs.h 于是由编译器自动生成的依赖关系，这样一来，你就不必再手动书写若干文件的依赖关系，而由编译器自动生成了。需要提醒一句的是，如果你使用GNU的C/C++编译器，你得用“-MM”参数，不然，“-M”参数会把一些标准库的头文件也包含进来。 gcc-M main.c的输出是： main.o: main.c defs.h /usr/include/stdio.h /usr/include/features.h \ ​ /usr/include/sys/cdefs.h /usr/include/gnu/stubs.h \ ​ /usr/lib/gcc-lib/i486-suse-linux/2.95.3/include/stddef.h \ ​ /usr/include/bits/types.h /usr/include/bits/pthreadtypes.h \ ​ /usr/include/bits/sched.h /usr/include/libio.h \ ​ /usr/include/_G_config.h /usr/include/wchar.h \ ​ /usr/include/bits/wchar.h /usr/include/gconv.h \ ​ /usr/lib/gcc-lib/i486-suse-linux/2.95.3/include/stdarg.h \ ​ /usr/include/bits/stdio_lim.h gcc-MM main.c的输出则是： main.o: main.c defs.h 那么，编译器的这个功能如何与我们的Makefile联系在一起呢。因为这样一来，我们的Makefile也要根据这些源文件重新生成，让Makefile自已依赖于源文件？这个功能并不现实，不过我们可以有其它手段来迂回地实现这一功能。GNU组织建议把编译器为每一个源文件的自动生成的依赖关系放到一个文件中，为每一个“name.c”的文件都生成一个“name.d”的Makefile文件，[.d]文件中就存放对应[.c]文件的依赖关系。 于是，我们可以写出[.c]文件和[.d]文件的依赖关系，并让make自动更新或自成[.d]文件，并把其包含在我们的主Makefile中，这样，我们就可以自动化地生成每个文件的依赖关系了。 这里，我们给出了一个模式规则来产生[.d]文件： %.d: %.c ​ @set -e; rm -f $@; \ ​ $(CC) -M $(CPPFLAGS) $&lt; &gt; $@. ; \ ​ sed ‘s,$∗$∗.o[ :]*,\1.o $@ : ,g’ &lt; $@. > $@; \ ​ rm -f $@. 这个规则的意思是，所有的[.d]文件依赖于[.c]文件，“rm-f $@”的意思是删除所有的目标，也就是[.d]文件，第二行的意思是，为每个依赖文件“$&lt;”，也就是[.c]文件生成依赖文件，“$@”表示模式“%.d”文件，如果有一个C文件是name.c，那么“%”就是“name”，“”意为一个随机编号，第二行生成的文件有可能是“name.d.12345”，第三行使用sed命令做了一个替换，关于sed命令的用法请参看相关的使用文档。第四行就是删除临时文件。 总而言之，这个模式要做的事就是在编译器生成的依赖关系中加入[.d]文件的依赖，即把依赖关系： main.o : main.c defs.h 转成： main.o main.d : main.c defs.h 于是，我们的[.d]文件也会自动更新了，并会自动生成了，当然，你还可以在这个[.d]文件中加入的不只是依赖关系，包括生成的命令也可一并加入，让每个[.d]文件都包含一个完赖的规则。一旦我们完成这个工作，接下来，我们就要把这些自动生成的规则放进我们的主Makefile中。我们可以使用Makefile的“include”命令，来引入别的Makefile文件（前面讲过），例如： sources = foo.c bar.c include $(sources:.c=.d) 上述语句中的“$(sources:.c=.d)”中的“.c=.d”的意思是做一个替换，把变量$(sources)所有[.c]的字串都替换成[.d]，关于这个“替换”的内容，在后面我会有更为详细的讲述。当然，你得注意次序，因为include是按次来载入文件，最先载入的[.d]文件中的目标会成为默认目标]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[makefile笔记(二)]]></title>
    <url>%2F2019%2F08%2F02%2Fmakefile%E7%AC%94%E8%AE%B0(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[makefile笔记(二)2.1 makefile里面有什么？Makefile里主要包含了五个东西：显式规则、隐晦规则、变量定义、文件指示和注释。 显式规则。显式规则说明了，如何生成一个或多的的目标文件。这是由Makefile的书写者明显指出，要生成的文件，文件的依赖文件，生成的命令。 隐晦规则。由于我们的make有自动推导的功能，所以隐晦的规则可以让我们比较粗糙地简略地书写Makefile，这是由make所支持的。 变量的定义。在Makefile中我们要定义一系列的变量，变量一般都是字符串，这个有点像C语言中的宏，当Makefile被执行时，其中的变量都会被扩展到相应的引用位置上。 文件指示。其包括了三个部分，一个是在一个Makefile中引用另一个Makefile，就像C语言中的include一样；另一个是指根据某些情况指定Makefile中的有效部分，就像C语言中的预编译#if一样；还有就是定义一个多行的命令。有关这一部分的内容，我会在后续的部分中讲述。 注释。Makefile中只有行注释，和UNIX的Shell脚本一样，其注释是用“#”字符，这个就像C/C++中的“//”一样。如果你要在你的Makefile中使用“#”字符，可以用反斜框进行转义，如：“#”。 最后，还值得一提的是，在Makefile中的命令，必须要以[Tab]键开始。 2.2Makefile的文件名​ 默认的情况下，make命令会在当前目录下按顺序找寻文件名为“GNUmakefile”、“makefile”、“Makefile”的文件，找到了解释这个文件。在这三个文件名中，最好使用“Makefile”这个文件名，因为，这个文件名第一个字符为大写，这样有一种显目的感觉。最好不要用“GNUmakefile”，这个文件是GNU的make识别的。有另外一些make只对全小写的“makefile”文件名敏感，但是基本上来说，大多数的make都支持“makefile”和“Makefile”这两种默认文件名。 ​ 当然，你可以使用别的文件名来书写Makefile，比如：“Make.Linux”，“Make.Solaris”，“Make.AIX”等，如果要指定特定的Makefile，你可以使用make的“-f”和“—file”参数，如：make -f Make.Linux或make —file Make.AIX。 2.3 引用其它的Makefile在Makefile使用include关键字可以把别的Makefile包含进来，这很像C语言的#include，被包含的文件会原模原样的放在当前文件的包含位置。include的语法是： 1include&lt;filename&gt; filename可以是当前操作系统Shell的文件模式（可以保含路径和通配符） 在include前面可以有一些空字符，但是绝不能是[Tab]键开始。include和可以用一个或多个空格隔开。举个例子，你有这样几个Makefile：a.mk、b.mk、c.mk，还有一个文件叫foo.make，以及一个变量$(bar)，其包含了e.mk和f.mk，那么，下面的语句： include foo.make *.mk $(bar) 等价于： include foo.make a.mk b.mk c.mk e.mk f.mk make命令开始时，会把找寻include所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找： 123&gt; 1.如果make执行时，有“-I”或“--include-dir”参数，那么make就会在这个参数所指定的目录下去寻找。&gt; 2.如果目录/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。&gt; ​ 如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。如果你想让make不理那些无法读取的文件，而继续执行，你可以在include前加一个减号“-”。如： -include 其表示，无论include过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。 2.4 环境变量MAKEFILES​ 如果你的当前环境中定义了环境变量MAKEFILES，那么，make会把这个变量中的值做一个类似于include的动作。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 ​ 但是在这里我还是建议不要使用这个环境变量，因为只要这个变量一被定义，那么当你使用make时，所有的Makefile都会受到它的影响，这绝不是你想看到的。在这里提这个事，只是为了告诉大家，也许有时候你的Makefile出现了怪事，那么你可以看看当前环境中有没有定义这个变量。 2.5 make的工作方式GNU的make工作时的执行步骤入下：（想来其它的make也是类似） \1. 读入所有的Makefile。 \2. 读入被include的其它Makefile。 \3. 初始化文件中的变量。 \4. 推导隐晦规则，并分析所有规则。 \5. 为所有的目标文件创建依赖关系链。 \6. 根据依赖关系，决定哪些目标要重新生成。 \7. 执行生成命令。 1-5步为第一个阶段，6-7为第二个阶段。第一个阶段中，如果定义的变量被使用了，那么，make会把其展开在使用的位置。但make并不会完全马上展开，make使用的是拖延战术，如果变量出现在依赖关系的规则中，那么仅当这条依赖被决定要使用了，变量才会在其内部展开。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[makefile笔记(一)]]></title>
    <url>%2F2019%2F08%2F02%2Fmakefile%E7%AC%94%E8%AE%B0(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[makefile笔记(一)什么是makefile​ 一个工程中的源文件不计其数，其按类型、功能、模块分别放在若干个目录中，makefile定义了一系列的规则来指定哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为 makefile就像一个Shell脚本一样，也可以执行操作系统的命令。 ​ makefile的好处就是”自动化编译”，一旦写好，只需一个make命令，整个工程完全自动编译，极大地提高了软件开发的效率。make是一个命令工具，是一个解释makefile中指令的命令工具，一般来说，大多数的IDE都有这个命令。 0.1 预备知识：关于程序的编译和连接​ 一般来说，无论c,c++，首先要把源文件编译成中间代码文件，在windows下也就是.obj文件，UNIX下是.o文件，即Object File.这个动作叫编译(compile).然后再把大量Object File合成执行文件，这叫做链接(link). **编译时**，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常是你需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。 **链接时**，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“**库文件”（Library File)**，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。 ​ 总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。你需要指定函数的ObjectFile. 1.Makefile介绍​ make命令执行时，需要一个 Makefile 文件，以告诉make命令需要怎么样的去编译和链接程序。 ​ 首先，我们用一个示例来说明Makefile的书写规则。这个示例来源于GNU的make使用手册，在这个示例中，我们的工程有8个C文件，和3个头文件，我们要写一个Makefile来告诉make命令如何编译和链接这几个文件。我们的规则是：​ 1.如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。 ​ 2.如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。 ​ 3.如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。 ​ 只要我们的Makefile写得够好，所有的这一切，我们只用一个make命令就可以完成，make命令会自动智能地根据当前的文件修改的情况来确定哪些文件需要重编译，从而自己编译所需要的文件和链接目标程序。 1.1 Makefile的规则让我们先看看makefile的规则 target… : prerequisites … ​ command ​ … ​ … ———————————————————————————————————————- ​ target也就是一个目标文件，可以是Object File，也可以是执行文件。还可以是一个标签（Label），对于标签这种特性，在后续的“伪目标”章节中会有叙述。 ​ prerequisites就是，要生成那个target所需要的文件或是目标。 ​ command也就是make需要执行的命令。（任意的Shell命令） ​ 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。说白一点就是说，prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。这就是Makefile的规则。也就是Makefile中最核心的内容。 1.2 一个示例正如前面所说的，如果一个工程有3个头文件，和8个C文件，我们为了完成前面所述的那三个规则，我们的Makefile应该是下面的这个样子的。 edit : main.o kbd.o command.o display.o \ ​ insert.o search.o files.o utils.o ​ cc -o edit main.o kbd.o command.o display.o \ ​ insert.o search.o files.o utils.o main.o : main.c defs.h ​ cc -c main.c kbd.o : kbd.c defs.h command.h ​ cc -c kbd.c command.o : command.c defs.h command.h ​ cc -c command.c display.o : display.c defs.h buffer.h ​ cc -c display.c insert.o : insert.c defs.h buffer.h ​ cc -c insert.c search.o : search.c defs.h buffer.h ​ cc -c search.c files.o : files.c defs.h buffer.h command.h ​ cc -c files.c utils.o : utils.c defs.h ​ cc -c utils.c clean : ​ rm edit main.o kbd.o command.o display.o \ ​ insert.o search.o files.o utils.o ​ 反斜杠（\）是换行符的意思。这样比较便于Makefile的易读。我们可以把这个内容保存在文件为“Makefile”或“makefile”的文件中，然后在该目录下直接输入命令“make”就可以生成执行文件edit。如果要删除执行文件和所有的中间目标文件，那么，只要简单地执行一下“make clean”就可以了。 ​ 在这个makefile中，目标文件（target）包含：执行文件edit和中间目标文件（*.o），依赖文件（prerequisites）就是冒号后面的那些 .c 文件和 .h文件。每一个 .o 文件都有一组依赖文件，而这些 .o 文件又是执行文件 edit 的依赖文件。依赖关系的实质上就是说明了目标文件是由哪些文件生成的，换言之，目标文件是哪些文件更新的。 ​ 在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个Tab键作为开头。记住，make并不管命令是怎么工作的，他只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期要比targets文件的日期要新，或者target不存在的话，那么，make就会执行后续定义的命令。 ​ 这里要说明一点的是，clean不是一个文件，它只不过是一个动作名字，有点像C语言中的lable一样，其冒号后什么也没有，那么，make就不会自动去找文件的依赖性，也就不会自动执行其后所定义的命令。要执行其后的命令，就要在make命令后明显得指出这个lable的名字。这样的方法非常有用，我们可以在一个makefile中定义不用的编译或是和编译无关的命令，比如程序的打包，程序的备份，等等。 1.3 make是如何工作的在默认的方式下，也就是我们只输入make命令。那么， make会在当前目录下找名字叫“Makefile”或“makefile”的文件。 如果找到，它会找文件中的第一个目标文件（target），在上面的例子中，他会找到“edit”这个文件，并把这个文件作为最终的目标文件。 如果edit文件不存在，或是edit所依赖的后面的 .o 文件的文件修改时间要比edit这个文件新，那么，他就会执行后面所定义的命令来生成edit这个文件。 如果edit所依赖的.o文件也存在，那么make会在当前文件中找目标为.o文件的依赖性，如果找到则再根据那一个规则生成.o文件。（这有点像一个堆栈的过程） 当然，你的C文件和H文件是存在的啦，于是make会生成 .o 文件，然后再用 .o 文件声明make的终极任务，也就是执行文件edit了。 1这就是整个make的依赖性，make会一层又一层地去找文件的依赖关系，直到最终编译出第一个目标文件。在找寻的过程中，如果出现错误，比如最后被依赖的文件找不到，那么make就会直接退出，并报错，而对于所定义的命令的错误，或是编译不成功，make根本不理。make只管文件的依赖性，即，如果在我找了依赖关系之后，冒号后面的文件还是不在，那么对不起，我就不工作啦。 ​ 通过上述分析，我们知道，像clean这种，没有被第一个目标文件直接或间接关联，那么它后面所定义的命令将不会被自动执行，不过，我们可以显示要make执行。即命令——“make clean”，以此来清除所有的目标文件，以便重编译。 ​ 于是在我们编程中，如果这个工程已被编译过了，当我们修改了其中一个源文件，比如file.c，那么根据我们的依赖性，我们的目标file.o会被重编译（也就是在这个依性关系后面所定义的命令），于是file.o的文件也是最新的啦，于是file.o的文件修改时间要比edit要新，所以edit也会被重新链接了（详见edit目标文件后定义的命令）。 而如果我们改变了“command.h”，那么，kdb.o、command.o和files.o都会被重编译，并且，edit会被重链接。 1.4 makefile中使用变量在上面的例子中，先让我们看看edit的规则： ​ edit : main.o kbd.o command.o display.o \ ​ insert.o search.o files.o utils.o ​ cc -o edit main.o kbd.o command.o display.o \ ​ insert.o search.o files.o utils.o ​ 我们可以看到[.o]文件的字符串被重复了两次，如果我们的工程需要加入一个新的[.o]文件，那么我们需要在两个地方加（应该是三个地方，还有一个地方在clean中）。当然，我们的makefile并不复杂，所以在两个地方加也不累，但如果makefile变得复杂，那么我们就有可能会忘掉一个需要加入的地方，而导致编译失败。所以，为了makefile的易维护，在makefile中我们可以使用变量。makefile的变量也就是一个字符串，理解成C语言中的宏可能会更好。 比如，我们声明一个变量，叫objects, OBJECTS, objs, OBJS, obj, 或是 OBJ，反正不管什么啦，只要能够表示obj文件就行了。我们在makefile一开始就这样定义： ​ objects = main.o kbd.o command.o display.o \ ​ insert.o search.o files.o utils.o 于是，我们就可以很方便地在我们的makefile中以“$(objects)”的方式来使用这个变量了，于是我们的改良版makefile就变成下面这个样子： 12345678910111213141516171819202122objects = main.o kbd.o command.o display.o \ insert.osearch.o files.o utils.o edit : $(objects) cc -o edit $(objects)main.o : main.c defs.h cc -c main.ckbd.o : kbd.c defs.h command.h cc -c kbd.ccommand.o : command.c defs.h command.h cc -c command.cdisplay.o : display.c defs.h buffer.h cc -c display.cinsert.o : insert.c defs.h buffer.h cc -c insert.csearch.o : search.c defs.h buffer.h cc -c search.cfiles.o : files.c defs.h buffer.h command.h cc -c files.cutils.o : utils.c defs.h cc -c utils.cclean : rm edit $(objects) 于是如果有新的 .o 文件加入，我们只需简单地修改一下 objects 变量就可以了。 1.5 让makefile自动推导GNU的make很强大，它可以自动推导文件以及文件依赖关系后面的命令，于是我们就没必要去在每一个[.o]文件后都写上类似的命令，因为，我们的make会自动识别，并自己推导命令。 ​ 只要make看到一个[.o]文件，它就会自动的把[.c]文件加在依赖关系中，如果make找到一个whatever.o，那么whatever.c，就会是whatever.o的依赖文件。并且 cc -c whatever.c 也会被推导出来，于是，我们的makefile再也不用写得这么复杂。我们的是新的makefile又出炉了。 123456789101112131415161718objects = main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o edit : $(objects) cc -o edit $(objects) main.o : defs.hkbd.o : defs.h command.hcommand.o : defs.h command.hdisplay.o : defs.h buffer.hinsert.o : defs.h buffer.hsearch.o : defs.h buffer.hfiles.o : defs.h buffer.h command.hutils.o : defs.h .PHONY : cleanclean : rm edit $(objects) 这种方法，也就是make的“隐晦规则”。上面文件内容中，“.PHONY”表示，clean是个伪目标文件。 1.6 另一种风格的makefile 既然我们的make可以自动推导命令，那么我看到那堆[.o]和[.h]的依赖就有点不爽，那么多的重复的[.h]，能不能把其收拢起来，好吧，没有问题，这个对于make来说很容易，谁叫它提供了自动推导命令和文件的功能呢？来看看最新风格的makefile吧。 12345678910111213objects = main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o edit : $(objects) cc -o edit $(objects) $(objects) : defs.hkbd.o command.o files.o : command.hdisplay.o insert.o search.o files.o : buffer.h .PHONY : cleanclean : rm edit $(objects) 这种风格，让我们的makefile变得很简单，但我们的文件依赖关系就显得有点凌乱了。鱼和熊掌不可兼得。还看你的喜好了。我是不喜欢这种风格的，一是文件的依赖关系看不清楚，二是如果文件一多，要加入几个新的.o文件，那就理不清楚了。 1.7 清空目标文件的规则​ 每个Makefile中都应该写一个清空目标文件（.o和执行文件）的规则，这不仅便于重编译，也很利于保持文件的清洁。 ​ 一般的风格都是： ​ clean: ​ rm edit $(objects) 更为稳健的做法是： ​ .PHONY : clean ​ clean : ​ -rm edit $(objects) 前面说过，.PHONY意思表示clean是一个“伪目标”，。而在rm命令前面加了一个小减号的意思就是，也许某些文件出现问题，但不要管，继续做后面的事。当然，clean的规则不要放在文件的开头，不然，这就会变成make的默认目标，相信谁也不愿意这样。不成文的规矩是——“clean从来都是放在文件的最后”。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git总结(四)]]></title>
    <url>%2F2019%2F07%2F29%2Fgit%E6%80%BB%E7%BB%93(%E5%9B%9B)%2F</url>
    <content type="text"><![CDATA[git总结(四)1.标签管理发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。 Git有commit，为什么还要引入tag？ “请把上周一的那个版本打包发布，commit号是6a5819e…” “一串乱七八糟的数字不好找！” 如果换一个办法： “请把上周一的那个版本打包发布，版本号是v1.2” “好的，按照tag v1.2查找commit就行！” 所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 2.创建标签 命令git tag &lt;tagname&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id； 命令git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息； 命令git tag可以查看所有标签。 3.操作标签如果标签打错了，也可以删除： 12$ git tag -d v0.1Deleted tag &apos;v0.1&apos; (was f15b0dd) 因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除。 如果要推送某个标签到远程，使用命令git push origin &lt;tagname&gt;： 1234$ git push origin v1.0Total 0 (delta 0), reused 0 (delta 0)To github.com:michaelliao/learngit.git * [new tag] v1.0 -&gt; v1.0 或者，一次性推送全部尚未推送到远程的本地标签： 1234$ git push origin --tagsTotal 0 (delta 0), reused 0 (delta 0)To github.com:michaelliao/learngit.git * [new tag] v0.9 -&gt; v0.9 如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除： 12$ git tag -d v0.9Deleted tag &apos;v0.9&apos; (was f52c633) 然后，从远程删除。删除命令也是push，但是格式如下： 123$ git push origin :refs/tags/v0.9To github.com:michaelliao/learngit.git - [deleted] v0.9 要看看是否真的从远程库删除了标签，可以登陆GitHub查看。 小结 命令git push origin &lt;tagname&gt;可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d &lt;tagname&gt;可以删除一个本地标签； 命令git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签。 4.使用github 在GitHub上，可以任意Fork开源仓库； 自己拥有Fork后的仓库的读写权限； 可以推送pull request给官方仓库来贡献代码。 5.忽略特殊文件 忽略某些文件时，需要编写.gitignore； .gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理！ 6.配置别名有没有经常敲错命令？比如git status？status这个单词真心不好记。 如果敲git st就表示git status那就简单多了，当然这种偷懒的办法我们是极力赞成的。 我们只需要敲一行命令，告诉Git，以后st就表示status： 1$ git config --global alias.st status 好了，现在敲git st看看效果。 当然还有别的命令可以简写，很多人都用co表示checkout，ci表示commit，br表示branch： 123$ git config --global alias.co checkout$ git config --global alias.ci commit$ git config --global alias.br branch 以后提交就可以简写成： 1$ git ci -m &quot;bala bala bala...&quot; --global参数是全局参数，也就是这些命令在这台电脑的所有Git仓库下都有用。 在撤销修改一节中，我们知道，命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区。既然是一个unstage操作，就可以配置一个unstage别名： 1$ git config --global alias.unstage &apos;reset HEAD&apos; 当你敲入命令： 1$ git unstage test.py 实际上Git执行的是： 1$ git reset HEAD test.py 配置一个git last，让其显示最后一次提交信息： 1$ git config --global alias.last &apos;log -1&apos; 这样，用git last就能显示最近一次的提交： 1234567$ git lastcommit adca45d317e6d8a4b23f9811c3d7b7f0f180bfe2Merge: bd6ae48 291bea8Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Thu Aug 22 22:49:22 2013 +0800 merge &amp; fix hello.py 甚至还有人丧心病狂地把lg配置成了： 1git config --global alias.lg &quot;log --color --graph --pretty=format:&apos;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&apos; --abbrev-commit&quot; 来看看git lg的效果： 为什么不早点告诉我？别激动，咱不是为了多记几个英文单词嘛！ 配置文件配置Git的时候，加上--global是针对当前用户起作用的，如果不加，那只针对当前的仓库起作用。 配置文件放哪了？每个仓库的Git配置文件都放在.git/config文件中： 12345678910111213141516$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[remote &quot;origin&quot;] url = git@github.com:michaelliao/learngit.git fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;] remote = origin merge = refs/heads/master[alias] last = log -1 别名就在[alias]后面，要删除别名，直接把对应的行删掉即可。 而当前用户的Git配置文件放在用户主目录下的一个隐藏文件.gitconfig中： 123456789$ cat .gitconfig[alias] co = checkout ci = commit br = branch st = status[user] name = Your Name email = your@email.com 配置别名也可以直接修改这个文件，如果改错了，可以删掉文件重新通过命令配置。 7.搭建Git服务器在远程仓库一节中，我们讲了远程仓库实际上和本地仓库没啥不同，纯粹为了7x24小时开机并交换大家的修改。 GitHub就是一个免费托管开源代码的远程仓库。但是对于某些视源代码如生命的商业公司来说，既不想公开源代码，又舍不得给GitHub交保护费，那就只能自己搭建一台Git服务器作为私有仓库使用。 搭建Git服务器需要准备一台运行Linux的机器，强烈推荐用Ubuntu或Debian，这样，通过几条简单的apt命令就可以完成安装。 假设你已经有sudo权限的用户账号，下面，正式开始安装。 第一步，安装git： 1$ sudo apt-get install git 第二步，创建一个git用户，用来运行git服务： 1$ sudo adduser git 第三步，创建证书登录： 收集所有需要登录的用户的公钥，就是他们自己的id_rsa.pub文件，把所有公钥导入到/home/git/.ssh/authorized_keys文件里，一行一个。 第四步，初始化Git仓库： 先选定一个目录作为Git仓库，假定是/srv/sample.git，在/srv目录下输入命令： 1$ sudo git init --bare sample.git Git就会创建一个裸仓库，裸仓库没有工作区，因为服务器上的Git仓库纯粹是为了共享，所以不让用户直接登录到服务器上去改工作区，并且服务器上的Git仓库通常都以.git结尾。然后，把owner改为git： 1$ sudo chown -R git:git sample.git 第五步，禁用shell登录： 出于安全考虑，第二步创建的git用户不允许登录shell，这可以通过编辑/etc/passwd文件完成。找到类似下面的一行： 1git:x:1001:1001:,,,:/home/git:/bin/bash 改为： 1git:x:1001:1001:,,,:/home/git:/usr/bin/git-shell 这样，git用户可以正常通过ssh使用git，但无法登录shell，因为我们为git用户指定的git-shell每次一登录就自动退出。 第六步，克隆远程仓库： 现在，可以通过git clone命令克隆远程仓库了，在各自的电脑上运行： 123$ git clone git@server:/srv/sample.gitCloning into &apos;sample&apos;...warning: You appear to have cloned an empty repository. 剩下的推送就简单了。 管理公钥如果团队很小，把每个人的公钥收集起来放到服务器的/home/git/.ssh/authorized_keys文件里就是可行的。如果团队有几百号人，就没法这么玩了，这时，可以用Gitosis来管理公钥。 这里我们不介绍怎么玩Gitosis了，几百号人的团队基本都在500强了，相信找个高水平的Linux管理员问题不大。 管理权限有很多不但视源代码如生命，而且视员工为窃贼的公司，会在版本控制系统里设置一套完善的权限控制，每个人是否有读写权限会精确到每个分支甚至每个目录下。因为Git是为Linux源代码托管而开发的，所以Git也继承了开源社区的精神，不支持权限控制。不过，因为Git支持钩子（hook），所以，可以在服务器端编写一系列脚本来控制提交等操作，达到权限控制的目的。Gitolite就是这个工具。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git总结(三)]]></title>
    <url>%2F2019%2F07%2F29%2Fgit%E6%80%BB%E7%BB%93(%E4%B8%89)%2F</url>
    <content type="text"><![CDATA[git总结(三)1.分支管理分支在实际中有什么用呢？假设你准备开发一个新功能，但是需要两周才能完成，第一周你写了50%的代码，如果立刻提交，由于代码还没写完，不完整的代码库会导致别人不能干活了。如果等代码全部写完再一次提交，又存在丢失每天进度的巨大风险。 现在有了分支，就不用怕了。你创建了一个属于你自己的分支，别人看不到，还继续在原来的分支上正常工作，而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，这样，既安全，又不影响别人工作。 其他版本控制系统如SVN等都有分支管理，但是用过之后你会发现，这些版本控制系统创建和切换分支比蜗牛还慢，简直让人无法忍受，结果分支功能成了摆设，大家都不去用。 但Git的分支是与众不同的，无论创建、切换和删除分支，Git在1秒钟之内就能完成！无论你的版本库是1个文件还是1万个文件。 2.创建与合并分支在版本回退里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 一开始的时候，master分支是一条线，Git用master指向最新的提交，再用HEAD指向master，就能确定当前分支，以及当前分支的提交点： 每次提交，master分支都会向前移动一步，这样，随着你不断提交，master分支的线也越来越长。 当我们创建新的分支，例如dev时，Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上： 你看，Git创建一个分支很快，因为除了增加一个dev指针，改改HEAD的指向，工作区的文件都没有任何变化！ 不过，从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变： 假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并： 所以Git合并分支也很快！就改改指针，工作区内容也不变！ 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支： 真是太神奇了，你看得出来有些提交是通过分支完成的吗？ 下面开始实战。 首先，我们创建dev分支，然后切换到dev分支： 12$ git checkout -b devSwitched to a new branch &apos;dev&apos; git checkout命令加上-b参数表示创建并切换，相当于以下两条命令： 123$ git branch dev$ git checkout devSwitched to branch &apos;dev&apos; 然后，用git branch命令查看当前分支： 123$ git branch* dev master git branch命令会列出所有分支，当前分支前面会标一个*号。 然后，我们就可以在dev分支上正常提交，比如对readme.txt做个修改，加上一行： 1Creating a new branch is quick. 然后提交： 1234$ git add readme.txt $ git commit -m &quot;branch test&quot;[dev b17d20e] branch test 1 file changed, 1 insertion(+) 现在，dev分支的工作完成，我们就可以切换回master分支： 12$ git checkout masterSwitched to branch &apos;master&apos; 切换回master分支后，再查看一个readme.txt文件，刚才添加的内容不见了！因为那个提交是在dev分支上，而master分支此刻的提交点并没有变： 现在，我们把dev分支的工作成果合并到master分支上： 12345$ git merge devUpdating d46f35e..b17d20eFast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) git merge命令用于合并指定分支到当前分支。合并后，再查看readme.txt的内容，就可以看到，和dev分支的最新提交是完全一样的。 注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。 当然，也不是每次合并都能Fast-forward，我们后面会讲其他方式的合并。 合并完成后，就可以放心地删除dev分支了： 12$ git branch -d devDeleted branch dev (was b17d20e). 删除后，查看branch，就只剩下master分支了： 12$ git branch* master 因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。 小结Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; 3.解决冲突人生不如意之事十之八九，合并分支往往也不是一帆风顺的。 准备新的feature1分支，继续我们的新分支开发： 12$ git checkout -b feature1Switched to a new branch &apos;feature1&apos; 修改readme.txt最后一行，改为： 1Creating a new branch is quick AND simple. 在feature1分支上提交： 12345$ git add readme.txt$ git commit -m &quot;AND simple&quot;[feature1 14096d0] AND simple 1 file changed, 1 insertion(+), 1 deletion(-) 切换到master分支： 1234$ git checkout masterSwitched to branch &apos;master&apos;Your branch is ahead of &apos;origin/master&apos; by 1 commit. (use &quot;git push&quot; to publish your local commits) Git还会自动提示我们当前master分支比远程的master分支要超前1个提交。 在master分支上把readme.txt文件的最后一行改为： 1Creating a new branch is quick &amp; simple. 提交： 1234$ git add readme.txt $ git commit -m &quot;&amp; simple&quot;[master 5dc6824] &amp; simple 1 file changed, 1 insertion(+), 1 deletion(-) 现在，master分支和feature1分支各自都分别有新的提交，变成了这样： 这种情况下，Git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突，我们试试看： 1234$ git merge feature1Auto-merging readme.txtCONFLICT (content): Merge conflict in readme.txtAutomatic merge failed; fix conflicts and then commit the result. 果然冲突了！Git告诉我们，readme.txt文件存在冲突，必须手动解决冲突后再提交。git status也可以告诉我们冲突的文件： 123456789101112131415$ git statusOn branch masterYour branch is ahead of &apos;origin/master&apos; by 2 commits. (use &quot;git push&quot; to publish your local commits)You have unmerged paths. (fix conflicts and run &quot;git commit&quot;) (use &quot;git merge --abort&quot; to abort the merge)Unmerged paths: (use &quot;git add &lt;file&gt;...&quot; to mark resolution) both modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 我们可以直接查看readme.txt的内容： 123456789Git is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1 Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改如下后保存： 1Creating a new branch is quick and simple. 再提交： 123$ git add readme.txt $ git commit -m &quot;conflict fixed&quot;[master cf810e4] conflict fixed 现在，master分支和feature1分支变成了下图所示： 用带参数的git log也可以看到分支的合并情况： 1234567891011121314$ git log --graph --pretty=oneline --abbrev-commit* cf810e4 (HEAD -&gt; master) conflict fixed|\ | * 14096d0 (feature1) AND simple* | 5dc6824 &amp; simple|/ * b17d20e branch test* d46f35e (origin/master) remove test.txt* b84166e add test.txt* 519219b git tracks changes* e43a48b understand how stage works* 1094adb append GPL* e475afc add distributed* eaadf4e wrote a readme file 最后，删除feature1分支： 12$ git branch -d feature1Deleted branch feature1 (was 14096d0). 工作完成。 小结当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。 用git log --graph命令可以看到分支合并图。 4.分支管理策略通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。 如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。 下面我们实战一下--no-ff方式的git merge： 首先，仍然创建并切换dev分支： 12$ git checkout -b devSwitched to a new branch &apos;dev&apos; 修改readme.txt文件，并提交一个新的commit： 1234$ git add readme.txt $ git commit -m &quot;add merge&quot;[dev f52c633] add merge 1 file changed, 1 insertion(+) 现在，我们切换回master： 12$ git checkout masterSwitched to branch &apos;master&apos; 准备合并dev分支，请注意--no-ff参数，表示禁用Fast forward： 1234$ git merge --no-ff -m &quot;merge with no-ff&quot; devMerge made by the &apos;recursive&apos; strategy. readme.txt | 1 + 1 file changed, 1 insertion(+) 因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。 合并后，我们用git log看看分支历史： 1234567$ git log --graph --pretty=oneline --abbrev-commit* e1e9c68 (HEAD -&gt; master) merge with no-ff|\ | * f52c633 (dev) add merge|/ * cf810e4 conflict fixed... 可以看到，不使用Fast forward模式，merge后就像这样： 分支策略在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活； 那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本； 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。 所以，团队合作的分支看起来就像这样： 小结Git分支十分强大，在团队开发中应该充分应用。 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 5.Bug分支软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交： 123456789101112$ git statusOn branch devChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: hello.pyChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txt 并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？ 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作： 12$ git stashSaved working directory and index state WIP on dev: f52c633 add merge 现在，用git status查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。 首先确定要在哪个分支上修复bug，假定需要在master分支上修复，就从master创建临时分支： 1234567$ git checkout masterSwitched to branch &apos;master&apos;Your branch is ahead of &apos;origin/master&apos; by 6 commits. (use &quot;git push&quot; to publish your local commits)$ git checkout -b issue-101Switched to a new branch &apos;issue-101&apos; 现在修复bug，需要把“Git is free software …”改为“Git is a free software …”，然后提交： 1234$ git add readme.txt $ git commit -m &quot;fix bug 101&quot;[issue-101 4c805e2] fix bug 101 1 file changed, 1 insertion(+), 1 deletion(-) 修复完成后，切换到master分支，并完成合并，最后删除issue-101分支： 123456789$ git checkout masterSwitched to branch &apos;master&apos;Your branch is ahead of &apos;origin/master&apos; by 6 commits. (use &quot;git push&quot; to publish your local commits)$ git merge --no-ff -m &quot;merged bug fix 101&quot; issue-101Merge made by the &apos;recursive&apos; strategy. readme.txt | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) 太棒了，原计划两个小时的bug修复只花了5分钟！现在，是时候接着回到dev分支干活了！ 123456$ git checkout devSwitched to branch &apos;dev&apos;$ git statusOn branch devnothing to commit, working tree clean 工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看： 12$ git stash liststash@&#123;0&#125;: WIP on dev: f52c633 add merge 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法： 一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； 另一种方式是用git stash pop，恢复的同时把stash内容也删了： 1234567891011121314$ git stash popOn branch devChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: hello.pyChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtDropped refs/stash@&#123;0&#125; (5d677e2ee266f39ea296182fb2354265b91b3b2a) 再用git stash list查看，就看不到任何stash内容了： 1$ git stash list 你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令： 1$ git stash apply stash@&#123;0&#125; 小结修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除； 当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场。 6.Feature分支开发一个新feature，最好新建一个分支； 如果要丢弃一个没有被合并过的分支，可以通过git branch -D &lt;name&gt;强行删除。 7.多人协作因此，多人协作的工作模式通常是这样： 首先，可以试图用git push origin &lt;branch-name&gt;推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin &lt;branch-name&gt;推送就能成功！ 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;。 这就是多人协作的工作模式，一旦熟悉了，就非常简单。 小结 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 git pull有冲突如何解决在使用git pull代码时，经常会碰到有冲突的情况，提示如下信息： error: Your local changes to ‘c/environ.c’ would be overwritten by merge. Aborting.Please, commit your changes or stash them before you can merge. 这个意思是说更新下来的内容和本地修改的内容有冲突，先提交你的改变或者先将本地修改暂时存储起来。 处理的方式非常简单，主要是使用git stash命令进行处理，分成以下几个步骤进行处理。 1、先将本地修改存储起来 $ git stash 这样本地的所有修改就都被暂时存储起来 。 其中stash@{0}就是刚才保存的标记。 2、pull内容 暂存了本地修改之后，就可以pull了。 $ git pull 3、还原暂存的内容 $ git stash pop stash@{0} 系统提示如下类似的信息： Auto-merging c/environ.cCONFLICT (content): Merge conflict in c/environ.c 意思就是系统自动合并修改的内容，但是其中有冲突，需要解决其中的冲突。 4、解决文件中冲突的的部分 打开冲突的文件，其中Updated upstream 和=====之间的内容就是pull下来的内容，====和stashed changes之间的内容就是本地修改的内容。碰到这种情况，git也不知道哪行内容是需要的，所以要自行确定需要的内容。 解决完成之后，就可以正常的提交了。 8.Rebase rebase操作可以把本地未push的分叉提交历史整理成直线； rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。 特别强调需要注意的使用情形和原则： 只对尚未推送或分享给别人的本地修改执行变基操作清理历史，从不对已推送至别处的提交执行变基操作。 因为rebase会改变提交历史记录，这会影响到别人使用这一远程仓库。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git总结(二)]]></title>
    <url>%2F2019%2F07%2F29%2Fgit%E6%80%BB%E7%BB%93(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[git总结(二)1.远程仓库完全可以自己搭建一台运行Git的服务器，不过现阶段，为了学Git先搭个服务器绝对是小题大作。好在这个世界上有个叫GitHub的神奇的网站，从名字就可以看出，这个网站就是提供Git仓库托管服务的，所以，只要注册一个GitHub账号，就可以免费获得Git远程仓库。 在继续阅读后续内容前，请自行注册GitHub账号。由于你的本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以，需要一点设置： 第1步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Key： 1$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可，由于这个Key也不是用于军事目的，所以也无需设置密码。 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 第2步：登陆GitHub，打开“Account settings”，“SSH Keys”页面： 然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容： 点“Add Key”，你就应该看到已经添加的Key： 为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。 当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。 最后友情提示，在GitHub上免费托管的Git仓库，任何人都可以看到喔（但只有你自己才能改）。所以，不要把敏感信息放进去。 如果你不想让别人看到Git库，有两个办法，一个是交点保护费，让GitHub把公开的仓库变成私有的，这样别人就看不见了（不可读更不可写）。另一个办法是自己动手，搭一个Git服务器，因为是你自己的Git服务器，所以别人也是看不见的。这个方法我们后面会讲到的，相当简单，公司内部开发必备。 确保你拥有一个GitHub账号后，我们就即将开始远程仓库的学习。 2.添加远程库现在的情景是，你已经在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作，真是一举多得。 首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库： 在Repository name填入learngit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库： 目前，在GitHub上的这个learngit仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。 现在，我们根据GitHub的提示，在本地的learngit仓库下运行命令： 1$ git remote add origin git@github.com:michaelliao/learngit.git 请千万注意，把上面的michaelliao替换成你自己的GitHub账户名，否则，你在本地关联的就是我的远程库，关联没有问题，但是你以后推送是推不上去的，因为你的SSH Key公钥不在我的账户列表中。 添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。 下一步，就可以把本地库的所有内容推送到远程库上： 12345678910$ git push -u origin masterCounting objects: 20, done.Delta compression using up to 4 threads.Compressing objects: 100% (15/15), done.Writing objects: 100% (20/20), 1.64 KiB | 560.00 KiB/s, done.Total 20 (delta 5), reused 0 (delta 0)remote: Resolving deltas: 100% (5/5), done.To github.com:michaelliao/learngit.git * [new branch] master -&gt; masterBranch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;origin&apos;. 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 推送成功后，可以立刻在GitHub页面中看到远程库的内容已经和本地一模一样： 从现在起，只要本地作了提交，就可以通过命令： 1$ git push origin master 把本地master分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！ SSH警告当你第一次使用Git的clone或者push命令连接GitHub时，会得到一个警告： 123The authenticity of host &apos;github.com (xx.xx.xx.xx)&apos; can&apos;t be established.RSA key fingerprint is xx.xx.xx.xx.xx.Are you sure you want to continue connecting (yes/no)? 这是因为Git使用SSH连接，而SSH连接在第一次验证GitHub服务器的Key时，需要你确认GitHub的Key的指纹信息是否真的来自GitHub的服务器，输入yes回车即可。 Git会输出一个警告，告诉你已经把GitHub的Key添加到本机的一个信任列表里了： 1Warning: Permanently added &apos;github.com&apos; (RSA) to the list of known hosts. 这个警告只会出现一次，后面的操作就不会有任何警告了。 如果你实在担心有人冒充GitHub服务器，输入yes前可以对照GitHub的RSA Key的指纹信息是否与SSH连接给出的一致。 小结要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git； 关联后，使用命令git push -u origin master第一次推送master分支的所有内容； 此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改； 分布式版本系统的最大好处之一是在本地工作完全不需要考虑远程库的存在，也就是有没有联网都可以正常工作，而SVN在没有联网的时候是拒绝干活的！当有网络的时候，再把本地提交推送一下就完成了同步，真是太方便了！ 3.从远程库克隆上次我们讲了先有本地库，后有远程库的时候，如何关联远程库。 现在，假设我们从零开发，那么最好的方式是先创建远程库，然后，从远程库克隆。 首先，登陆GitHub，创建一个新的仓库，名字叫gitskills： 我们勾选Initialize this repository with a README，这样GitHub会自动为我们创建一个README.md文件。创建完毕后，可以看到README.md文件： 现在，远程库已经准备好了，下一步是用命令git clone克隆一个本地库： 12345$ git clone git@github.com:michaelliao/gitskills.gitCloning into &apos;gitskills&apos;...remote: Counting objects: 3, done.remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3Receiving objects: 100% (3/3), done. 注意把Git库的地址换成你自己的，然后进入gitskills目录看看，已经有README.md文件了： 123$ cd gitskills$ lsREADME.md 如果有多个人协作开发，那么每个人各自从远程克隆一份就可以了。 你也许还注意到，GitHub给出的地址不止一个，还可以用https://github.com/michaelliao/gitskills.git这样的地址。实际上，Git支持多种协议，默认的git://使用ssh，但也可以使用https等其他协议。 使用https除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放http端口的公司内部就无法使用ssh协议而只能用https。 小结要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。 Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git总结(一)]]></title>
    <url>%2F2019%2F07%2F29%2Fgit%E6%80%BB%E7%BB%93(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[git总结(一)1.创建版本库初始化一个Git仓库，使用git init命令。 添加文件到Git仓库，分两步： 使用命令git add &lt;file&gt;，注意，可反复多次使用，添加多个文件； 使用命令git commit -m &lt;message&gt;，完成。 2.时光机穿梭 要随时掌握工作区的状态，使用git status命令。 如果git status告诉你有文件被修改过，用git diff可以查看修改内容。 3.版本回退 HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset --hard commit_id。 在Git中，用HEAD表示当前版本，也就是最新的提交1094adb...（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 现在，我们要把当前版本append GPL回退到上一个版本add distributed，就可以使用git reset命令： 12$ git reset --hard HEAD^HEAD is now at e475afc add distributed 穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本。 要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。 4.工作区和暂存区 git diff比较的是工作目录中当前文件和暂存区域快照之间的差异， 也就是修改之后还没有暂存起来的变化内容。若要查看已暂存的将要添加到下次提交里的内容，可以用 git diff —cached 命令。 请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。 所以有时候你一下子暂存了所有更新过的文件后，运行 git diff 后却什么也没有，就是这个原因。 5.管理修改为什么说Git管理的是修改，而不是文件呢？我们还是做实验。第一步，对readme.txt做一个修改，比如加一行内容： 12345$ cat readme.txtGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes. 然后，添加： 12345678$ git add readme.txt$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)## modified: readme.txt# 然后，再修改readme.txt： 12345$ cat readme.txt Git is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files. 提交： 123$ git commit -m &quot;git tracks changes&quot;[master 519219b] git tracks changes 1 file changed, 1 insertion(+) 提交后，再看看状态： 123456789$ git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 咦，怎么第二次的修改没有被提交？ 别激动，我们回顾一下操作过程： 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git commit 你看，我们前面讲了，Git管理的是修改，当你用git add命令后，在工作区的第一次修改被放入暂存区，准备提交，但是，在工作区的第二次修改并没有放入暂存区，所以，git commit只负责把暂存区的修改提交了，也就是第一次的修改被提交了，第二次的修改不会被提交。 提交后，用git diff HEAD -- readme.txt命令可以查看工作区和版本库里面最新版本的区别： 1234567891011$ git diff HEAD -- readme.txt diff --git a/readme.txt b/readme.txtindex 76d770f..a9c5755 100644--- a/readme.txt+++ b/readme.txt@@ -1,4 +1,4 @@ Git is a distributed version control system. Git is free software distributed under the GPL. Git has a mutable index called stage.-Git tracks changes.+Git tracks changes of files. 那怎么提交第二次修改呢？你可以继续git add再git commit，也可以别着急提交第一次修改，先git add第二次修改，再git commit，就相当于把两次修改合并后一块提交了： 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git add -&gt; git commit 每次修改，如果不用git add到暂存区，那就不会加入到commit中。 6.撤销修改场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。 7.删除文件在Git中，删除也是一个修改操作，我们实战一下，先添加一个新文件test.txt到Git并且提交： 123456$ git add test.txt$ git commit -m &quot;add test.txt&quot;[master b84166e] add test.txt 1 file changed, 1 insertion(+) create mode 100644 test.txt 一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用rm命令删了： 1$ rm test.txt 这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了： 123456789$ git statusOn branch masterChanges not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) deleted: test.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： 1234567$ git rm test.txtrm &apos;test.txt&apos;$ git commit -m &quot;remove test.txt&quot;[master d46f35e] remove test.txt 1 file changed, 1 deletion(-) delete mode 100644 test.txt 现在，文件就从版本库中被删除了。 小提示：先手动删除文件，然后使用git rm 和git add效果是一样的。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1$ git checkout -- test.txt git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 注意：从来没有被添加到版本库就被删除的文件，是无法恢复的！ 小结命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux最常用命令速查]]></title>
    <url>%2F2019%2F07%2F25%2FLinux%E6%9C%80%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[Linux最常用命令速查1.Is命令1234567891011121314151617181920212223242526272829303132333435#Is命令是列出目录内容（List Directory Contents）的意思。运行它就是列出文件夹里的内容，可能是文件也可能是文件夹。root@tecmint:~# lsAndroid-Games MusicPictures PublicDesktop Tecmint.comDocuments TecMint-SyncDownloads Templates#“Is-I”命令以详情模式（long listing fashion）列出文件夹的内容。root@tecmint:~# ls -ltotal 40588drwxrwxr-x 2 ravisaive ravisaive 4096 May 8 01:06 Android Gamesdrwxr-xr-x 2 ravisaive ravisaive 4096 May 15 10:50 Desktopdrwxr-xr-x 2 ravisaive ravisaive 4096 May 16 16:45 Documentsdrwxr-xr-x 6 ravisaive ravisaive 4096 May 16 14:34 Downloadsdrwxr-xr-x 2 ravisaive ravisaive 4096 Apr 30 20:50 Musicdrwxr-xr-x 2 ravisaive ravisaive 4096 May 9 17:54 Picturesdrwxrwxr-x 5 ravisaive ravisaive 4096 May 3 18:44 Tecmint.comdrwxr-xr-x 2 ravisaive ravisaive 4096 Apr 30 20:50 Templates#"ls -a"命令会列出文件夹里的所有内容，包括以"."开头的隐藏文件。root@tecmint:~# ls -a. .gnupg .dbus .goutputstream-PI5VVW.adobe deja-dup .grsync .mozilla#在Linux中，文件以“.”开头的就是隐藏文件，并且每个文件，文件夹，设备或者命令都是以文件对待。ls -l 命令输出：d (代表了是目录).rwxr-xr-x 是文件或者目录对所属用户，同一组用户和其它用户的权限。上面例子中第一个ravisaive 代表了文件文件属于用户ravisaive上面例子中的第二个ravisaive代表了文件文件属于用户组ravisaive4096 代表了文件大小为4096字节.May 8 01:06 代表了文件最后一次修改的日期和时间.最后面的就是文件/文件夹的名字 2.lsblk命令lsblk是最有用和最简单的方式来了解新插入的USB设备的名字，特别是当你在终端上处理磁盘/块设备时。** “lsblk“就是列出块设备。除了RAM外，以标准的树状输出格式，整齐地显示块设备。 1234567891011root@tecmint:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 232.9G 0 disk ├─sda1 8:1 0 46.6G 0 part /├─sda2 8:2 0 1K 0 part ├─sda5 8:5 0 190M 0 part /boot├─sda6 8:6 0 3.7G 0 part [SWAP]├─sda7 8:7 0 93.1G 0 part /data└─sda8 8:8 0 89.2G 0 part /personalsr0 11:0 1 1024M 0 rom “lsblk -l”命令以列表格式显示块设备(而不是树状格式)。 1234567891011root@tecmint:~# lsblk -lNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 232.9G 0 disk sda1 8:1 0 46.6G 0 part /sda2 8:2 0 1K 0 part sda5 8:5 0 190M 0 part /bootsda6 8:6 0 3.7G 0 part [SWAP]sda7 8:7 0 93.1G 0 part /datasda8 8:8 0 89.2G 0 part /personalsr0 11:0 1 1024M 0 rom 3.dd命令“dd”命令代表了转换和复制文件。可以用来转换和复制文件，大多数时间是用来复制iso文件(或任何其它文件)到一个usb设备(或任何其它地方)中去，所以可以用来制作USB启动器。 1root@tecmint:~# dd if=/home/user/Downloads/debian.iso of=/dev/sdb1 bs=512M; sync 注意：在上面的例子中，usb设备就是sdb1（你应该使用lsblk命令验证它，否则你会重写你的磁盘或者系统），请慎重使用磁盘的名，切忌。 4.uname命令“uname“命令就是Unix Name的简写。显示机器名，操作系统和内核的详细信息。 123root@tecmint:~# uname -aLinux tecmint 3.8.0-19-generic #30-Ubuntu SMP Wed May 1 16:36:13 UTC 2013 i686 i686 i686 GNU/Linux 5.history命令“history”命令就是历史记录。它显示了在终端中所执行过的所有命令的历史。 123456789101112131415root@tecmint:~# history 1 sudo add-apt-repository ppa:tualatrix/ppa 2 sudo apt-get update 3 sudo apt-get install ubuntu-tweak 4 sudo add-apt-repository ppa:diesch/testing 5 sudo apt-get update 6 sudo apt-get install indicator-privacy 7 sudo add-apt-repository ppa:atareao/atareao 8 sudo apt-get update 9 sudo apt-get install my-weather-indicator 10 pwd 11 cd &amp;&amp; sudo cp -r unity/6 /usr/share/unity/ 12 cd /usr/share/unity/icons/ 13 cd /usr/share/unity 注意：按住“CTRL + R”就可以搜索已经执行过的命令，它可以在你写命令时自动补全。 1(reverse-i-search)`if&apos;: ifconfig 6.sudo命令“sudo”(super user do)命令允许授权用户执行超级用户或者其它用户的命令。通过在sudoers列表的安全策略来指定。 1root@tecmint:~# sudo add-apt-repository ppa:tualatrix/ppa 注意：sudo 允许用户借用超级用户的权限，然而”su“命令实际上是允许用户以超级用户登录。所以s**udo比su更安全。并不建议使用sudo或者su**来处理日常用途，因为它可能导致严重的错误如果你意外的做错了事 7.mkdir命令“mkdir”(Make directory)命令在命名路径下创建新的目录。然而如果目录已经存在了，那么它就会返回一个错误信息”不能创建文件夹，文件夹已经存在了”(“cannot create folder, folder already exists”) 1root@tecmint:~# mkdir tecmint 注意：目录只能在用户拥有写权限的目录下才能创建。mkdir：不能创建目录tecmint，因为文件已经存在了。（上面的输出中不要被文件迷惑了，你应该记住我开头所说的-在linux中，文件，文件夹，驱动，命令，脚本都视为文件） 8.touch命令“touch”命令代表了将文件的访问和修改时间更新为当前时间。touch命令只会在文件不存在的时候才会创建它。如果文件已经存在了，它会更新时间戳，但是并不会改变文件的内容。 1root@tecmint:~# touch tecmintfile 注意：touch 可以用来在用户拥有写权限的目录下创建不存在的文件。 9.chmod命令“chmod”命令就是改变文件的模式位。chmod会根据要求的模式来改变每个所给的文件，文件夹，脚本等等的文件模式（权限）。 在文件(文件夹或者其它，为了简单起见，我们就使用文件)中存在3中类型的权限 123Read (r)=4Write(w)=2Execute(x)=1 所以如果你想给文件只读权限，就设置为’4’;只写权限，设置权限为’2’;只执行权限，设置为1; 读写权限，就是4+2 = 6, 以此类推。 现在需要设置3种用户和用户组权限。第一个是拥有者，然后是用户所在的组，最后是其它用户。 1rwxr-x--x abc.sh 这里root的权限是 rwx（读写和执行权限），所属用户组权限是 r-x (只有读和执行权限, 没有写权限)，对于其它用户权限是 -x(只有只执行权限) 为了改变它的权限，为拥有者，用户所在组和其它用户提供读，写，执行权限。 1root@tecmint:~# chmod 777 abc.sh 三种都只有读写权限 1root@tecmint:~# chmod 666 abc.sh 拥有者用户有读写和执行权限，用户所在的组和其它用户只有可执行权限 1root@tecmint:~# chmod 711 abc.sh 注意：对于系统管理员和用户来说，这个命令是最有用的命令之一了。在多用户环境或者服务器上，对于某个用户，如果设置了文件不可访问，那么这个命令就可以解决，如果设置了错误的权限，那么也就提供了为授权的访问。 10.chown命令“chown”命令就是改变文件拥有者和所在用户组。每个文件都属于一个用户组和一个用户。在你的目录下，使用”ls -l“,你就会看到像这样的东西。 1234root@tecmint:~# ls -l drwxr-xr-x 3 server root 4096 May 10 11:14 Binary drwxr-xr-x 2 server server 4096 May 13 09:42 Desktop 在这里，目录Binary属于用户”server“,和用户组”root“,而目录”Desktop“属于用户“server”和用户组”server“ “chown”命令用来改变文件的所有权，所以仅仅用来管理和提供文件的用户和用户组授权。 1234root@tecmint:~# chown server:server Binarydrwxr-xr-x 3 server server 4096 May 10 11:14 Binary drwxr-xr-x 2 server server 4096 May 13 09:42 Desktop 注意：“chown”所给的文件改变用户和组的所有权到新的拥有者或者已经存在的用户或者用户组。 11.tar命令“tar”命令是磁带归档(Tape Archive)，对创建一些文件的的归档和它们的解压很有用。 123456789101112131415root@tecmint:~# tar -zxvf abc.tar.gz (记住'z'代表了.tar.gz)root@tecmint:~# tar -jxvf abc.tar.bz2 (记住'j'代表了.tar.bz2)root@tecmint:~# tar -cvf archieve.tar.gz(.bz2) /path/to/folder/abc#tar -rvf sysconfig.tar /etc/sysconfig/命令解释：将目录/etc/sysconfig/目录下的文件添加到文件sysconfig.tar文件中去。参数解释如下：-r 表示增加文件，把要增加的文件追加在压缩文件的末尾。#tar -tvf sysconfig.tar命令解释：查看压缩文件sysconfig.tar文件里面的内容参数解释如下：-t 表示查看文件，查看文件中的文件内容#tar -xvf sysconfig.tar命令解释：解压文件sysconfig.tar，将压缩文件sysconfig.tar文件解压到当前文件夹内。参数解释如下：-x 解压文件。 注意： “tar.gz“代表了使用gzip归档，“bar.bz2”使用bzip压缩的，它压缩的更好但是也更慢。 12.date命令“date”命令使用标准的输出打印当前的日期和时间，也可以深入设置。 123456root@tecmint:~# dateFri May 17 14:13:29 IST 2013root@tecmint:~# date --set=&apos;14 may 2013 13:57&apos; Mon May 13 13:57:00 IST 2013 注意：这个命令在脚本中十分有用，以及基于时间和日期的脚本更完美。而且在终端中改变日期和时间，让你更专业！！！（当然你需要root权限才能操作这个，因为它是系统整体改变） 13.cat命令“cat”代表了连结（Concatenation），连接两个或者更多文本文件或者以标准输出形式打印文件的内容。 12345root@tecmint:~# cat a.txt b.txt c.txt d.txt abcd.txtroot@tecmint:~# cat abcd.txt....contents of file abcd... 注意：“&gt;&gt;”和“&gt;”调用了追加符号。它们用来追加到文件里，而不是显示在标准输出上。“&gt;”符号会删除已存在的文件，然后创建一个新的文件。所以因为安全的原因，建议使用“&gt;&gt;”，它会写入到文件中，而不是覆盖或者删除。 这里就是常用通配符列表： 12345678Wildcard Matches * 零个或者更多字符 ? 恰好一个字符[abcde] 恰好列举中的一个字符 [a-e] 恰好在所给范围中的一个字符[!abcde] 任何字符都不在列举中 [!a-e] 任何字符都不在所给的范围中&#123;debian,linux&#125; 恰好在所给选项中的一整个单词 14.cp 命令“copy”就是复制。它会从一个地方复制一个文件到另外一个地方。 1root@tecmint:~# cp /home/user/Downloads abc.tar.gz /home/user/Desktop (Return 0 when sucess) 注意： cp，在shell脚本中是最常用的一个命令，而且它可以使用通配符（在前面一块中有所描述），来定制所需的文件的复制。 15.mv 命令“mv”命令将一个地方的文件移动到另外一个地方去。 1root@tecmint:~# mv /home/user/Downloads abc.tar.gz /home/user/Desktop (Return 0 when sucess) 注意：mv 命令可以使用通配符。mv需谨慎使用，因为移动系统的或者未授权的文件不但会导致安全性问题，而且可能系统崩溃。 16.pwd 命令“pwd”（print working directory），在终端中显示当前工作目录的全路径。 123root@tecmint:~# pwd /home/user/Desktop 注意： 这个命令并不会在脚本中经常使用，但是对于新手，当从连接到nux很久后在终端中迷失了路径，这绝对是救命稻草。 17.cd 命令最后，经常使用的“cd”命令代表了改变目录。它在终端中改变工作目录来执行，复制，移动，读，写等等操作。 1234root@tecmint:~# cd /home/user/Desktopserver@localhost:~$ pwd/home/user/Desktop 注意： 在终端中切换目录时，cd就大显身手了。“cd ～”会改变工作目录为用户的家目录，而且当用户发现自己在终端中迷失了路径时，非常有用。“cd ..”从当前工作目录切换到(当前工作目录的)父目录。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5G物理层系统设计架构及关键技术]]></title>
    <url>%2F2019%2F07%2F17%2F5G%E7%89%A9%E7%90%86%E5%B1%82%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[5G物理层系统设计架构及关键技术1.5G的物理层系统设计呈现如下特点1.OFDM加MIMO技术作为物理层设计的基础​ 这两技术的结合可以有效地利用系统宽带和无线链路空间特性，是提升系统频谱效率及峰值速率最有效的技术。 2.采用更加灵活的基础系统架构设计1.灵活的帧结构设计 2.灵活的双工设计 3.一体化的大规模天线设计​ 这里面关键的是波束赋形即beamforming技术，尤其是hybrid beamforming，因NR的设计需高达100Ghz的频谱范围，而随着频率的升高，天线系统使用的天线数目也随之增加，但单天线的有效覆盖距离受loss影响会快速降低。而beamforming可以有效提升大规模天线的覆盖距离和传输速率，因此是NR大规模天线设计的核心。 4.采用多项新技术​ 最具有代表性的新技术就是新空口采用数据信道LDPC码，控制信道Polar码的组合，替代了原来数据信道Turbo码，控制信道TBCC码的组合。具体优点如下: LDPC码相对Turbo码具有更低的编码复杂度和译码时延，能更好地支持大数据的传输。而Polar码在小数据包的性能优势将有效提升新空口的覆盖性能 2.物理层关键技术1.参数集和帧结构​ NR支持多种参数集，多个子载波间隔由一个基本的子载波间隔乘以一个整数N来得出，支持从15KHz到240KHz的子载波间隔。而LTE只支持15KHz的载波间隔，NR有了更多的选择。 ​ 因NR相对于LTE的一个重要的任务是要支持更高频率的使用。NR将支持高达100GHz频段的数据传输。NR的帧结构设计以时隙(slot)为基础进行，每个slot包含14个符号，且支持基于Mini-slot的数据发送，mini-slot的长度可以从1~13个符号。 2.基本波形​ 在5G NR设计中，OFDM(多载波技术的典型代表)仍然是基本波形。NR的设计中上下行都将支持CP=OFDM，意味着上下行采用相同的波形，当上下行发生相互干扰时，为采用更先进的接收机进行干扰删除提供了可能。 3.多址接入​ NR的多用户接入，尤其是针对传统的移动宽带增强业务(eMBB)主要基于正交多址技术。 4.调制编码​ 信道编码是5G设计最基础的部分。最终数据信道采用的是LDPC码，因其相对Turbo码和Polar码在大数据包的处理上具有比较明显的优势，尤其在高码率区域。其性能和译码时延优势更加突出。而控制信道最终采用的是Polar码，因其在小包传输上的卓越性能。 5.BWP​ BWP定义为一个载波内连续的多个资源块(RB,Resource block)的组合。引入BWP主要是为了UE可以更好地使用大的载波带宽。因对于一个大的载波带宽，比如100Mhz,一个UE需要使用的带宽往往非常有限，若UE实时进行全带宽的检测和维护，终端的性能将带来极大挑战。BWP就是在整个大的载波内划出部分带宽给UE进行接入和数据传输，UE只需在这部分带宽内进行相应的操作，从而达到节能的效果。 6.前向兼容性​ 这是NR提出的一个新的概念，主要是为了保证对未来新业务和新特性的引入，也能保证对相同频谱上的已开展业务的有效支持。主要做法是在给UE的RRC信令中预留一部分资源以便将来的版本使用。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5G无线接口]]></title>
    <url>%2F2019%2F07%2F16%2F5G%E6%97%A0%E7%BA%BF%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[5G无线接口​ 5G系统的无线接口继承了LTE系统的说法，即把终端和接入网之间的接口仍简称为Uu接口，也称为空中接口。无线接口协议主要是用来建立，重配置和释放各种无线承载业务的。在5G新空口技术中，无线接口是终端和gNB之间的接口。 ​ 无线接口协议栈主要分三层，两面。三层包括物理层(L1),数据链路层(L2),网络层(L3).两面指控制平面和用户平面 1.物理层​ 物理层提供物理介质中比特流传输所需要的所有功能，且为MAC层和高层提供信息传输的服务(用传输信道来描述). 1.DL传输信道类型​ 只有3种，相比LTE少了多播信道。 广播信道(BCH,Broadcast Channel) 采用固定的预定义传输格式 下行共享信道(DL-SCH,Downlink Shared Channel) 使用HARQ传输，能够调整传输使用的调制方式，编码速率和发送功率来实现链路自适应，能够在整个小区内发送或使用波束赋形发送，支持终端非连续接收以达到节电的目的 寻呼信道(PCH,Paging Channel) 支持终端非连续接收以达到节电的目的 2.UL传输信道类型​ 分为2种 上行共享信道(UL-SCH,Uplink Shared Channel) 随机接入信道(RACH,Random Access Channel) 3.传输信道到物理信道映射NR定义的物理信道包括以下内容: 物理广播信道(PBCH) 物理下行链路控制信道(PDCCH) 物理下行链路共享信道(PDSCH) 物理随机接入信道(PRACH) 物理上行链路控制信道(PUCCH) 物理上行链路共享信道(PUSCH) 2.数据链路层​ 数据链路层包括媒体接入控制(MAC,Medium Access Control),无线链路控制(RLC,Radio Link Control)，分组数据汇聚协议(PDCP,Packet Data Convergence Protocol)和服务数据调整协议(SDAP,Service Data Adaptation Protocol)4个子层。 ​ 相比于LTE,NR额外引入了SDAP，SDAP层位于用户面，而其它数据链路层的3个子层同时位于控制平面和用户平面。SDAP层在控制平面负责无线承载信令的传输，加密和完整性保护，在用户平面负责用户业务数据的传输和加密。网络层是指无线资源控制(RRC,Radio Resources Control)层，位于接入网的控制平面，负责完成接入网和终端之间交互的所有信令处理。 ​ 下面为上下行数据链路层架构，其中层与层之间的连接点称之为服务接入点(SAP,Service Access Point)。 3.RRC层RRC协议模块的功能如下 发送系统信息广播(NAS层相关和AS层相关)消息 发送由核心网5GC和接入网NG-RAN发起的寻呼消息 UE和NG-RAN之间的RRC连接的建立，维护和释放 安全功能密钥管理 无线承载管理(包括建立，配置，维护和释放信令无线承载和用户无线承载) 移动性管理(包括切换，UE小区选择和重选，切换时候上下文传输) Qos管理 UE测量报告和控制 无线链路失败的检测和恢复 NAS消息的传输 其中，RRC的协议状态为3个：RRC空闲状态和非激活状态以及连接状态，特征如下]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++易忘但重要的知识点(二)]]></title>
    <url>%2F2019%2F07%2F11%2FC%2B%2B%E6%98%93%E5%BF%98%E7%9F%A5%E8%AF%86%E7%82%B92%2F</url>
    <content type="text"><![CDATA[1.new/delete1.newnew分配内存空间时，不能保证按需分配，分配内存空间大小可能会大于所需空间大小。因此，new会分配至少申请大小的内存空间。 A、开辟单变量地址空间 12int *p=new int；//开辟大小至少为sizeof（int）空间int*a=new int（5）；//开辟大小至少为sizeof（int）空间，并初始化为5 B、开辟数组空间 123一维：int*a=new int[100]；/开辟一个大小不少于400字节的整型数组空间二维：int（*a）[6]=new int[5][6]三维：int（*a）[5][6]=new int[3][5][6] 2.deleteA、释放单变量空间 12int*a=new int；delete a；//释放单个int的空间 B、释放数组空间 12int*a=new int[5]；delete[]a；//释放int数组空间 3.new/delete应用1234567891011121314151617181920212223242526int *p=new int(5); cout&lt;&lt;*p&lt;&lt;end1; deletep; char *pp=new char[10]; strcpy(pp,"china"); cout&lt;&lt;pp&lt;&lt;end1; delete []pp; string *ps=new string("china"); cout&lt;&lt;*ps&lt;&lt;end1;//cout&lt;&lt;ps&lt;&lt;end1; delete ps; char **pa=new char*[5]; memset(pa,0,sizeof(char*[5])); pa[o]="china"; pa[1]="america";char **pt=pa; while(*pt)&#123; cout&lt;&lt;*pt++&lt;&lt;end1;&#125;delete[]pt; int(*qg)[3][4]=new int[2][3][4]; delete []qq; 4.注意事项C++中堆空间的分配和释放注意事项如下：A、new/delete是关键字，效率高于malloc和free. B、配对使用，避免内存泄漏和多重释放。C、避免交叉使用。比如malloc申请的空间去delete，new出的空间被free； D、重点用在类对像的申请与释放。申请的时候会调用构造器完成初始化，释放的时候，会调用析构器完成内存的清理。 5.malloc与new的区别malloc 与new的区别如下A、new是C++关键字，malloc是C语言库函数 B、new以具体类型为单位进行内存分配，malloc以字节位单位分配内存 C、new在申请单个类型变量时可以进行初始化，malloc不具备 D、new在所有C++编译器中都支持，malloc 在某些系统开发中不可调用 E、new 能够触发构造函数的调用，malloc仅分配需要的内存空间 F、对象的创建只能使用new，malloc不适合面向对象开发 6.free与delete的区别free与delete的区别如下：A、delete 是C++关键，free是库函数 B、delete在所有C++编译器中都支持，free在某些系统开发中不可调用 C.delete能够触发析构函数的调用，free仅归还分配的内存空间 D、对象的销毁只能使用delete，free不适合面向对象开发 E、free可以归还new申请的内存空间，但不会调用析构函数，可能会造成内存泄漏 F、delete可以释放malloc分配的内存空间，但会调用析构函数，会造成其他问题。 2.命名空间1.命名空间简介​ C语言中，只有一个全局作用域，所有的全局标识符共享一个作用域，因此标识符之间可能存在冲突。​ C++语言中，提出了命名空间的概念。命名空间将全局作用域分为不同的部分，不同命令空间中的标识符可以重名而不会发生冲突，命名空间可以嵌套。全局作用域即默认命名空间。 2.默认命名空间​ global scope是一个程序中最大的scope，是引起命名冲突的根源。C语言没有从语言层面提供命名空间机制来解决。global scope是无名的命名空间。 3.命名空间的划分​ Namespace是对全局区域的再次划分，命名空间的声明如下: 123456namespace NAMESPACE&#123; 全局变量 int a; 数据类型 struct Stu&#123;&#125;; 函数 void func()；&#125; 4.命名空间的使用方法​ 直接指定命名空间：NameSpace:：a=5； ​ 使用using+命名空间+空间元素：using NameSpace:a；a=2000； ​ 使用using +namespace+命名空间； 5.命名空间使用示例1234567891011121314151617181920212223242526272829#include &lt;ioatream&gt;using namespace std; namespace MySpace&#123; int x=1; int y=2;&#125;namespace Other&#123; int x=3; int y=4;&#125;int main()&#123; &#123; using namespace MySpace; cout&lt;&lt;x&lt;&lt;y&lt;&lt;endl; &#125;&#123; using namespace other; cout&lt;&lt;x&lt;&lt;y&lt;&lt;end1; &#125;&#123; MySpace::x=100; other::y=200; cout&lt;&lt;MySpace::x&lt;&lt;Other::y&lt;&lt;end1; &#125; return 0;&#125; 可以使用块语句将命名空间限定在语句内部。 3.c++语言的封装1.C++语言中，可以对类的成员变量和成员函数定义访问级别public：使用public关键字声明的类的成员变量和成员函数可以在类的内部和外部访问与调用private：使用private关键字声明的类的成员变量和成员函数只能在类的内部访问和调用protected：使用protected关键字声明的类的成员变量和成员函数只能在本类以及派生子类的内部访问和调用。 2.c++语言的struct​ C++语言中，对struct关键字进行了扩展，struct已经不只是C语言中变量集合的struct，C++语言中的struct不仅可以定义成员函数，也可以实现继承和多态。与C语言中的struct一样，C++语言中使用struct定义类时，成员的默认访问级别为public。 3.c++语言struct和class的区别​ C++语言中struct与class最本质的区别如下：A.默认访问权限不同。struct 默认的访问权限是public的，class默认的访问权限是private的B.默认继承访问权限不同。struct默认的继承访问权限是public的，class默认的继承访问权限是private的。C.class可用作定义模板参数的关键字，而struct不可以。 ​ C++语言中，继承时如果没有指定继承访问权限，默认继承访回权限是public继承还是private继承，取决于子类而不是基类。struct 可以继承class，class也可以继承struct，默认的继承访问权限取决于子类是struct还是class，如果子类使用struct声明，默认继承访问权限是public；如果子类使用class声明，默认继承访问权限是private。 4.容器类模板：定义vector序列模板，是一个大小可以重新设置的数组类型，比普通数组更安全、更灵活。 ：定义list序列模板，是一个序列的链表，常常在任意位置插入和删图元素。 ：定义deque序列模板，支得在开始和结尾的高效插入和删除操作 ：为队列（先选先出）数据结构定义序列适配器器queue和priority_queue。 ：为堆栈（后进先出）数据结构定义序列适配器stack。 ：map是一个关联容器类型，充许根据键值是唯一的，且按照升序存储。multimap类似于map，但键不是唯一的。 ：set是一个关联容器类型，用于以升序方式存储唯一值。multiset类于set，但是值不必是唯一的。 ：为固定长度的位序列定义bitset模板，它可以看作固定长度的紧凑型bool数组。 ：（TR1）固定大小数组，支持复制。 ：（c++11）单向列表，支持快速随机访问。 ：（TR1）无序容器set，其元素随机存放。multiset类似于set，但是值不必是唯一的。 ：（TR1）无序容器map，其键值随机存放。multimap类似于map，但键不是唯一的。]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++易忘但重要的知识点(一)]]></title>
    <url>%2F2019%2F07%2F11%2FC%2B%2B%E6%98%93%E5%BF%98%E7%9F%A5%E8%AF%86%E7%82%B91%2F</url>
    <content type="text"><![CDATA[1.struct​ 注意c与c++中struct的区别，C语言中，struct定义了一种变量的集合，struct定义的标识符不是一种新类型。C语言中的struct内部不可以定义函数。 123456789101112131415161718192021#include &lt;stdio.h&gt;struct tag_student&#123; const char* name; int age;&#125;;typedef struct tag_student Student；int main（int argc，char*argv[]）&#123; //合法定义 Student s； s.name="lee"； s.age=30； //非法定义 tag_student ts;//error:unknown type name'tag_student' //合法定义 gtruct tag_student sts； sts.name="bauer"； sts.age=23； return 0；&#125; C语言中只有使用typedef关键字重命名struct后才可以使用Student定义变量。C++语言中struct用于定义一种全新的类型，可以使用struct定义的标识符直接定义变量。 1234567891011121314#inlucde &lt;iostream&gt;using namespace std;struct Student&#123; const char* name; int age;&#125;;int main(int argc，char*argv[])&#123; Student s; s.name="bauer"; s.age=20; return 0;&#125; 2.const​ C语言中，const修饰的变量是只读的，本质还是变量，可以借助指针修改变量空间的值；const修饰的变量会分配存储空间，const修饰的局部变量分配在栈上，const修饰的全局变量分配在只读存储区，修改const修饰的全局变量的值将会导致异常错误；const只在编译期有效，在运行期无效；const关键词用于向编译器表明const修饰的变量不能做左值。 12345678910111213141516171819#include&lt;stdio.h&gt;//const全局变量，分配在只读存储区const int number = 10;int main（int argc，char*argv[]）&#123;const int c=0；int*p=（int*）&amp;c；//编译器会为c分配空间printf（"Begin...\n"）;*p=5；printf（"c=%d\n"，c）：//5c=10；//error:assignment of read-only variable'c'int*cp=&amp;number；*cp=100；//很序异常const int number=10；int array[number]=&#123;0&#125;；//只读变量，编译时无法确定其值//error:variable-sized object may not be initialized printf（"End...\n"）；return 0；&#125; ​ C++语言中，对C语言基础的const进行了优化处理。编译器编译过程中遇到const修饰的标识符时，会将const修饰的标识符放入符号表中。如果后续编译过程中发现const修饰的标识符时，直接使用符号表中const修饰的标识符对应的值直接替换。但在以下情况下C++编译器会给const声明的常量分配空间：A、const修饰的常量为全局（extern修饰），并且需要在其它文件中使用B、使用&amp;操作符对cosnt常量取地址C++编译器虽然会对const常量分配空间，但不会使用其存储空间的值。const常量的判别：A、只有用字面量初始化的const常量才会进入符号表B、使用其他变量初始化的const常量仍然是只读变量C、被volatile修饰的const常量不会进入符号表const引用类型与初始化变量的类型相同时，初始化变量为只读变量；不同时，生成一个新的只读变量。 1234567891011121314151617181920212223#include&lt;iostream&gt;using namespace std；int main（int argc，char*argv[]）&#123; const int a=10； //error:invalid conversion from'const int*'to'int* int* p = &amp;a； int* cp =（int*）&amp;a； *cp=100； printf（"a=%d\n"，a）；//10 printf（"*cp=%d\n"，*cp）；//100 int b = 3； //使用其它变量初始化的const常量是只读变量 const int c = b； //error:variable-sized object'array'may not be initialized int array[c]=&#123;0&#125;； //使用volatile修饰的const常量不会进入符号表 volatile const int d=10； //error:variable-sized object'varray'may not be initialized int varray[d]=&#123;0&#125;； return 0；&#125; ​ C++语言中const与宏定义的不同在于，const常量由编译器处理，编译器会对const常量进行类型检查和作用域检查，而宏定义由预处理器进行处理，是单纯的文本替换。 12345678910111213141516171819202122232425#inlucde&lt;iostream&gt;using namespace std;void func1()&#123; #define NUMBER 100 const int number = 10; printf("NUMBER = %d\n",NUMBER); printf("NUMBER = %d\n",number);&#125;void func2()&#123; #宏没有作用域概念，预处理时直接替换 printf("NUMBER = %d\n",NUMBER); printf("NUMBER = %d\n",number); //‘number’was not declared in this scope&#125;int main（int argc，char*argv[]）&#123; funcl（）； func2（）； const int number=10； //编译时使用符号表的值替换 int array[number]=&#123;0&#125;； return 0；&#125; 3.函数重载1.函数重载规则如下： 函数名相同。 参数个数不同，参数的类型不同，参数顺序不同，均可构成重载。 返回值类型不同则不可以构成重载。 2.函数重载的匹配规则编译器调用重载函数的匹配规则如下： 将所有同名函数作为候选者 寻找可行的候选参数 匹配成功或失败 函数重载的匹配规则如下： 精确匹配实参，找到则调用。 通过默认参数能够匹配实参 通过默认类型转换匹配实参 通过默认类型转换匹配实参时，通过隐式转换寻求一个匹配，找到则调用。C++允许int到long和double的隐式类型转换，因此在函数重载时会引起二义性，解决方法是在调用时强转类型。 编译器调用重载函数匹配失败的规则： 如果最终找到的候选函数不唯一，则出现二义性，编译报错。 如果无法匹配所有候选者，函数未定义，编译报错。 有一个易错点，重载函数使用默认参数可能会造成二义性 1234567891011121314151617#include &lt;iostream&gt;using namespace std；int func（int a，int b，int c=0）&#123; return a+b+c；&#125;int func（int a，int b）&#123; return a+b；&#125;int main（int argc，char*argv[]）&#123; //函数调用时出现二义性 int x=func（1，2）； //error:call of overloaded'func（int，int）'is ambiguous return 0；&#125; 3.函数重载与函数指针​ 将重载函数名赋值给函数指针时，根据重载规则选择与函数指针参数列表一致的函数。重载函数的函数类型与函数指针类型必须严格匹配（不能有任何类型的隐式转换），此时函数返回类型将参与函数类型匹配。​ 函数重载必须发生在同一个作用域，无法通过函数名得到重载函数的入口地址。​ 重载函数的函数类型不同。 4.函数重载的注意事项​ 函数重载的注意事项如下：A、函数重载必然发生在同一个作用域中。B、编译器需要使用参数列表或函数类型进行函数的选择。C、不能直接通过函数名得到重载函数的入口地址。 4.函数默认参数1、函数参数默认值​ C++语言中，可以在函数声明时为参数提供一个默认值。当函数调用没有提供参数的值时，使用默认值。 2、默认参数的规则​ 函数默认参数的规则如下：A、默认参数的顺序，是从右向左，不能跳跃。B、定义在前，调用在后（此时定义和声明为一体），默认参数在定义处；声明在前，调用在后，默认参数在声明处。 C、一个函数，不能既作重载，又作默认参数的函数。当你少写一个参数时，系统无法确认是重载还是默认参数。 函数调用时参数从左到右匹配，如果一个参数使用了默认值，则后续参数必须使用默认值。 5.引用1.引用简介​ 变量名，本身是一段内存的引用，即别名（alias）。引用，是为己有变量起一个别名。 123Type&amp;name=var；int a；int&amp;b=a； 普通引用在定义时必须使用同类型的变量进行初始化。 2.引用的规则A、引用没有定义，是一种关系型声明。声明它和原有某一变量（实体）的关系。故而类型与原类型保持一致，且不分配内存，与被引用的变量有相同的地址。B、声明的时候必须初始化，一经声明，不可变更。C、可对引用再次引用。多次引用的结果，是某一变量具有多个别名。D、&amp;符号前有数据类型时，是引用。其它皆为取地址。 E.引用的类型必须与变量类型相同，引用不可以使用字面值初始化 3.引用的定义​ 函数中的引用形参不需要进行初始化，函数调用时进行初始化 4.引用的提高A、可以定义指针的引用，但不能定义引用的引用。 12345int a；int*p=&amp;a；int*&amp;rp=p；//ok int&amp;r=a；int&amp;&amp;rr=r；//error B、可以定义指针的指针（二级指针），但不能定义引用的指针。 12345int a；int*p=&amp;a；int**pp=&amp;p；//ok int&amp;r=a； int&amp;* pr = &amp;r; //error C.可以定义指针数组，但不能定义引用数组，可以定义数组引用 12345int a，b，c；int*parr[]=&#123;&amp;a，&amp;b，&amp;c&#125;；//okint&amp;rarr[]=&#123;a，b，c&#125;；//error int arr[]=&#123;1，2，3&#125;；int（&amp;rarr）[3]=arr；//ok的 ​ 数组是连续的存储空间，数组中的元素如果是引用，会导致数组的元素存储不连续。引用数组会破坏数组存储空间的连续性。 5.const引用​ const引用所引用的对象必须是const的，将普通引用绑定到const引用对象是不合法的。 1const type&amp;name=var； const引用可使用相关类型的对象（常量，非同类型的变量或表达式）初始化，const引用让变量具有只读属性，是const引用与普通引用最大的区别。非const引用只能绑定到与该引用同类型的对象。当const引用使用字面常量值初始化时，C++编译器会为常量值分配空间，使用字面常量对const引用初始化将生成一个只读变量。 12345678910111213141516171819202122232425#include&lt;iostream&gt;uaing nameapace std；int main（int argc，char*argv[]）&#123; int a=10； const int&amp; ra=a；//const引用，为只读变量 //只读变量不能作为左值 ra=100；//error:assignment of read-only reference'c' int*p=（int*）&amp;ra； *p=5； printf（"ra=%d\n"，ra）；//5 const int&amp;rb=10；//rb为只读变量，占用内存空间 //只读变量不能作为左值 rb=100；//error int arraya[ra]=&#123;0&#125;；//error //error:variable-sized object'arraya'may not be initialized int arrayb[rb]=&#123;0&#125;；//error //error:variable-sized object'arrayb'may not be initialized double pi=3.14； int&amp;rpi=pi；//非法 //error:invalid initialization of reference of type'int&amp;'from expression of type'double' const int&amp;crpi=pi；//合法 printf（"rpi=%d\n"，crpi）；//3 return 0；&#125; 6.引用的本质​ C++编译器在编译过程中使用指针常量作为引用的内部实现，因此引用所占用空间大小与指针相同。引用的本质是一个指针常量。type&amp;name&lt;====&gt;type*const name； 使用引用时不能返回局部变量的引用 12345678910#include&lt;iostream&gt;using nameapace std；int main（int argc，char*argv[]）&#123; printf（"sizeof（char&amp;）=%d\n"，sizeof（char&amp;））；//1 char c=‘a'； char&amp;rc=c； printf（"sizeof（char&amp;）=%d\n"，sizeof（rc））；//1 return 0；&#125;]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3内核5]]></title>
    <url>%2F2019%2F07%2F06%2Fns3%E5%86%85%E6%A0%B85%2F</url>
    <content type="text"><![CDATA[一.Tracing 系统​ 无论是标准C++输出还是Logging系统输出，都仅仅适合非常小的程序。因为当程序不断增加时，打印语句和控制输出格式将变得非常艰难，即使能够得到想要的输出结果，那么分析这些多条复杂的信息也将变得没有可能。 所以，ns-3需要提供一种机制，这种机制允许用户进入系统内核来获取所需要的信息，前提是不改变内核代码和不用再次编译。更好的方式是，当用户感兴趣的信息发生改变时，系统通知用户对信息进行处理，而不是去深入到系统内核。 ​ 下面要讲到的Tracing系统机制就是ns-3提供的用来解决上述问题的一种方法。 1.综述​ ns-3 Tracing系统大体可以分为3个部分：Tracing Sources、Tracing Sinks和将Tracing Sources和Tracing Sinks关联一起的方法。 ​ Tracing Sources是一个实体，它可以用来标记仿真中发生的时间，也可以提供一个访问底层数据的方法。例如，当一个网络设备或网卡收到一个网络分组时，Tracing Source可以指示并提供一个途径将分组的内容传递给对该分组感兴趣的Tracing Sink。还有，Tracing Sources还可在感兴趣的状态发生变化时给出相应的指示。例如，TCP网络协议模块中的拥塞窗口发生改变时，Tracing Sources会给出指示。 ​ Trace Sources本身是起不到任何作用的，只有当它和一段有实际功能的代码相关联时才有意义，这段代码就是使用Trace Sources提供的信息来做相关事务。使用或者说消费Trace Sources提供信息的实体就称为Trace Sink。换句话说，Trace Sources提供信息，而Trace Sink消费信息，它们2个的关系可以比喻为生产者和消费者的关系。一个Trace Sources生产的信息可以没有Trace Sink消费，也可以一个或者多个Trace Sink消费，它们之间是一对多的关系。这样大家就知道了，单独使用Trace Sources或单独使用Trace Sink是没有任何意义可言的，而针对不同用户给出不同的Trace Sink代码来处理Trace Sources产生的信息时得出的结果也是不同的，也就是说用户可以根据自己的需求给出不同的Trace Sink 以便得出不同的结果。 ​ 下面就通过一个简单的例子来说明Trace Sources和Trace Sink以及它们之间是如何相关联的。这里需要用到前面学习过的回调，如果有些遗忘，用户可以再回头学习，这里不再重复。首先给出整体的代码，为了方便大家理解，在代码中通过插入注释的方式来解释该代码段的意义，代码如下： 12345678910111213141516171819202122232425262728293031323334353637#include "ns3/object.h"#include"ns3/uinteger.h"#include "ns3/traced-value.h""#include "ns3/trace-source-accessor.h"#include &lt;iostream&gt;using namespace ns3;//首先要定义自己的类，该类的父类为Object，因此要引入头文件#include“ns3/object.h”，再次引入了ns-3自定义的无符号整型所声明的头文件#include“ns3/uinteger.h”。下面着重讲解 traced-value.h头文件，在这个头文件中引入了要跟踪数据的类型，即TracedValue。trace-source-accessor.h这个头文件中包含了本程序要使用的能把自定义数据转换为Trace Sources的函数。class MyObject: public Object &#123;public: static TypeId GetTypeId(void)&#123;static TypeId tid=TypeId("Myobject"). SetParent (Object:: GetTypeId ()). AddConstructor&lt;Myobject&gt;(). AddTraceSource ("MyInteger","An integer value to trace.", MakeTraceSourceAccessor(&amp; MyObject::m myInt)); return tid;&#125;Myobject()&#123;&#125;TracedValue&lt;uint32_t&gt; m_myInt;&#125;；//因为Tracing系统和属性系统有很大的关联，而属性系统和对象相关联，所以，每一个要追踪的数据都必须属于一个特定的类，这里定义这个类为MyObject，而要追踪的数据为m mylnt。GetTypeld 这个函数在前面已经讲述过，这里要注意的是AddTraceSource函数，这个函数使得m_myInt成为一个Trace Sources。void IntTrace(Int oldValue,Int newValue)std::cout &lt;&lt;"Traced "&lt;&lt; oldValue&lt;&lt;"to "&lt;&lt; newValue&lt;&lt; std::endl;&#125;//上述代码就是定义Trace Sinkint main(int argc,char* argv[])&#123;Ptr&lt;MyObject&gt;myObject=CreateObject&lt;MyObject&gt;(); myobject-&gt;TraceConnectWithoutContext("MyInteger", MakeCallback(&amp;IntTrace)); myObject-&gt;m_myInt=1234;&#125;//主函数中首先定义了一个类对象实例，这个实例中包含了一个TraceSource。下面一个函数是至关重要的，因为就是TraceConnectWithoutContext这个函数将Trace Sources 和Trace Sink相关联。只要调用了这个函数，当Trace Sources数据m_myInt 发生改变时，IntTrace函数才会被调用。最后一行代码可以被解释为把常量1234赋值给m_mylnt，这时系统会识别这一行为，并将m_mylnt赋值前和赋值后的2个值作为形参传递给身为Trace Sink的回调函数IntTrace。运行这个例子的结果如图5-8所示。 2.配置系统​ 在上一小节例子中学习的函数TraceConnectWithoutContext 在实际编程中很少用到。通常使用被称作“config path”的子系统从系统中选取用户所要使用的Trace Sources。具体的内部细节不再深入讨论，这一部分通过具体操作来讲解如何通过配置路径来建立Trace Sources和Trace Sink的关联。这里使用前面曾经学习过的third.cc作为基本代码，然后通过定义一个Trace Sink来输出移动节点的位置变化信息。 1234567891011121314using namespace std;//记得加上这行代码，小细节//下面的函数CourseChange就是要定义的Trace Sink，和定义普通函数没有太大的区别，只要在主函数前声明定义就行。这段代码大家应该比较了解，一个回调函数包含2个参数。void CourseChange（string context，Ptr&lt;const MobilityModel&gt;mode1）&#123; Vector position=model-&gt;GetPosition（）； NS LOG UNCOND（context&lt;&lt;"x="&lt;&lt; position.x&lt;&lt;"y="&lt;&lt; position.y）；&#125;//下面编写的代码就是使上面的CourseChange（Trace Sink）和CourseChange（Trace Source）相关联的代码。下面代码放在Simulator:：Run()；前面就好。ostringstream oss;oss&lt;&lt;"/NodeList"&lt;&lt; wifistaNodes. Get(nWifi-1)-&gt;GetId()&lt;&lt; "/$ns3:: MobilityModel/CoureChange";Config:: Connect(oss. str(), MakeCallback(&amp; CourseChange)); ​ 使用类Config的一个静态成员函数Connect将二者关联在一起。这个函数有2个参数，首先看第2个参数，这个参数功能是使函数CourseChange成为一个回调函数。本小节主要针对第一个参数进行讲解，首先，这个参数是一个由各种字符组成的字符串。下面对该函数的参数——作为路径的字符串分析其代表的含义。第一个“/”符号代表后面要紧跟的是命名空间，后面所跟的“/”符号可以像目录与子目录一样来理解。这里用到的命名空间为NodeList。而NodeList是一个仿真中使用的节点的一个列表，紧随其后的是这个列表的一个索引，这里是通过调用Get函数来获取该节点，然后再通过Getld函数来得到该节点的索引。下一段字符串的第一个字符为“$”，当程序遇见这个符号时，就会调用函数GetObject来返回一个对象，这是因为大家在实际仿真中使用的对象聚合技术已经把许多对象全都集成在这个节点中。因为节点中集成了需要的对象，所以后面就要给出返回对象的类型，这里要返回的对象类型为MobilityModel。而类MobilityModel有一个称为CoureChange的属性，也就是我们要跟踪的Tracing Sources。 ​ 那么如何确定“Config path”是一个不可避免的问题，很简单，只要进入API文档，进入你需要的类，你就会发现一个标题Config Paths，之后就一目了然了。 3.如何确定Trace Sources​ 大家已经学习了2种方法把Trace Sources和Trace Sink相关联起来。这里不可避免地提出疑问，我们是如何知道CourseChange就是想要得到的Trace Sources以及CourseChange是否是可用的Trace Sources。答案就是，去ns3的官网找到自己版本的API文档，然后展开“Modules”，继续展开“C++Constructs Used by All Modules”然后你就会看到几条链接，其中有一个链接是“The list of all trace sources”。在这里你就可以找到ns3中已经定义好的可以直接使用的Trace Sources。比如上面例子中使用的CourseChange就可以在里面找到。 4.如何确定Trace Sink​ Trace Sink 实际上是一个函数，所以确定Trace Sink主要就是确定这个函数的返回值和参数。最简单的方法就是参考系统给出的一些例子中已经写好的回调函数。比如在上面的例子中使用的回调函数CourseChange，可以在ns-3.16/examples/wirless目录中的mixed-wireless.cc代码中找到一个函数CourseChangeCallback与之类似。系统中给出了很多这样的可以用来参考的例子，但是这样对于用户来说太过于繁琐，也有可能用户找不到自己想要参考的那个例子，这时候读者要做的就是首先把函数的返回值定义为空也就是void，而参数列表中参数的类型就是Trace Sources的变量类型，比如本文使用的Trace Sources是CourseChange，读者可以在头文件mobility-model.cc中的GetTypeld函数中找到与之对应的MobilityModel成员变量m_courseChangeTrace，然后它的声明类型就可以在mobility-model.h中找到，即 TracedCallback]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3内核4]]></title>
    <url>%2F2019%2F07%2F05%2Fns3%E5%86%85%E6%A0%B84%2F</url>
    <content type="text"><![CDATA[一.属性系统在使用ns-3对网络进行模拟仿真时，要对以下2个方面进行设置。 对使用的网络构件进行连接组织和模拟拓扑。 对网络构件中实例化的模型所使用的参数值进行设置。 1.对象概述​ ns-3提供了自己特有的定义类的方法，即大部分类都是继承于Object类。这些对象有很多附加的属性，这些属性是为了对系统进行组织和改进对象的内存管理而开发的。 ​ 下面通过Node类从整体上对对象以及后面要学习到的属性系统进行一个简单的介绍。在头文件node.h中，类中声明了一个静态成员函数GetTypeld如下： 12345Class Node : public Object&#123; public: Static TypeId GetTypeId(void):&#125; 然后在node.cc文件中对函数GetTypeId进行了定义，如下: 12345678910TypeId Node:: GetTypeId (void)&#123;static TypeId tid=TypeId ("ns3:: Node") . SetParent&lt;Object&gt;() . AddConstructor&lt;Node&gt;() . AddAttribute ("DeviceList","The list of devices associated to this Node.", ObjectVectorValue(), MakeObjectVectorAccessor (&amp; Node::m devices), MakeObjectVectorChecker&lt;NetDevice&gt;()) . AddAttribute("ApplicationList","The list of applications associated to this Node.", objectVectorValue(), MakeObjectVectorAccessor(&amp; Node::m applications), MakeObjectVectorChecker&lt;Application&gt;()) . AddAttribute("Id","The id(unique integer) of this Node.", TypeId:: ATTR GET, UintegerValue (0), MakeUintegerAccessor (&amp; Node::m id), MakeUintegerChecker&lt;uint32t&gt;()); return tid;&#125; ns-3中所有由Object类派生的类都可以包含一个叫Typeld的元数据类，该类用来记录关于类的元信息，以便在对象聚合以及构件管理中使用，TypeId类中涉及了用唯一的字符串来标识一个类、子类的基类以及子类中可以访问的构造函数。这里可以把Typeld看作RTTI的一个扩展形式，RTTI（运行时类型识别）是标准C++提供的一种使程序能够使用基类的指针或引用来检查这些指针或引用所指对象的实际派生类型。 ​ 函数SetParent()为声明该类的基类，方便在使用GetObject()函数时安全地进行向上或向下类型转换。 ​ 函数AddConstructor()和抽象对象工厂机制可以方便用户在不了解对象类型具体细节的情况下构建对象。 ​ 接下来的3个AddAttribute()函数的调用目的是把一个给定的唯一字符串和类的成员变量相关联。函数第1个参数是要绑定的字符串，第2参数是对第1个参数的解释说明，第3个参数是要绑定的类成员变量必须转化的类型，第四个参数就是把类成员变量强制转化为第3个参数的类型，第4个参数是对第3个参数使用的成员变量是否合法的一个检查。 ​ 对于上面提到的对象工厂机制在前面已经提到，这里再简单说明一下。如果用户想要创建节点，可以使用CreateObject()函数，如下： 1Ptr&lt;Node&gt; n = CreateObject&lt;Node&gt;(); 也可以使用对象工程机制来创建，如下: 1234ObjectFactory factory; const st::string typeId="ns3:: Node"; factory. SetTypeId(typeId); Ptr(Object) node=factory. Create&lt;Object&gt;(); 上面2种创建对象的方法对于对象的属性都进行了初始化 2.属性系统​ 在上一小节，回顾了ns-3中对象模型的基本概念和相关知识。本小节将继续学习在对象模型基础上的属性系统。ns-3提出属性系统机制的目的就是为了管理和组织仿真中的内部对象。主要原因是用户在仿真过程中要不断地修改已经存在的仿真脚本，来跟踪、收集和研究仿真程序中变量或数据的变化，比如： 用户想要跟踪第一个接入点的无线接口上的分组。 若用户对TCP拥塞窗口比较感兴趣，也就是说对拥塞窗口大小的跟踪。 用户想要获取并记录模拟上所有被使用的值。 类似地，用户可能想对模拟中的内部变量进行细致的访问，或者可能广泛地修改某个特定参数的初始值，以便涉及所有随后要创建的对象。用户可能知道在模拟配置中哪些变量是可以设置的，哪些是可以获许的。这不仅仅是为了命令行直接交互，还考虑到图像用户界面，该界面可能提供让用户在节点右击鼠标就能获取信息的功能，这些信息可能是一个层次性组织的参数列表，显示该节点上可以设置的参数以及构成节点的成员对象，还有帮助信息和每个参数的默认值。 ​ ns-3给用户提供了可以访问系统深处的值的方法，而不是在系统中加入指针通过指针链来获取想要的值。下面就来分析类Drop TailQuenue，该类有一个叫做m_maxPackets的无符号整型成员变量，该成员控制队列的大小。在内核代码中查看该类的声明，如下： 12345678class DropTailQueue:public Queue&#123; public: static typeId GetTypeId(void); ... private: std::queue&lt;Ptr&lt;Packet&gt;&gt;m_packets; uint32_t m_maxPackets;&#125; 考虑用户可能对m_maxPackets的值想要做的事情：为系统设置一个默认值，以便无论何时一个新的DropTailQueue被创建时，这个成员变量都被初始化为默认值。对于一个已经实例化的对象，设置或获取该对象的值。上述情况在标准C++中一般会提供Get()和Get()函数来实现。而在ns-3中，通过属性系统中的Typeld类来实现。例如： 123456789NS_OBJECT_ENSURE_REGISTERED (DropTailQueue); TypeId DropTailQueue::GetTypeId (void)&#123;static TypeId tid=TypeId ("ns3:: DropTailQueue") . SetParent&lt;Queue&gt;() . AddConstructor&lt;DropTailQueue&gt;() . AddAttribute("MaxPackets", "The maximum number of packets accepted by this DropTailQueue.", UintegerValue (100), MakeUintegerAccessor (&amp; DropTailQueue::m_maxPackets), MakeUintegerChecker&lt;uint32_t&gt;()); return tid;&#125; 上述代码中方法AddAttribute()对m_maxPackets进行了一系列的处理。将变量m_maxPackets绑定到一个字符串“MaxPackets”中。 提供默认值100。 提供为该值定义帮助的信息。 提供“checker”，可以用来设置所允许的范围。 关键一点是该变量的值以及它的默认值在属性空间中是可以基于字符串“MaxPackets”访问的，下面将通过一个例子来说明用户是如何来操纵这些值的。 对于NS_OBJECT_ENSURE_REGISTERED(DropTailQueue)；这行代码对于自定义的类是必须的，否则你的属性系统涉及的属性都不能正常地初始化。本小节通过一个脚本例子来说明如何在脚本中操纵属性系统中的数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include "ns3/log.h"#include"ns3/command-line.h"#include "ns3/ptr.h"#include"ns3/config.h"#include"ns3/uinteger.h"#include "ns3/string.h"#include "ns3/pointer.h"#include "ns3/simulator.h"#include "ns3/node.h"#include "ns3/queue.h"#include "ns3/drop-tail-queue.h"#include"ns3/point-to-point-net-device.h"using namespace ns3;int main (int argc,char *argv[])&#123; LogComponentEnable("AttributeValueSample",LOG_LEVEL_INFO); //设置默认值，方法如下: //这2行代码的功能是为随后即将声明的DropTailQueue类的实例化设置默认值，换句话，假如随后生成了一个DropTailQueue对象dtq,那么dtq的成员变量m_maxPackets的默认值为80. //正如上面两行代码所写的一样，使用SetDefault函数时，第二个参数可以是String Value 类型也可以是Uinteger Value类型，具体使用什么类型用户可以自己决定，没有什么实际的区别。 Config::SetDefault("ns3::DropTailQueue::MaxPackets",StringValue("80")); Config::SetDefault("ns3::DropTailQueue::MaxPackets",UintegerValue (80)); CommandLine cmd; cmd.Parse(argc, argv); //设置好默认的值以后用户就可以实例化自己的对象，如下： //下面代码中，创建了一个唯一的节点（Node0），并且在这个节点上创建了一个唯一的Point ToPointNetDevice网络设备（NetDevice0），然后为其添加了一个尾部分组丢失队列。这里新创建对象的成员变量m_ maxPackets的初始化值并不是系统一开始给定的值100，而是使用过SetDefaults函数修改过的值80。因此，可以得出结论，在每次创建一个对象时，都可以通过SetDefaults函数来修改默认的值。 Ptr&lt;Node&gt; n0 = CreateObject&lt;Node&gt; (); Ptr&lt;PointToPointNetDevice&gt; net0 = CreateObject&lt;PointToPointNetDevice&gt; (); n0-&gt;AddDevice(net0); Ptr&lt;Queue&gt;q=Createobject&lt;DropTailQueue&gt;(); net0-&gt;SetQueue (q); //上面大家学习了怎么在创建对象实例化之前修改默认的属性值，这里大家将继续学习怎么从已创建的对象中获取所想得到的属性值，或者在必要的情况下修改这些值。 //通过指针访问属性值。 //在本节的例子中首先创建一个指针变量，注意这里是创建一个指针变量而不是创建一个指针。 PointerValue ptr; //然后为指针变量赋值： net0-&gt;GetAttribute("TxQueue", ptr); //获取队列 Ptr&lt;Queue&gt;txQueue=ptr.Get&lt;Queue&gt;(); //下面通过GetObject函数来安全地把txQueue向下类型转化： Ptr&lt;DropTailQueue&gt;dtq=txQueue-&gt;GetObject &lt;DropTailQueue&gt;(); NS_ASSERT (dtq); //下面可以通过输出数据来验证本程序是否将默认值100改成了80： UintegerValue limit; dtq-&gt;GetAttribute("MaxPackets", limit); NS_LOG_INFO("1. dtq limit:"&lt;&lt;limit. Get()&lt;&lt;"packets"); //实际上，编程者不进行向下类型转换一样可以得到MaxPackets值： txQueue-&gt;GetAttribute("MaxPackets", limit); NS_LOG_INFO("2. txQueue limit:"&lt;&lt;limit. Get()&lt;&lt;"packets"); //上述2种方法得到的结果是一样的，稍后给出运行结果。 //如果用户想要在创建对象之后再改变MaxPackets的值该怎么做？方法如下： txQueue-&gt;SetAttribute ("MaxPackets", UintegerValue (60)); txQueue-&gt;GetAttribute("MaxPackets", limit); NS LOG INFO("3. txQueue limit changed:"&lt;&lt;limit. Get ()&lt;&lt;"packets"); //用户也可以通过基于命名空间的方式访问属性值。 //不同于上述通过指针访问属性值的方法，这里用户可以通过使用配置命名空间的方法来操作属性值。针对本例中使用的方法如下： Config:: Set("/NodeList/0/DeviceList/0/TxQueue/MaxPackets", UintegerValue (25)); txQueue-&gt;GetAttribute ("MaxPackets", limit); NS LOG INFO("4. txQueue limit changed through namespace:"&lt;&lt; limit. Get()&lt;&lt;"packets"); //这样就将MaxPackets的值改为了25。这里仅仅是修改了第一个节点的第一个网络设备对应的MaxPackets值。当然大家也可以修改其他节点的所有网络设备对应的MaxPackets值。如下： Config::Set("/NodeList/*/DeviceList/*/TxQueue/MaxPackets",UintegerValue (15)); txQueue-&gt;GetAttribute("MaxPackets",limit); NS_LOG_INFO("5.txQueue limit changed through wildcarded namespace:"&lt;&lt; limit.Get()&lt;&lt;"packets"); Simulator::Destroy ();&#125; 二. ​ ns-3作为一个网络仿真工具，虽然它提供的是大多数现在已有的网络模块能够满足大多数用户的需求，但是毕竟网络仿真主要用在前沿的科学研究中，因此很多前沿的网络模块或者数据ns-3是不可能更新得那么及时。那么对于这些用户来说自己扩展必要的网络模块或数据是必要的，ns-3作为一个开源的系统，可扩展性是系统必须支持的特性。 1.添加现有类的成员变量到元数据系统中。 ​ 上一部分大家学习了系统怎么样把类的成员变量和元数据系统绑定在一起。把现有类的成员变量添加到元数据系统里的方法和上面绑定的方式基本一样，下面通过一个例子来学习。考虑类TcpSocket的一个成员变量：uint32_tm_cWnd。 ​ 假设使用TCP模块的某个人想要使用元数据获得或设置该变量的值。如果ns-3还没有提供这个变量，用户可以在元数据系统中添加如下声明： 1234.AddAttribute（"Congestion window"，"Tcp congestion window（bytes）"，UintergerValue（1），MakeUintergerAccessor（&amp;TcpSocket:：m_cWnd），MakeUintergerChecker&lt;uint16_t&gt;（）） 现在用户可以使用指向该TcpSocket的指针来执行设置和获取操作，而不是显式地添加这些函数。此外，访问控制可以被应用，比如使用该参数只读不可写或对参数的上下界检查。 2.向属性系统中添加自定义类。 ​ 从用户的角度来看，编写新的类并将其加入属性系统，也就是说要将编写的字符串和类的成员变量的属性值进行对应。下面把自定义的类A添加到属性系统中。首先在a.h头文件中声明类A，如下： 1234567class A:public object&#123;public： static TypeId GetTypeId（void）；private： int16_t m_int16；&#125;；NS_OBJECT_ENSURE_REGISTERED（A）； 上述代码中使用默认构造函数，除了要声明函数GetTypeld和NS_OBJECT_ENSURE_REGISTERED(A)；如标准C++创建类有区别之外，别的都一样。 然后在a.cc文件里定义类函数GetTypeId,如下: 123456789static TypeId GetTypeId(void)&#123; static TypeId tid=TypeId("ns3::A") .SetParent&lt;Object&gt;() .addAttribute("TestInt16","help text", IntgerValue(-2), MakeIntegerAccessor(&amp;A::m int16), MakeInterChecker&lt;int16_t&gt;()); return tid;&#125;]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3内核3]]></title>
    <url>%2F2019%2F07%2F05%2Fns3%E5%86%85%E6%A0%B83%2F</url>
    <content type="text"><![CDATA[一.对象模型本节描述针对ns-3的对象设计。总体来说，ns-3中使用的设计模式包括经典的面向对象设计、接口与实现分离、非虚拟的公共接口设计模式、对象聚合以及引用计数内存管理模式。 1.面向对象编程​ 因ns-3是基于C++开发，所以相应保留了C++所具有的面向对象编程的特点与语法规则，如封装性，继承性和多态性。 2.对象基类​ ns-3中提供了3个对象基类供用户继承。这3个基类分别是：Object、ObjectBase、SimpleRefCount。ns-3并不强迫每一个类都必须继承自这3个类，但是从这3个基类继承的类包含了ns-3提供的特有特性： 属性系统 对象聚合系统 智能指针和引用计数系统 从ObjectBase中继承的类包含上述前2个功能特性，但是不包含智能指针，从SimpleRefCount中继承的类只包含智能指针功能，只有从Object类继承的子类才具备以上3种特性。 3.内存管理与引用计数指针(Ptr)​ 在C++程序中，内存管理一直是一个复杂问题，一旦操作不对就很有可能造成内存泄漏，而且如果用户使用了new操作符申请了内存，而忘记在变量无效时使用delete操作符释放内存，就容易导致内存不足的假象。因此，C++程序员在对new和delete的使用时要非常小心，这是程序员不愿意接受的。在ns-3中，使用引用计数设计模式来管理内存。 ​ 所有使用引用计数的对象都持有一个内部的计数器来判断对象是否可以安全地删除，释放已分配的内存空间。当一个接口获得一个对象指针时，该对象的引用计数通过调用函数Ref()来增加1，同样当用户不再使用该指针时，用户有义务调用Unref()函数来减小引用计数，当引用计数器减少为0时，该对象就被删除。 ​ 当用户通过构造函数或GetObject()函数（这个函数将在后面章节提到，这里知道就好）来获取指针时，引用计数不增加。当用户通过复制构造函数来获取一个指针时，引用计数增加。所有的对象指针都必须通过Unref()来释放引用。 这里用户最关心的莫过于Ref()和Unref()这2个函数怎么调用，需要谁调用。ns-3提供了引用计数智能指针来调用这些函数，用户在编程时基本不需要考虑这些，智能指针将在后面讲到。 a.引用计数指针(Ptr) ​ 在编程时时时刻刻想着调用Ref）和Unref）是非常繁琐讨人厌的事，就好比使用new和delete一样，所以ns-3提供了引用计数指针Ptr类来处理上述函数调用。智能指针认为每一个基本类型提供一对函数Ref）和Unref）来增加和减小对象实例的引用计数。 ​ 智能指针的使用和C++里面指针的使用是相同的，可以被赋值为空，也可以通过其他指针来复制等，熟悉指针的用户用起来非常顺手。 ​ 如果你想要让一个指针指向一个对象，建议你使用ns-3提供的CreateObject模版函数来创建类对象，并将其地址复制给指针以避免内存泄漏。ns-3提供的这个函数旨在为用户提供方便并减少代码量。 b.CreateObject和Create 在C++中，对象的创建可以是动态的也可以使静态的，这些特征在ns-3中同样适用，不过ns-3还提供了一种创建方式，特别是针对包含引用计数的对象，就是通过模版函数CreateObject和Create来创建。对于基类为Object的类创建对象的方法如下： 12Ptr&lt;MyObject&gt;mo=CreateObject&lt;Myobject&gt;（）；如：Ptr&lt;WifiNetDevice&gt;device=CreateObject&lt;WifiNetDevice&gt;（）； 在ns-3中尽量地使用CreateObject()来创建类对象，而不是在C++中使用new操作符。只要这个类支持智能指针，强烈建议用户使用上述方法，这是对new操作符的一个封装，它能正确地处理智能指针引用计数。 4.聚合​ ns-3中对象聚合系统是总结了ns-2中的缺点进而提出的。在ns-2中使用继承和多态来扩张协议模型。例如，TCP的特殊版本Reno TcpAgent 是由TcpAgent派生的，其对基类的函数进行了重写覆盖。也因如此，ns-2模型中出现了2个问题：向下类型转换（downcasts）和弱基类（weak base class）。向下类型转换是指一个过程，即使用指向某个基类对象的指针在程序运行时查询该指针获得类型的信息，然后该指针显示转换为子类的指针，以便子类的成员函数能被该指针使用。弱基类是指当某个类无法被有效地重用时，因为它可能缺少某种必要的功能，导致开发者不得不修改基类，这将导致基类API的增生，某些API可能并不是对所有的子类都是语义正确的。 ​ ns-3使用查询接口设计模式来避免这些问题。在ns-3中，类Node是使用聚合的一个很好的例子。注意ns-3中没有Node的子类，也就是说任何网络终端都使用相同的Node类，但是不同网络终端的构件和协议是不同的，那么怎么创建满足不同网络节点的需求呢？这就要使用聚合的方式将不同的构件协议聚合到节点中。下面这个例子大家一起学习研究IPv4协议是如何被加入到节点中的。 1234567891011static voidAddIpv4Stack(Ptr&lt;Node&gt;node)&#123; Ptr&lt;IPv4L3Protocol&gt; ipv4=Createobject&lt;IPv4L3Protocol&gt;(); //创建一个IPv4协议的指针对象 ipv4-&gt;SetNode (node); //把IPv4协议聚合到节点中，这样Node就不需要被编辑来使用户使用指向基类Node的指针来访问IPv4接口；用户可以在程序运行时向节点请求指向该节点IPv4接口的指针。值得注意的是：不能将同一类型的对象聚合到某一Object中，比如试图向一个节点中聚合2个路由协议是不正确的。 node-&gt;AggregateObject (ipv4); Ptr&lt;IPv4Imp1&gt;ipv4Impl=CreateObject&lt;IPv4Impl&gt;(); ipv4Imp1-&gt;SetIPv4 (ipv4); node-&gt;AggregateObject (ipv4Imp1);&#125; 考虑一个节点指针m_node，该指针指向一个节点对象，且先前已经将IPv4的实现聚合到了该节点。用户想要配置一个路由协议，为了实现这点，必须访问已经聚合到该节点的一个对象，且该对象具有IP配置接口。例如， 1Ptr&lt;IPv4&gt;ipv4=m_node-&gt;Getobject&lt;IPv4&gt;()； 如果没有IPv4的对象被聚合到该节点上，那么该方法就会返回一个NULL。因此检查该函数调用的返回值是一个非常好的习惯。如果成功，则用户可以使用Ptr，该指针指向先前被聚合到该节点的IPv4对象。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3内核2]]></title>
    <url>%2F2019%2F07%2F05%2Fns3%E5%86%85%E6%A0%B82%2F</url>
    <content type="text"><![CDATA[一.回调​ 在网络仿真的研究中，保持模块的独立性或者说模块之间的通信比较灵活不是一个抽象的问题，相反这是一个很有必要解决的问题，在以往很多仿真研究中出现过很多类似的问题，比如当科研人员想扩展或修改系统来做不同的仿真研究时，模块的独立性就能方便地满足上述要求。 1.回调机制的基础​ ns3解决以上问题的机制是使用回调。最终的目标是让一个模块来调用另一个模块中的一个函数，而且这2个模块之间几乎是没有任何依赖的。 ​ 你需要某种方法——要把被调用函数的地址作为一个变量，这个变量叫做一个函数指针变量，函数和函数指针之间就像对象和对象的指针一样。在c中函数指针还是挺简单的，但在C++中增加了类和对象的概念，同时也不可避免地面对一个问题：怎么为成员函数建立函数指针。即成员函数指针和函数指针的区别。 ​ 声明类成员函数指针看起来与c中函数指针略有不同，如下: 1int (MyClass::*pmi)(int arg)=0; 这里声明一个函数指针变量命名为pmi，前面加了类名，这是因为该函数指针pmi将要指向一个特定类的成员函数，函数声明如下： 123456789class MyClass&#123; public: int MyMethod(int arg);&#125;;//Pmi类函数指针变量的赋值和调用pmi=&amp;MyClass::MyMethod;MyClass myClass;myClass.*pmi(1234); 前面讲了函数指针的声明、初始化以及通过函数指针调用函数，下面来讲到底什么是回调。用一个例子来解释一下，例如，写一个快速排序函数供他人调用，其中必包含比较大小。麻烦来了：此时并不知要比较的是何类型的数据——整数、浮点数、字符串？于是只好为每类数据制作一个不同的排序函数。更通用的办法是在函数参数中列一个回调函数地址，并通知调用者：需自己准备一个比较函数，其中包含2个指针类参数，函数要比较此二指针所指数据之大小，并由函数返回值说明比较结果。排序函数借调用者提供的比较函数来比较大小，借函数指针传递参数，可以全然不管所比较的数据类型。最后被调用者回调调用者的排序函数得到最终序列。上述过程涉及被调用者调用调用者的函数，故称其为回调（callback）。 2.回调API​ ns3提供了Callback类API接口来为用户提供服务，主要就是回调类型声明：用给定的签名声明回调的类型，还有就是回调实例化。 1.针对静态函数如何声明和实例化Callback类型，看下面这个程序。 12345678910111213static double Cbone（double a，double b）&#123;std:：cout&lt;&lt;"invoke cbone a="&lt;&lt;a&lt;&lt;"，b="&lt;&lt; b&lt;&lt; std:：end1；return a；&#125;int main（int argc，char*argv[]）&#123;//返回类型：double//第一个参数：double//第二个参数：double Ca1lback&lt;double，double，double&gt;one；//实例化回调&#125; 本例通过这个Callback类模版来实例化，这里需要注意的是通过Callback类模版实例化时，至少一个参数对应回调函数的返回值类型，最多5个参数，除了第一个参数外，后面4个分别对应自己声明回调函数的参数，如果自己定义的函数加上返回值超过5个，那么就必须通过扩展回调来处理。 在上面的代码中声明了一个叫做“one”的回调，该回调保留一个函数指针，它保留的函数指针指向的函数必须是double型，并且必须支持2个double参数。如果试图传递一个参数，该函数的签名不与所声明的回调匹配，那么编译就会失败。可以用下面代码形象地表示回调实例与目标函数的匹配。 12Static double CbOne(double a, double b)&#123;&#125;Callback&lt;double, double, double&gt;one 只有在签名匹配的情况下才能将函数与回调进行绑定。下面将回调“one”与签名匹配的函数进行绑定： 1One=MakeCallback（&amp;Cbone）； 在后面的程序代码中如果用到回调，使用方法如下： 123NS_ASSERT（！one.IsNul1（））；//检查回调“one”是否为空Double retOne；retOne=one（10.0，20.0）； 检查函数IsNullO确保该回调不是NULL，即该回调的背后存在一个函数调用。调用回调“one”和直接调用函数CbOne）返回的结果是相同的。 2.针对类成员函数使用回调 在C++面向对象编程中，所调用的函数都是类的公共成员函数，不是全局静态函数。在这种情况下，MakeCallBack函数就要额外地增加一个参数，该参数告诉系统在哪个对象上调用该回调函数。考虑下面的程序代码： 1234567891011121314151617181920212223class MyCb&#123;public：int CbTwo（double a）&#123;std:：cout &lt;&lt;"invoke cbTwo a="&lt;&lt; a &lt;&lt; std:：endl；return-5；&#125;&#125;//返回类型：int//第一个参数的类型：double Callback&lt;int，double&gt;two；MyCb cb；//创建一个回调变量并指向MyCb:：cbTwo two=MakeCallback（&amp;MyCb:：CbTwo，&amp;cb）；//this is not a null callback NS ASSERT（！two.IsNull（））；//通过回调变量调用MyCb:：cbTwo函数int retTwo；retTwo=two（10.0）；NS ASSERT（retTwo ==-5）；two=MakeNullallback&lt;int，double&gt;（）；NS ASSERT（two.IsNull（））；return 0；&#125; 这里传递了一个额外的指针给函数MakeCallback&lt;&gt;()，即当函数two()被调用时，调用的是&amp;cb所指向的对象函数CbTwo，也就是类MyCb中的CbTwo函数。 3.构建Null回调​ 在上述两小节程序代码中，有这么一个函数IsNull（）用来判定回调是否为空，也就是回调不指向任何函数。因为ns3允许回调是空，如果用户想要构建空回调，ns3提供MakeNullCallback&lt;&gt;函数来实现这一功能： 12Two=MakeNullCallback&lt;int，double&gt;（）；Int retTwoNull=two（20.0）； 调用空的回调就相当于调用了一函数指针不指向任何函数。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3内核1]]></title>
    <url>%2F2019%2F07%2F05%2Fns3%E5%86%85%E6%A0%B81%2F</url>
    <content type="text"><![CDATA[一.ns-3的组织结构​ ns-3是一个离散事件仿真软件，它的内核和模块都是用C++语言实现的。实质上，ns-3可以认为是一个动态或者静态的库，这些库函数（API）可以供由C++语言所编写的仿真程序调用。同样，ns-3也实现了支持Python语言库函数，Python仿真程序可以像C++语言调用库函数的方法调用ns-3提供的API。也就是说，ns-3提供2种语言的支持，用户只需要懂得其中一种语言就可以轻松地使用ns-3来编写自己的仿真程序。 ​ ns-3是一个开源的软件，它的整体组织结构在src目录中实现。从上到下ns-3的每个模块仅仅依赖于它的底层模块，也就是说底层模块为上层模块提供服务。 ​ 首先来认识一下内核模块，内核模块对于现在所有的协议、硬件以及场景模型都是通用的，可以在目录src/core中查看内核的源代码。分组在网络中是最基本的单元，因此在仿真中对分组的刻画也是必不可少的，在src/network中读者可以找到有关分组的实现代码。内核和网络这2个模块的实现旨在为不同的网络仿真提供一般性服务，而不仅仅是单纯地只为因特网服务。而在网络模块以上的模块的使用要依赖于不同的网络仿真，针对不同网络仿真程序使用的ns-3模块也是大有不同的，如对于移动网络需要移动模型，不同的移动网络，其移动模型还可能不一样，这都要针对具体的网络要求来定。 二.随机变量1.背景​ 因网络仿真实验中，CPU将花掉不少时间来处理随机数，因此用户不得不去关注随机数的效率和不同随机数之间的独立性，下面是用户不得不经常考虑的问题： 在仿真实验中使用到的随机数是否有真实的随机性或可靠的随机性 在多次实验中如何保证随机数之间的独立性 一个随机数生成序列的周期长度 首先得了解上述几个术语，每一个RNG（随机数生成器）都会提供一个相当长的随机数序列，这个序列的长度称为循环周期或者循环长度，RNG在完成一次循环后将自动进行下一次重复。这个序列可以被分隔为几个相互没有关联的数据流。一个RNG的随机数据流是这个RNG序列的一个连续子集。例如，一个RNG队列长度为N，同时RNG提供2个数据流，第一个数据流使用序列的前半部分，第二个数据流使用序列的后半部分，当然这里最重要的是：这2个数据流是相互独立的。同样，每一个数据流还可以被分为更小的子流。一个RNG最希望做到的就是提供一个非常长的循环序列，并有效地分配给每一个数据流。 2.综述​ ns-3的随机变量是通过调用类ns3：：RandomVariableStream来提供的。类ns:：Random VariableStream 是对底层的随机数生成器进行封装并提供相关的接口供用户使用（Uniform VariableStream 和ExponetialVariableStream是RandomVariable Stream的子类）。 ​ 如果仿真程序具有随机性，而要在相同的仿真中体现结果的随机性，那么就要提供不同的种子或者运行标识。可以通过函数 ns3：：RngSeedManager:SetSeed()来设置种子，同样通过函数ns3：：RngSeedManager:SetRun）设置运行标识。 ​ ns-3中用到的每一个随机变量都有一个随机数生成器与之相关联，所有的随机变量都使用一个基于全局固定的或者随机的种子。 3.播种与独立重复​ 通过对ns-3进行配置可以让输出结果是确定的也可以是随机的。如果ns-3在仿真模拟中使用固定的种子和不变运行标识，那么每一次仿真结果都是固定不变的。ns-3默认使用固定的种子和标识，它们分别存储在类GlobalValue的g_rmgseed和g_rmgRun成员中。 ​ 但是，人们通常会对同一仿真程序进行多次运行实验，对结果进行统计分析。为了使结果更具有科学性，那么应该让每次实验都具有独立性，这里提供2种方法。 在每一次独立重复实验时，调用RngSeedManager:：SetSeed）为其设置不同的全局种子。 示例如下，可以通过SetSeed传入不同参数使其具有独立性，但每次实验都要修改源代码，显然不明智 在保持种子不变的情况下，通过改变运行标识来保证实验的独立性。改变标识的方法有以下4种。 a.通过函数RngSeedManager:SetRun（3）改变运行标识。 b.通过修改环境变量NSGLOGALVALUEB的值来修改运行标识。代码如下： 1NS GLOBAL VALUE="RngRun=3" ./waf --run program-name c.通过命令行传递参数来修改运行标识，如下： 1./waf --run "program-name --RngRun=3" d.还有一种方式就是不用waf，而是直接用build。如： 1./build/optimized/scratch/program-name --RngRun=3 建议使用第3种方式。 在这里提倡在保持种子不变的情况下，通过改变运行标识保证独立性，因为在改变种子的情况下进行重复实验，ns3无法保证每个种子对应的RNG序列不会重复，而使用相同种子对应的RNG子序列就能保证每个序列不会重复。 这里要注意的是在改变运行标识时，程序的种子是没有改变的，所以对应的RNG序列没有变化，只是把这个序列分割成很多小的子序列来供每次运行使用。 4.随机变量​ 下面介绍ns3中提供的用来声明随机变量的类。首先，所有用来声明随机变量的类都有一个基类Random VariableStream。下面简单介绍从RandomVariableStream继承的派生类。 UniformRandomVariable：最基础也是最常用的类。给定一个最大值和最小值，按均匀分布的方式返回一个随机数。 ConstantRandomVariable：返回一个固定的数，默认值为0。 SequentialRandomVariable：返回一串等差序列，如果超过了给定的最大上限则重新从给定的最小值开始再次循环。 ExponentialRandomVariable：根据指数概率分布返回随机数。 NormalRandomVariable：根据正态分布返回随机数。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ns3关键概念]]></title>
    <url>%2F2019%2F07%2F05%2Fns3%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[一.关键概念1.节点​ 在因特网术语中，主机即终端是指任何一台连接到网络的计算设备。ns-3并非一个专门的因特网模拟器，而是一个网络模拟器。为此不采用术语“主机”，因为这个词太容易让人联想到因特网和及其相关协议。因此，选用其他术语：节点。 ​ ns-3中基本计算设备被抽象为节点。你应该将节点设想为一台可以添加各种功能的计算机。为了使一台计算机有效地工作，可以给它添加应用程序、协议栈、外设卡及驱动程序等。ns-3采用了与此相同的模型。 ​ 节点由C++中的Node类来描述。Node类提供了用于管理仿真器中网络组件表示的各种方法。例如NodeContainer类，用于追踪一组节点指针。通常，ns-3助手一次在多个节点上工作，如果一个设备助手可能在大量的相同节点上安装设备。助手采用的安装方法通常需要NodeContainer作为一个参数。NodeContainers承载多个Ptr用来指向参考节点。下面两行将会创建ns-3节点对象，它们在仿真中代表计算机。 12NodeContainer nodes;nodes.Create(2); ​ NodeContainer的拓扑生成器提供了一种简便的方式来创建、管理和使用任何节点对象，本文用这些节点来运行模拟器。上面的第一行只是声明了一个名为“nodes”的NodeContainer。第二行调用了nodes对象的Create()方法创建了2个节点。这个容器调用ns-3中的内部函数来产生2个节点对象，并把指向这2个对象的指针存储在系统中。在脚本中它们所代表的节点什么都没有做。构建拓扑的下一步是把节点连接到一个网络中。 2.应用​ 计算机软件通常分为系统软件和应用软件两大类。应用程序需要使用由系统软件控制的资源，而系统软件通常并不直接使用这些资源来完成用户任务。用户往往需要运行应用程序来完成一些特定的任务。系统软件根据计算模型配置和管理计算机中的各种资源，例如内存、处理器周期、硬盘、网络等。 ​ 通常，系统软件和应用软件的界线表现为特权级的变化，而这种变化是通过操作系统的自陷功能（operating system traps）来实现的。在ns-3中并没有真正的操作系统的概念，更没有特权级别或者系统调用的概念。正如“现实世界”中在计算机上运行应用程序以执行各种任务一样，我们有应用程序的概念，ns-3仿真环境中的应用程序在节点上运行来驱动模拟过程。 ​ 在ns-3中，需要被仿真的用户程序被抽象为应用。应用在C++中用Application类来描述。这个类提供了管理仿真时用户层应用的各种方法。开发者应当用面向对象的方法自定义和创建新的应用。本文中会使用Application类的实例：UdpEchoClientApplication 和UdpEchoServerApplication。这些应用程序包含了一个client/server应用来发送和回应仿真网络中的数据分组。其中，UdpEchoClientApplication用来回显服务器端所回复的分组。同其他生成器对象类似，UdpEchoServerApplication由 UdpEchoServerHelper 管理。实际上是这个方法的执行才初始化回显服务器的应用，并将应用连接到一个节点上。应用对象需要一个时间参数来“开始”产生数据通信并且可能在一个可选的时间点“停止”。本文提供了开始和停止的2个参数。这些方法以“Time”对象为参数，记录接收时间和发出时间。echo客户端应用的设置与回显服务器端类似。也有一个UdpEchoClientHelper 来管理 UdpEchoClientApplication。UdpEchoServerApplication是回显服务器端所接收到的分组，同时可以记录时间。 3.信道​ 通常把网络中数据流流过的媒介称为信道。当你把以太网线插入到墙壁上的插孔时，你正通过信道将计算机与以太网连接。在ns-3的模拟环境中，你可以把节点连接到代表数据交换信道的对象上。在这里，基本的通信子网这一抽象概念被称为信道，在C++中用channel类来描述。 ​ Channel类提供了管理通信子网对象和把节点连接至它们的各种方法。信道类同样可以由开发者以面向对象的方法自定义。一个信道实例可以模拟一条简单的线缆（wire），也可以模拟一个复杂的巨型以太网交换机，甚至可以是无线网络中充满障碍物的三维空间。 ​ 本文中将使用几个信道模型的实例，包括：CsmaChannel、Point ToPointChannel和WifiChannel。 ​ 举例来说，CsmaChannel信道模拟了一个可以用于实现载波侦听多路访问通信子网中的媒介，这个信道具有和以太网相似的功能。Point ToPointChannel这个类代表一个简单的点对点信道，此通道上没有多点通信能力，可以有最多2个点至点连接的网络设备。在通道中有2种“线”。第一个连接的设备获取0号线上的传输，第二个设备得到1号线上的传输。每根线有空闲或传输状态。Wi-FiChannel此无线通道实现描述为“又一网络模型”传播模型，在默认情况下，如果没有传播模型需要设置，则是调用者使用通道设置。 4.网络设备​ 以前，如果想把一台计算机连接到网络上，你就必须买一根特定的网络线缆，并在你的计算机上安装称为外设卡的硬件设备。能够实现网络功能的外接卡被称为网络接口卡（网卡）。现在大多数计算机出厂时已经配置了网卡，所以用户看不到这些模块。 ​ 一张网卡如果缺少控制硬件的软件驱动是不能工作的。在UNIX（或Linux）系统中，外围硬件被称为“设备”。设备通过驱动程序来控制，而网卡通过网卡驱动程序来控制。在UNIX和Linux系统中，网卡被称为像eth0这样的名字。 ​ 在ns-3中，网络设备这一抽象概念相当于硬件设备和软件驱动的总和。ns3仿真环境中，网络设备安装在节点上，使得节点通过信道和其他节点通信。像真实的计算机一样，一个节点可以通过多个网络设备同时连接到多条信道上。 ​ 网络设备由C++中的NetDevice类来描述。NetDevice类提供了管理连接其他节点和信道对象的各种方法，并且允许开发者以面向对象的方法来自定义。本文中将使用几个特定的网络设备作为实例，它们分别是CsmaNetDevice、PointToPointNetDevice 和Wi-FiNetDevice。正如以太网卡被设计成在以太网中工作一样，CsmaNetDevice被设计成在CSMA信道中工作，而Point ToPointNetDevice在PointToPoint信道中工作，Wi-FiNetNevice在Wi-Fi信道中工作。 ​ 如果需要一个所有被创建的NetDevice对象列表，就需要使用一个NetDeviceContainer 对象来存放它们，就像本文使用一个NodeContainer对象来存放所创建节点。一个NetDeviceContainer被创建了。对于在NodeContainer对象中的每一个节点（对于一个点到点链路必须明确有2个节点），一个PointToPointNetDevice 被创建和保存在设备容器内。一个Point ToPointChannel对象被创建，2个Point ToPointNetDevices与之连接。 5.拓扑帮助​ 在现实的网络中，你会发现主机已装有（或者内置）的网卡。在ns-3中你会发现节点附加了网络设备。在大型仿真网络中，你需要在节点、网络设备和信道之间部署许多连接。 ​ 既然把网络设备连接到节点、信道、配置IP地址等在ns-3是很普遍的任务，那么干脆提供“拓扑生成器”来使这个工作变得尽可能的容易。举例来说：创建一个网络设备，配置一个MAC地址，把此网络设备装载到节点上，设置节点的协议栈以及连接网络设备到一个信道，这些事情都需要许多分立的ns-3核心操作。而当需要把许多设备连接到多点信道，在网际间将单个网络进行连接时，则需要对ns-3核心进行更多的分立操作。本文提供了拓扑生成器来整合这些大量分立的步骤，使其成为一个简单易用的操作。 ​ 很明显，Helper类可以极大地方便用户。例如TopologyReaderHelper类可以使得更容易配置和使用通用的TopologyReader。再如类InternetStackHelper是一个安装PointToPointHelper对象和点到点网络设备的网络协议栈的拓扑生成器类，它会为每一个节点容器中的节点安装一个网络协议栈（如TCP、UDP和IP等）。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>ns3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++常用数据结构总结]]></title>
    <url>%2F2019%2F07%2F03%2FC%2B%2B%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[C++常用数据结构总结 前言 所有的容器归根到底都是内存空间的排列方式和在空间上施加各种各种不同的限制所得的。空间排列方式只有线性和链式两种方式，链式是通过记录每一个数据的地址来实现查找下一位数据的。而每一个容器所具有的特性就决定了它所适用的情况，总的来看容器常用的无非是增删改查操作，下面将从适用场景、常用操作来进行总结。 array数组内存空间为连续的一段地址，适用于提前已知所要存储的数据类型和数量、进行大量的查、改操作，不适用于含有大量交换、删除、增加数据的操作，该容器无法动态改变大小，所以说提前已知存储数据类型和数量。图片介绍了数组的初始化、赋值、遍历、获取大小、获取特定位置数据的方法。 queue队列该容器内存结构最好为链式结构，最知名的特点是先进先出，能动态调整大小，适用于包含大量增、删操作的情况，但不适用于含有大量查找操作的数据。图片介绍了队列初始化、赋值、弹出操作。 stack 栈栈在内存上可为连续或者链式，于队列相反的是它为先进后出，适用于压栈出栈操作，如可用于图的遍历、递归函数的改写等，图片介绍了栈的创始化、压栈、出栈等操作。 list 链表链表在内存结构上为链式结构，也就决定它可以动态增加，适用于包含大量增加、删除的应用，但不适用于包含大量查询的操作，图片介绍了链表的创建、添加数据、删除数据、获取数据等操作。 mapmap为关联式容器，提供一对一服务，每个关键字在容器中只能出现一次，适用于一对一服务。 set 集合set集合最大的特点是里面的元素按序排列不重复，图片演示集合初始化、插入、删除、查找等操作。 vector向量vector向量和array不同，它可以根据数据的大小而进行自动调整，图片仅展示初始化、插入、删除等操作。 总结: 这些容器在http://www.cplusplus.com/网站都有详细的介绍，看文档很容易学会它们，毕竟实现过程都隐藏起来了，多加实践即可掌握]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5G关键技术(三)---小基站大机遇]]></title>
    <url>%2F2019%2F04%2F26%2F5G_3%2F</url>
    <content type="text"><![CDATA[5G关键技术(三)—-小基站大机遇与3G，4G相比，5G的新兴技术主要是毫米波（点我查看）与波束成形（点我查看）。此外，在载波聚合，多天线输入输出（MIMO）等4G技术上有了新的演进。那么，他对集成电路设计带来了怎样的挑战呢？今天，我们就来预测下5G挑战下，集成电路的新趋势——小基站。 最近又个词很火，叫做“一言不合”，这让小编想到了一些故事。 某天，在我家对面的中电信服务点上竖起了一个不高不低的铁架子，上面有两个金属盒子，还有两根高高竖起的天线。在经过了一场盛大广场舞聚会预谋后，小区的女性同胞们开始致力于完成一项具有里程碑式意义的革命—— 一言不合 我就要去抗议基站在我家门口 几天之后，中电信就只能乖乖的把那个铁架子给撤了。 这应该是我看到最有效率的一言不合吧。 我无法告诉大家基站会不会影响麻麻肚子里的宝宝，但是大家真的以为移动运营商会把基站撤走么？如果撤了，那你手机的信号格数应该就是最低的那一档了吧。 Too Simple SOMETIMES NAIVE 在摩尔定律的发展最直接的变化就把巨无霸变成袖珍丸。 请看以下今天的基站长什么样： 你以为的基站（左） Vs 基站实际的样子（右）。 很多人会问一个问题，右边那张图不是一只路灯么？这就是小基站，可以伪装在路灯，公交车站牌和任何不起眼角落的变色龙。（这又是一个摩尔定律带来的生活改变） 随着集成电路的发展，终端芯片的SoC渐渐走到了穷途末路，而基站芯片的SoC却正在澎湃发展。由此，小基站在4G时期成为了对抗广场舞大妈抗议的重要技术手段。 问题来了： 5G下，小基站会更有意义么？ 目前，限制小基站的主要是无线天线尺寸的大小。一般要求天线的尺寸与电磁波波长在同一个数量级，而电磁波波长就是光速除以频率。3G/4G的载波波长在分米级，小基站的天线的长度也在类似。但是在5G下，载波波长变成了毫米级（”[毫米波]”的原因）。所以天线可以做的更小，做得更多（实现[波束成形]和Massive MIMO）。 小基站（small cell）的体积和称呼从 micro cell，nano cell, pico cell已经进化到femto cell。他们的主要应用场景在于人口密集区，覆盖大基站无法触及的末梢通信。特别是完成号称100Mbps-1GMbps的5G通信。小基站让你工作闲暇之余，在一分钟内下完一集《权利的游戏》高清成为了可能。未来，可以预期的是他会像你家的路由器一样小，藏在CBD和大型Shopping mall的角角落落。抗议大基站（marco station）的麻麻们是不是在上班和逛街的时候考虑下呢？ 小基站的实现，除了摩尔定律带来的高歌猛进外，还有很多智慧的硅工付出的辛劳努力。比如——非线性功放的数字预畸变（Digital Pre-distortion for Nonlinear Power amplifier.）。小基站不仅在规模上要远远小于大基站，在功耗上也是必然指数式下降，毕竟占的是人家建筑里220V的市电。随着集成电路的演进，计算功耗不断降低，可是射频发射机信号的发射功率可是没有多大变化的，毕竟这是由协议灵敏度决定。在大基站里，我们可以用非硅的工艺实现高线性度功放，反正功耗不care。但是在理想的小基站里，PA也是做成SoC的。CMOS工艺的功放在线性工作范围的低效率闻名遐迩，在大功率的输出下功率即将饱和。预期单纯地被限制在线性区是“坐井观天“。 于是天才型硅工就提出从在数字域寻找非线性PA的反函数，然后输出一个非线性的数字控制码。两者叠加，就有了一个线性的高效率输出。 然后，这个问题的解法又再一次地普及到频域，当宽带功放在带内的传递函数有波动时，也在数字域寻找其波动的逆函数，给出带有频率选通特性的调制结果，然后一叠加，又能看到幅度一致的EMV mask了。 这一思想的核心，就是把不随摩尔定律变化的射频功耗等转化为跟着摩尔定律走的数字计算功耗，所谓as much digital as possible. 在各式各样的努力下，5G小基站变成了PokemonGo中的小精灵般散落在人间。于是新的问题的就出现了，这那么多小基站，万一被踩一脚挂了怎么办？况且其灵活性体现在自由方便可配置上，如果动不动就要打电话给运营商派辆车过来，是否还合适呢？ 为了解决这个问题，我们要来介绍小基站实现中的另一个机制——自组网（SelfOrganizing Network, SON）。为了更好的，方便的对具有灵活性的小基站群进行配置、优化和修复，自适应的组网技术将取代大基站中繁杂的介入成本。 有关调查指出，在5G应用场景中，50%以上的通信资源被1%的终端占用，而这1%往往在大城市的中心地带和商业区。这些地区的实际通信场景复杂，需要可配置度高的网络。更有甚者，这些区域的物联网也比较丰富。在种种情况交叠下，SON可以被看作是5G通信与物联网通信的的桥梁，为这样的区域提供更有效的组网通信系统。 如果有服务导向的自组网成为可能，那么未来的小基站的实际运营上可能将从移动运营商部分地转换器实体商家和其他小企业运营单位。未来的“网管”不仅管着交换机、无线网，还要管都市移动通信。听上去是不是碉堡了？ 结束的话 由于社会和通信等一系列的原因，未来小基站会逐渐成为5G通信中不同于大基站的重要增长点，特别在城市CBD区域。毫米波将导致小基站的伪装更加让人难以分辨，与此同时，各种数字化的校准方法也实现了大基站到小基站的低功耗实现。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5G关键技术(二)---波束成型]]></title>
    <url>%2F2019%2F04%2F26%2F5G_2%2F</url>
    <content type="text"><![CDATA[5G关键技术(二)—-波束成型经典通讯问题：如何才能让很多人在一个房间内说话互不干扰 在上一期文章中我们介绍了如何利用毫米波技术来获得更多的频谱资源，那么，接下来的问题是我们在有了频谱资源后如何充分利用，就是如何让多个用户通讯但又互不干扰，专业术语叫做频谱复用。大家一定有过这样的经验，在一间房间里当人不多时，手机信号很好，但是当许多人聚集到房间里的时候，手机的信号就会变差，有时甚至无法打出电话。这种现象归根到底就是频谱复用做得不够好，无法给所有人分配必需的频谱资源。 在上一期文章中我们介绍了如何利用毫米波技术来获得更多的频谱资源，那么，接下来的问题是我们在有了频谱资源后如何充分利用。这涉及通讯理论的多路复用通讯(multiple access)问题，即如何让多个用户通讯但又互不干扰？在大学的通讯理论课本中，会介绍几种经典的复用方式。第一种是时分复用(time-division multiple access, TDMA，典型应用：中国移动2G)，即让给每个用户分配一个专用的时间段，每个用户只在自己的时间段内通讯。显然，时分复用降低了每个用户的平均数据传输速率，因为在大多数时候用户并不能进行通讯而必须等轮到自己时才能进行数据传输。第二种经典的复用方法是频分复用(frequency-division multiple access, FDMA，典型应用：中国联通3G)，即把频谱分成多个信道，给每个用户分配一个信道，这样多个用户可以同时通讯而不会互相干扰。然而，这样每个用户只能获得一部分频谱资源，虽然多个用户可以同时通讯但是每个用户无法做到全速传输。第三种复用方法是码分复用(code-division multiple access, CDMA，典型应用：中国联通3G)，即每个用户被分配一个唯一的m比特码片序列，发送的每个数据比特均被扩展成m位码片（扩频）。每个用户的比特码片序列互相正交，因此每个用户能互不干扰地通讯。码分复用抗干扰能力很强，然而用户需要传输m比特码片才能真正传输1比特数据，因此效率并不高。可以用一个例子来说明时分复用、频分复用和码分复用的区别。在一个屋子里有许多人要彼此进行通话，为了避免相互干扰，可以采用以下方法： 1) 讲话的人按照顺序轮流进行发言（时分复用）。 2) 讲话的人可以同时发言，但每个人说话的音调不同（频分复用）。 3) 讲话的人采用不同的语言进行交流，只有懂得同一种语言的人才能够相互理解（码分复用）。 当然，这三种方法相互结合，比如不同的人可以按照顺序用不同的语言交流（即中国移动3G的TD-SCDMA）。然而，这三种经典的复用方式都无法充分利用频谱资源，它们要么无法多用户同时间通讯(TDMA)，要么无法使用全部频谱资源(FDMA)，要么需要多比特码元才能传递1比特数据(CDMA)。那么，有没有一种方法可以克服以上多路方式的缺点，让多个用户同时使用全部频谱通讯呢？让我们先来思考一下，如果在一个房间里大家同时用同一种音调同一种语言说话会发生什么？很显然，在这种情况下会发生互相干扰。这是因为信号会向着四面八方传播，所以一个人会听到多个人说话的声音从而无法有效地通讯。但是，如果我们让每个说话的人都用传声筒，让声音只在特定方向传播，这样就可以多个人同时用同音调同语言说话但是不会互相干扰。在无线通讯中，也可以设法使电磁波按特定方向传播，从而在不同空间方向的用户可以同时使用全部频谱资源不间断地进行通讯，也即空分复用(space-division multiple access，SDMA)。SDMA还有另一重好处，即可以减少信号能量的浪费。当无线信号在空间中向全方向辐射时，只有一小部分信号能量被接收机收到成为有用信号。大部分信号并没有被相应的接收机收到，而是辐射到了其它的接收机成为了干扰信号。当使用SDMA时，信号能量集中在特定的方向，一方面减少了对其它接收机的干扰，一方面也减小了信号能量的浪费。在5G通讯中，SDMA是大规模MIMO(massive Multiple-Input Multiple-Output，指在发射端和接收端分别使用大规模发射天线和接收天线阵列，使信号通过发射端与接收端的大规模天线阵列传送和接收，从而改善通信质量)技术应用的一个重要例子，而将无线信号（电磁波）只按特定方向传播的技术叫做波束成型(beamforming)。有了波束成型，众多小伙伴就可以同时在同一个地方欢乐地刷手机上网而不用担心没信号的问题啦！ 什么是波束？ “波束”这个词看上去有些陌生，但是“光束”大家一定都很熟悉。当一束光的方向都相同时，就成了光束，类似手电筒发出的光。反之，如果光向四面八方辐射（如电灯泡发出的光），则不能形成光束。和光束一样，当所有波的传播方向都一致时，即形成了波束。 工程师利用波束已经有相当久的历史。在二战中，工程师已经将波束利用在雷达中。雷达通过波束的反射时间来计算波束方向物体的距离，并通过扫描波束方向来探测整个空间中所有目标的位置。如果雷达不使用波束而使用全方向辐射的电磁波，则雷达将无法确定空间物体的具体位置。另一个例子是卫星通讯，也即我们生活中常见的“锅盖天线”。卫星通讯使用波束的目的是为了补偿信道的衰减。卫星和地面接收天线的距离非常远，信道衰减非常大，于是卫星信号到达地面时能量已经非常小，甚至比热噪声还要低。因此，我们需要想方设法收取卫星发出的每一点信号能量。当卫星的信号向空间全方向辐射时，绝大多数能量并没有被地面天线接收到，而是被浪费了。为了避免这种浪费，我们在接收和发射卫星信号时，都会使用波束。这样，发射的电磁波信号都集中在一个方向上，只要接收天线能对准这个方向，就可以接收到每一点信号。 如何实现波束成型 光束很简单实现，只要用不透明的材料把其它方向的光遮住即可。这是因为可见光近似沿直线传播，衍射能力很弱。然而，在无线通讯系统中，信号以衍射能力很强的电磁波的形式存在。由于无线通讯使用的电磁波衍射能力很强，所以无法使用生成光束的方法来实现波束成型，而必须使用其他方法。 无线通讯电磁波的信号能量在发射机由天线辐射进入空气，并在接收端由天线接收。因此，电磁波的辐射方向由天线的特性决定。天线的方向特性可以由辐射方向图（即天线发射的信号在空间不同方向的幅度）来描述。普通的天线的辐射方向图方向性很弱（即每个方向的辐射强度都差不多，类似电灯泡），而最基本的形成波束的方法则是使用辐射方向性很强的天线（即瞄准一个方向辐射，类似手电筒）。然而，此类天线往往体积较大，很难安装到移动终端上（想象一下iPhone上安了一个锅盖天线会是什么样子）。另外，波束成型需要可以随着接收端和发射端之间的相对位置而改变波束的方向。传统使用单一天线形成波束的方法需要机械转动天线才能改变波束的方向，而这在手机上显然不可能。因此，实用的波束成型方案使用的是智能天线阵列。 智能天线阵列原理并不复杂，主要涉及的知识范围是高中物理教的波的干涉。当由两个波源产生的两列波互相干涉时，有的方向两列波互相增强，而有的方向两列波正好抵消（如下图）。 在波束成型中，我们有许多个波源（即天线阵列），通过仔细控制波源发射／接收的波之间的相对相位和幅度我们可以做到电磁波辐射／接收增益都集中在一个方向上（即接收机／发射机所在的位置），而在其他地方电磁波辐射／接收增益都很小（即减少了对其他接收机的干扰／减小了被其他发射机干扰的机会）。我们以接收天线阵列为例。对于沿我们想要方向传播的电磁波，波前到达天线阵列中每个天线的时间（相位）均有所不同。对于每一个天线，我们都加入一个特定的相位延迟用来补偿波前到达天线相位的区别，因此在经过该相位延迟后，我们就把每个天线收到的信号在相位上对齐了，从而不同天线接收到的有用信号在经过加和后会幅度变得很大。另一方面，当沿其它方向传播的干扰信号到达天线阵列时，每个天线对应的延迟与信号到达天线的时间差并不符合，因此在加和后幅度并不会变大。这样，天线阵列就可以通过多个普通天线配合特定的延迟来等效实现具有方向性的天线。根据天线的互易性原理，相同的架构也可以用在发射天线阵列里去等效一个高方向性的天线。此外天线辐射的方向可以通过改变波源之间的相对延时和幅度来实现，可以轻松跟踪发射端和接收端之间相对位置的改变。 波束成型与毫米波是天作之合 目前波束成型已经被使用在带有多天线的WiFi路由器中。然而，手机上不可能像路由器一样安装WiFi频段的多根天线，因为天线尺寸太大了。天线的尺寸是由电磁波信号的波长决定的，WiFi和当前手机频段的电磁波波长可达十几厘米，因此很难将如此大的天线集成在手机上。为了解决这个问题，我们可以把波束成型和毫米波技术结合在一起。毫米波波段的波长大约是WiFi和手机频段波长的十分之一左右，因此可以把多个毫米波天线集成到手机上，实现毫米波频段的波束成型。波束成型和毫米波技术可谓是天作之合，使用毫米波可以给信号传输带来更大的带宽，波束成型则能解决频谱利用问题，使得5G通讯如虎添翼。 毫米波天线阵列体积很小，可以安装到手机上 结语 波束成型可以使信号的能量集中在接收端所在的方向，从而改善频谱利用效率。波束成型配合毫米波技术可以让通讯系统拥有高带宽并且支持大量用户同时通讯，从而使5G系统如虎添翼。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5G关键技术(一)---毫米波]]></title>
    <url>%2F2019%2F04%2F26%2F5G_1%2F</url>
    <content type="text"><![CDATA[5G关键技术(一)—-毫米波第五代移动通信系统 （5th generation mobile networks，简称5G）离正式商用（2020年）越来越接近，这些日子华为、三星等各大厂商也纷纷发布了自己的解决方案，可谓“八仙过海，各显神通”。 5G的一个关键指标是传输速率：按照通信行业的预期，5G应当实现比4G快十倍以上的传输速率，即5G的传输速率可实现1Gb/s。这就意味着用5G传输一部1GB大小的高清电影仅仅需要10秒！从此以后手机不用连WiFi就能看在线高清视频了（当然前提是你有足够的流量）。另外如此高的传输速度也会带来一些其他的应用，比如云端游戏（游戏在云端服务器执行，直把执行画面传回手机，这样手机配置不高也能玩大型游戏），虚拟现实（同理把运算放到云端，手机端只负责输出画面）等等。 5G如何实现如此高的传输速率呢？ 无线传输增加传输速率大体上有两种方法，其一是增加频谱利用率，其二是增加频谱带宽。在无线传输中，数据以码元(symbol)的形式传送。在码元传送速率（码率）不变的情况下，信号占用的无线带宽不变，而每个码元传送的信息数据量是由调制方式决定的。 调制方式是指如何用信号传递信息。在古代，人们用烽火台传递信息，有情况的时候点燃烽火，每有情况的时候熄灭烽火。从现代通讯理论来说，就是我们调制了烽火。由于普通的烽火一共只有两种状态（点燃和熄灭），因此烽火台一次只能传递1比特的信息（0=熄灭=没有敌人，1=点燃=有敌人）。烽火台能不能改善一下来一次传递更多信息呢？我们可以通过引入更多状态来实现这一点。例如，改进的烽火台里面我们可以控制烽火的火势，将火势分为熄灭、小火、中火和大火四种状态，这样我们就可以一次传递两比特的信息（00=熄灭=没有敌人，01=小火=有敌人且离我们很远，10=中火=有敌人且离我们不远，11=大火=有敌人且已经兵临城下）。然而，天下没有免费的午餐，引入更多状态的同时也会增加信息传递出错的可能。例如如果天气不好的时候可能会把中火看成小火，这样信息的传递就出错了。相对地，如果只有两种状态（熄灭和点燃），则出错的几率比较小。 无线通讯中的调制也是这个道理，通过操纵无线电波的幅度和相位可以产生载波的不同状态。当调制方式由简单变到复杂时，载波状态数量增加，一个码元所代表的信息量（比特数）也增加。在最简单的QPSK调制中，传送任何一个码元时载波的幅度不变，而相位可能是0，90，180或270度中的一个（所有码元相对应的载波幅度和相位画在直角坐标系里就是调制方式的星座图）。因此，根据信息论，一个QPSK调制的码元可以传送2bit的数据。当使用复杂的调制方式时，每个码元可能出现的位置变多，因此每个码元所携带的信息也增多。相对于最左边的基础QPSK调制，使用QAM16和QAM64调制每个码元所携带的信息分别是4 bit和6 bit，因此可以把频谱利用率分别提高2倍和3倍 （QAM256的图我就不放了，容易引发密集恐惧症）。但另一方面每个码元状态之间的间距也变小，因此容易受到噪声干扰使得码元偏离原本应该在的位置从而造成解码出错。所以复杂调制对信道的要求比较高，在信道噪声很大的情况下使用复杂调制会导致数据传输误码率很高，而且解码所需要的电路也会非常复杂，导致功耗很大。由简单的QPSK（左）到复杂的QAM64（右）调制的状态图 相对于提高频谱利用率，增加频谱带宽的方法显得更简单直接。在频谱利用率不变的情况下，可用带宽翻倍则可以实现的数据传输速率也翻倍。但问题是，现在常用的5GHz以下的频段已经非常拥挤，到哪里去找新的频谱资源呢？各大厂商不约而同想到的方法就是使用毫米波技术。 毫米波是什么，毫米波的特点 毫米波是指波长在毫米数量级的电磁波，其频率大约在30GHz~300GHz之间。根据通信原理，无线通信的最大信号带宽大约是载波频率的5%左右，因此载波频率越高，可实现的信号带宽也越大。在毫米波频段中，28GHz频段和60GHz频段是最有希望使用在5G的两个频段。28GHz频段的可用频谱带宽可达1GHz，而60GHz频段每个信道的可用信号带宽则到了2GHz（整个9GHz的可用频谱分成了四个信道）。相比而言，4G-LTE频段最高频率的载波在2GHz上下，而可用频谱带宽只有100MHz。因此，如果使用毫米波频段，频谱带宽轻轻松松就翻了10倍，传输速率也可得到巨大提升。换句话说，使用毫米波频段我们可以轻轻松松用手机5G在线看蓝光品质的电影，只要你不怕流量用完！ 各个频段可用频谱带宽比较 毫米波频段的另一个特性是在空气中衰减较大，且绕射能力较弱。换句话说，用毫米波实现信号穿墙基本是不可能。但是，毫米波在空气中传输衰减大也可以被我们所利用，所谓”It’s not a bug，it’s a feature！”:你手机使用的毫米波信号衰减确实比较大，但是同样地其他终端发射出的毫米波信号（对你而言是干扰信号）的衰减也很大，所以毫米波系统在设计的时候不用特别考虑如何处理干扰信号，只要不同的终端之间不要靠得太近就可以。选择60GHz更是把这一点利用到了极致，因为60GHz正好是氧气的共振频率，因此60GHz的电磁波信号在空气中衰减非常快，从而可以完全避免不同终端之间的干扰。 当然，毫米波在空气中衰减非常大这一特点也注定了毫米波技术不太适合使用在室外手机终端和基站距离很远的场合。各大厂商对5G频段使用的规划是在户外开阔地带使用较传统的6GHz以下频段以保证信号覆盖率，而在室内则使用微型基站加上毫米波技术实现超高速数据传输。 毫米波必须配合微型基站（或接入点）使用 毫米波相比于传统6GHz以下频段还有一个特点就是天线的物理尺寸可以比较小。这是因为天线的物理尺寸正比于波段的波长，而毫米波波段的波长远小于传统6GHz以下频段，相应的天线尺寸也比较小。因此我们可以方便地在移动设备上配备毫米波的天线阵列，从而实现各种MIMO（Multiple-Input Multiple-Output，指在发射端和接收端分别使用多个发射天线和接收天线，使信号通过发射端与接收端的多个天线传送和接收，从而改善通信质量）技术，包括波束成型（有关波束成型，我们会在下一篇文章里面详细介绍）。 毫米波收发机芯片如何实现 NICT研发的毫米波收发机架构图 商用的毫米波收发机芯片会使用CMOS工艺（CMOS=complementary metal-oxide-semiconductor，指用半导体-氧化层-金属堆叠形成半导体器件的工艺，是最常用的集成电路制造工艺），这一方面为了能够和数字模块集成，另一方面为了节省成本。 毫米波收发机芯片的结构和传统频段收发机很相似，但是毫米波收发机有着独特的设计挑战。 其一是如何控制功耗。毫米波收发机要求CMOS器件能工作在毫米波频段，所以要求CMOS器件对信号的灵敏度很高。我们可以参照日常生活中的水龙头来说明这个问题。大家一定都经常有开关水龙头的经验，很多水龙头在关着时，需要拧很多下才会出来一点点水，然后随着水流越来越大，只要多拧一点点水流就会变大很多。在这里，手拧龙头的动作就是激励信号，而对应的水流变化就是输出响应。CMOS器件本质上和水龙头很像，都是通过控制端（即CMOS的栅极）调整输出流量（对水龙头是水流，对CMOS则是输出电流）。因此，如果需要CMOS器件对微弱的毫米波信号能快速响应，必须把它的直流电流调到很大（相当于把水龙头设置在水流很大的状态）。这样一来，CMOS电路就需要很大的功耗才能处理毫米波信号。 说得专业一点，CMOS器件的工作原理是栅端电压控制源端到漏端的载流子，从而控制源漏端的电流。当加在栅端的信号发生变化的时候，源漏端的电流也会发生相应变化，因此就起到了信号放大的作用。然而，如果源端的载流子还来不及走到漏端时栅端的信号就发生了改变，那么栅端的信号就无法得到有效放大。通常把CMOS器件能工作的最高频率称为截止频率。那么如何提高截止频率呢？在器件工艺不变的前提下，改进截止频率的方法就是增加载流子速度，让它们能赶在栅端信号变化之前就到达漏端。这就意味着我们可以通过加强沟道电场，即提高栅-源电压来改善截止频率。然而，提高栅-漏电压也意味着CMOS器件的直流电流也变大。由于毫米波频段已经和CMOS器件的截止频率在同一个数量级上(~100 GHz)，毫米波收发机芯片必须仔细设计才能把功耗控制在移动设备可以接受的范围内。 另一个毫米波芯片必须考虑的问题是传输线效应。相信大家还记得高中物理里面的受力分析，（下图左）分析一根静止绳子的受力情况（静力分析）是很简单的，绳子的弹力即等于人对绳子的拉力，而且每一点都相同，这样的问题在高中物理考试里面属于送分题。但如果不是静止地拉绳子，而是用手挥动绳子呢（下图右）?这时在绳子上产生了一列机械波，每一点的受力情况都不相同，而且受力的变化不仅取决于手挥动绳子手的施力还取决于绳子的材质（决定了波长）。这时候分析受力就比较困难，属于高中物理竞赛级别的题目。 毫米波电路设计也会遇到类似的挑战。我们可以把电路中的导线类比成绳子，而把电路中的信号源类比为对绳施力的人。当信号变化的频率很慢的时候，就近似地等于静力分析，此时导线上每一点的信号都近似地等于信号源的信号。当信号变化很快时，由于信号的波长接近或小于导线的长度，我们必须仔细考虑导线上每一点的情况，而且导线的性质（特征阻抗）会极大地影响信号的传播。这种效应在电磁学中被称为“传输线效应”，在设计毫米波芯片时必须仔细考虑传输线效应才能确保芯片正常工作。传输线效应引入了许多传统电路设计中不用考虑的问题。例如，传输线有自己的特征阻抗，如果电路的输入阻抗和传输线的特征阻抗差别很大就会造成信号反射，使得信号无法有效地从一个模块传递到另一个模块。为了避免这种情况，必须在电路输入端做阻抗匹配来消除信号反射。另外，为了分析传输线效应，电路仿真时连线必须使用传输线模型。一方面，连线的传输线模型提取很费时间（一根简单的连线使用电磁仿真工具HFSS提取s-参数传输线模型往往需要一天以上的时间），另一方面传输线模型和晶体管电路联合仿真也很耗时而且需要有经验的人去调整仿真器参数才能保证结果正确。这就使得毫米波芯片的设计流程困难重重，需要大量的人力物力投入。 不过，尽管设计充满挑战，毫米波芯片大规模商用化目前已现曙光。Broadcom已经推出了60GHz的收发机芯片（BCM20138），该产品主要针对60GHz频段的WiFi标准(802.11.ad)，也可以看作是为5G毫米波芯片解决方案投石问路。Qualcomm也于两年前不甘落后收购了专注于毫米波技术的Wilocity。同时，三星，华为海思等重量级选手也在加紧研发毫米波芯片。相信在近期我们就会看到毫米波射频芯片市场变得热闹非凡。 Wilocity推出的60GHz芯片 结语 毫米波技术可以通过提升频谱带宽来实现超高速无线数据传播，从而成为5G通讯技术中的关键之一。毫米波芯片设计必须克服功耗和电磁设计两大难关，当这两个问题解决后大规模商用只是时间问题。]]></content>
      <categories>
        <category>5G</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo next主题解决无法显示Latex数学公式------系列第六篇]]></title>
    <url>%2F2019%2F04%2F26%2Fblog6%2F</url>
    <content type="text"><![CDATA[hexo next主题解决无法显示数学公式在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。 原因Hexo 默认使用 hexo-renderer-marked 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线_代表斜体，会被渲染引擎处理为&lt;em&gt;标签。 因为类 Latex 格式书写的数学公式下划线_表示下标，有特殊的含义，如果被强制转换为&lt;em&gt;标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。 类似的语义冲突的符号还包括*, {, }, \\等。 解决方法更换 Hexo 的 markdown 渲染引擎，hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为 hexo-renderer-kramed 引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的 escape 变量的值做相应的修改： 12//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 12//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。 在 Next 主题中开启 MathJax 开关如果使用了主题，别忘了在主题（Theme）中开启 MathJax 开关，下面以 next 主题为例，介绍下如何打开 MathJax 开关。 进入到主题目录，找到 _config.yml 配置问题，把 math 默认的 false 修改为true，具体如下： 1234567891011# Math Equations Render Supportmath: enable: true # Default(true) will load mathjax/katex script on demand # That is it only render those page who has &apos;mathjax: true&apos; in Front Matter. # If you set it to false, it will load mathjax/katex srcipt EVERY PAGE. per_page: true engine: mathjax #engine: katex 还需要在文章的Front-matter里打开mathjax开关，如下： 123456---title: index.htmldate: 2018-07-05 12:01:30tags:mathjax: true-- 之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决node.js - npm ERR! network getaddrinfo ENOTFOUND]]></title>
    <url>%2F2019%2F04%2F26%2Fp1%2F</url>
    <content type="text"><![CDATA[原因最近在想用npm下载一些modules发现总是会报如下错误 1npERR! network getaddrinfo ENOTFOUND 在网上查了很久资料也没有解决 尝试过但无效的办法1.使用淘宝镜像源 1npm config set registry https://registry.npm.taobao.org 仍报相同的错 2.命令行指定 1npm --registry https://registry.npm.taobao.org info underscore 也无效 解决办法因为发现报错中总是提示proxy代理有问题，但实际上我并没有在用proxy，所以执行以下2条命令 12npm config delete proxynpm config delete https-proxy 以上2条命令就是删除你的proxy设置，因为你本来就没有在用代理]]></content>
      <categories>
        <category>node-js</category>
      </categories>
      <tags>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural network and deep learning chapter 2]]></title>
    <url>%2F2019%2F04%2F18%2FCHAPTER2%2F</url>
    <content type="text"><![CDATA[CHAPTER 2How the backpropagation algorithm works上一章中我们知道了神经网络是如何通过梯度下降算法来学习相应的Weights和biases.但我们却没有解释梯度到底是怎么算的，这一章我们就介绍反向传播算法来计算梯度. 这一章会涉及比较多的算术运算，我们学习它主要是因为它可以让我们更加深入地了解weights和biases是如何改变整个网络的行为的。 Warm up: a fast matrix-based approach to computing the output from a neural network 我们有了以上几种表示方式后，我们就可以通过以下公式将它们联系起来 a_{j}^{l}=\sigma\left(\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\right)我们用矩阵的形式重写上述式子， 还有一点要注意的就是vectorization，我们对一个矩阵用某个函数，实际上是把这个函数作用在矩阵的每个元素上，现在，我们重写的式子如下 a^{l}=\sigma\left(w^{l} a^{l-1}+b^{l}\right) The two assumptions we need about the cost function我们用反向传播算法的目的其实是计算2个对于Cost的偏导数，对于网络中的任意w和b求导.我们还是以平方误差函数为例来讲解 C=\frac{1}{2 n} \sum_{x}\left\|y(x)-a^{L}(x)\right\|^{2}上式中，n是训练样本的总量，求和是针对所有训练样本求和，y(x)是相应样本的y，L为网络层数 现在我们来谈谈有关cost function的2个假设 第一个假设是cost function能被写为对于每一个训练样本的cost的和的平均，即 C=\frac{1}{n} \sum_{x} C_{x} 这个假设对于quadratic cost function成立，对于这本书之后会谈到的其他cost function也成立；那么我们为什么需要这个假设呢？原因很简单，因为反向传播算法只允许我们对单个训练样本求相应partial derivative,只有第一个假设满足我们才能对每一个样本求偏导后平均 第二个假设即cost能被写为是神经网络输出的一个函数 我们的quadratic cost显然满足，对于每一个训练样本 C=\frac{1}{2}\left\|y-a^{L}\right\|^{2}=\frac{1}{2} \sum_{j}\left(y_{j}-a_{j}^{L}\right)^{2} The Hadamard product, s⊙t我们用s⊙t表示2个相同维度向量的对应元素乘积 (s \odot t)_{j}=s_{j} t_{j}举个例子 \left[ \begin{array}{l}{1} \\ {2}\end{array}\right] \odot \left[ \begin{array}{l}{3} \\ {4}\end{array}\right]=\left[ \begin{array}{l}{1 * 3} \\ {2 * 4}\end{array}\right]=\left[ \begin{array}{l}{3} \\ {8}\end{array}\right]这个运算有时也被称为Hadamard product or Schur product。 The four fundamental equations behind backpropagation 要理解上文所说的error是如何被定义的，假设我们的神经网络中有一个恶魔 由上述故事启发，我们可以定义error为 \delta_{j}^{l} \equiv \frac{\partial C}{\partial z_{j}^{l}}接下来我们给出4个基本等式 An equation for the error in the output layer： \delta_{j}^{L}=\frac{\partial C}{\partial a_{j}^{L}} \sigma^{\prime}\left(z_{j}^{L}\right)这个等式稍微解释一下，如果C并不是很依靠某个输出神经元j,那么对应的误差会很小，这正是我们想要的，而右边的那个sigmoid函数的导数，衡量在相应点处activation function即sigmoid的变化快慢 这个式子的vectorization形式为： \delta^{L}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right) 对于quadratic function因为它的 \nabla_{a} C=\left(a^{L}-y\right)所以 \delta^{L}=\left(a^{L}-y\right) \odot \sigma^{\prime}\left(z^{L}\right) \delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right) \frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l} \frac{\partial C}{\partial b}=\delta \frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l}上述等式可以重写为索引更少的 \frac{\partial C}{\partial w}=a_{\mathrm{in}} \delta_{\mathrm{out}} 从上述等式中我们可以看出C对于w的偏导若a_in很小，则偏导也会很小，即w学的很慢 总结一下，我们已经明白了，weight将会学的很慢如果输入神经元是low_activation或是输出神经元饱和(收敛)，下面这张图即最重要的4个等式 Problem Proof of the four fundamental equations (optional)) 让我们首先证明等式1 \delta_{j}^{L}=\frac{\partial C}{\partial z_{j}^{L}}运用链式法则，我们可以把它和输出的activations结合起来 \delta_{j}^{L}=\sum_{k} \frac{\partial C}{\partial a_{k}^{L}} \frac{\partial a_{k}^{L}}{\partial z_{j}^{L}}这里有一点要注意，输出层的各个神经元是无关的，某一个输出神经元的activation只与它自己的z有关，这个别搞混了 \delta_{j}^{L}=\frac{\partial C}{\partial a_{j}^{L}} \frac{\partial a_{j}^{L}}{\partial z_{j}^{L}} \delta_{j}^{L}=\frac{\partial C}{\partial a_{j}^{L}} \sigma^{\prime}\left(z_{j}^{L}\right)由此等式1得证 2.接下来我们来证等式2 同样是用到链式法则 \begin{aligned} \delta_{j}^{l} &=\frac{\partial C}{\partial z_{j}^{l}} \\ &=\sum_{k} \frac{\partial C}{\partial z_{k}^{l+1}} \frac{\partial z_{k}^{l+1}}{\partial z_{j}^{l}} \\ &=\sum_{k} \frac{\partial z_{k}^{l+1}}{\partial z_{j}^{l}} \delta_{k}^{l+1} \end{aligned}对于上述最后一个式子的第一个因子，我们有： z_{k}^{l+1}=\sum_{i} w_{k j}^{l+1} a_{j}^{l}+b_{k}^{l+1}=\sum_{i} w_{k j}^{l+1} \sigma\left(z_{j}^{l}\right)+b_{k}^{l+1}Differentiating, we obtain： \frac{\partial z_{k}^{l+1}}{\partial z_{j}^{l}}=w_{k j}^{l+1} \sigma^{\prime}\left(z_{j}^{l}\right)再把上式代入，得： \delta_{j}^{l}=\sum_{k} w_{k j}^{l+1} \delta_{k}^{l+1} \sigma^{\prime}\left(z_{j}^{l}\right)这就是等式2的component form The backpropagation algorithm 通过算法描述你就可以知道为什么它叫反向传播，我们从后往前计算error 通常，将backpropagation与其他如gradient descent的学习算法结合起来是很常见的，举个例子 Of course, to implement stochastic gradient descent in practice you also need an outer loop generating mini-batches of training examples, and an outer loop stepping through multiple epochs of training. I’ve omitted those for simplicity. The code for backpropagation回忆上一章最后我们实施的算法，现在应当比较清楚了 1234567891011121314151617class Network(object):... def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The "mini_batch" is a list of tuples "(x, y)", and "eta" is the learning rate.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Network(object):... def backprop(self, x, y): """Return a tuple "(nabla_b, nabla_w)" representing the gradient for the cost function C_x. "nabla_b" and "nabla_w" are layer-by-layer lists of numpy arrays, similar to "self.biases" and "self.weights".""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w)... def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y) def sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) Problem Fully matrix-based approach to backpropagation over a mini-batch Our implementation of stochastic gradient descent loops over training examples in a mini-batch. It’s possible to modify the backpropagation algorithm so that it computes the gradients for all training examples in a mini-batch simultaneously. In what sense is backpropagation a fast algorithm?后向传播这个算法很快主要就是因为它只需一次foreword和一次backword就能算出所有的partial derivative Backpropagation: the big picture我们讲了这么多，但针对backpropagation,我们还有2个疑问，一是对于算法中的矩阵和向量乘积，它实际上到底是完成了什么？二是这个算法是如何被合理地推理出来地呢？ 先看问题一，让我们假设我们对w做了一个很小的改变 weight的改变会导致相应输出神经元activation的改变 更进一步，会导致下一层所有activations的改变 这些改变会不断传递下去，直到最后的输出层和cost function \Delta C \approx \frac{\partial C}{\partial w_{j k}^{l}} \Delta w_{j k}^{l} \Delta a_{j}^{l} \approx \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l} 事实上，它会造成以下改变 \Delta a_{q}^{l+1} \approx \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \Delta a_{j}^{l}将这个式子与上个式子联立 \Delta a_{q}^{l+1} \approx \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l} \Delta C \approx \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}} \cdots \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l} \Delta C \approx \sum_{m n p . . . q} \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}} \ldots \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}结合一开始的式子 \Delta C \approx \frac{\partial C}{\partial w_{j k}^{l}} \Delta w_{j k}^{l}我们可以得到 \frac{\partial C}{\partial w_{j k}^{l}}=\sum_{m n p . \ldots q} \frac{\partial C}{\partial a_{m}^{L}} \frac{\partial a_{m}^{L}}{\partial a_{n}^{L-1}} \frac{\partial a_{n}^{L-1}}{\partial a_{p}^{L-2}} \ldots \frac{\partial a_{q}^{l+1}}{\partial a_{j}^{l}} \frac{\partial a_{j}^{l}}{\partial w_{j k}^{l}}]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>neural networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural network and deep learning chapter 1]]></title>
    <url>%2F2019%2F04%2F15%2FCHAPTER1%2F</url>
    <content type="text"><![CDATA[CHAPTER 1Using neural nets to recognize handwritten digits Goals我们这一章的目的是只用74行纯代码来写出一个神经网络模型用来识别手写数字，通过这一章你将体会到神经网络与深度学习的魅力 Problems计算机要能识别数字并不像你我一样那么简单，比如你尝试想一下如何写一个程序让计算机来识别图片中的手写数字？我们可以换一个思路，神经网络是怎么处理的呢？它是通过输入大量的training examples,然后开发出一个能从那些training examples中学习的系统，即通过实例来自动参考和形成识别数字的规则 Why我们专注于handwriting recognition是因为它是一个很好的学习neural networks的典型问题，可以为以后深入deep learning和其他应用打下良好基础 让我们赶快开始吧 Perceptrons这是一种artificial neuron，虽然今天常用的是sigmoid neuron,但通过perceptrons我们才可以理解sigmoid neuron为什么这么定义。 perceptrons输入几个二进制数x1,x2,…，产生一个二进制输出 Rosenblatt提出了一种计算输出的方法。即引入权重w1,w2,…实数表示相应输入对输出的重要性。神经元输出0或1根据加权和是小于或大于一个threshold value.这个阈值也是一个实数，它是这个神经元的参数。数学表达式如下： output=\left\{\begin{array}{ll}{0} & {\text { if } \sum_{j} w_{j} x_{j} \leq \text { threshold }} \\ {1} & {\text { if } \sum_{j} w_{j} x_{j}>\text { threshold }}\end{array}\right. 这就是最基础的数学模型。你可以把神经元当作一个权衡各种依据来做决定的设备。通过调整weights和threshold value，我们可以得到不同的决策模型 通过增加神经元个数与层数，我们可以做出很复杂的决策模型 上图中第一层为input layer,最后一层为output layer,其余的均为hidden layer,可以想象，随着层数的增加，我们可以拟合出很复杂的模型，因为每一层都是基于上一层所作的更复杂的决定 我们可以简化一下perceptrons的描述方式: 上图中b=-threshold,只是做了一个vectorization和移项，b即bias，描述了神经元激发的难易程度，b越大，越容易激发 perceptrons可用来进行逻辑运算 可自行带入x1,x2计算得知为一个NAND gate.这里要注意一点，通过这个NAND gate我们可以通过增加层数来计算任意逻辑函数 我们可以用perceptrons来代替上述逻辑结构，把每个与非门转换为一个perceptrons,weights为-2，bias为3 注意到上图中最下一层的perceptrons得到了来自同一个perceptron的2个输入，weights均为-2，可简化以一个输入 目前为止我们还把x1,x2当作浮点型变量，实际上更常用的是输入层也换为perceptrons,但这个perceptron没有输入，单纯的输出它的activation值，即圆圈内的值 Sigmoid neurons为了引入sigmoid neurons,我们先想象一下我们有一个perceptrons neural network，输入为raw pixel data from a scanned, handwritten image of a digit，我们想让神经网络来学习weights和 biases来让神经网络的输出正确分类识别数字。为了看这个学习过程是怎么工作的，我们希望我们对weights(bias)做一个微小的改变，相应的输出也只有微小的改变。下图即为我们想要的效果 如果能达到这种效果，如果我们的网络误判9为8，我们就可以通过修改weights和bias来使得结果不断地一点点靠近9，不断的重复来获得更好的输出，那么就完成了学习过程 但实际上，因为perceptron的原因，我们对输入的微小改变可能导致输出flip,即从0变到1.这样的话，如果我们确实通过修改后可以识别9，我们这个网络对于其它所有的图像的表现可能都会有很大的改变，这是很不好的，这个问题可被sigmoid neuron解决 下面来看什么是sigmoid neuron 与perceptron相比其他的基本一样，差别在于以下几点： 输入输出值可取0~1内任何值比如0.638 输入不再是wx+b，而是 其中sigmoid function定义如下 \sigma(z) \equiv \frac{1}{1+e^{-z}}带入z后可以得到具体输出为 \frac{1}{1+\exp \left(-\sum_{j} w_{j} x_{j}-b\right)}神经元的结构还是一样 实际上，2者很相似。我们假设 若z≡w⋅x+b>>0,则e^{-z}\approx0，\sigma(z)\approx1 若z≡w⋅x+b]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>neural networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在本地创建repositories并上传至github]]></title>
    <url>%2F2019%2F04%2F05%2FCreateGithubRepositories%2F</url>
    <content type="text"><![CDATA[如何在github上创建新repositories 首先在github上创建一个repositories,记下其仓库地址 进入我们想要创建本地仓库的文件夹，打开git bash 输入git init，意思是在当前项目的目录中生成本地的git管理（会发现在当前目录下多了一个.git文件夹） 输入git add .，这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把.换成这个特定的文件名即可 输入git commit -m “first commit”，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要修该 输入git remote add origin https://自己的仓库url地址（上面有说到） 将本地的仓库关联到github上 输入git push -u origin master，这是把代码上传到github仓库的意思 执行完后，如果没有异常，会等待几秒，然后跳出一个让你输入Username和Password 的窗口，你只要输入github的登录账号和密码就行了 等待上传成功无报错即可 如何更新本地代码到github仓库 首先git checkout master切换到master分支 再git status判断是否切换成功 git add * —代表更新全部 接着输入git commit -m “更新说明”，commit只是提交到缓存区域 如果多人同时开发维护代码，得先git pull,拉取当前分支最新代码 最后git push origin master,最后一步才是push到远程master分支上 打开github界面就能看到同步了]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《算法4》java环境配置]]></title>
    <url>%2F2019%2F04%2F05%2FAlgorithms4_javaEnvironment%2F</url>
    <content type="text"><![CDATA[《算法4》java环境配置一、JDK下载 首先要下载java开发工具包JDK，下载地址：http://www.oracle.com/technetwork/java/javase/downloads/index.html 点击如下下载按钮： 在下载页面点击接受许可，并根据自己的系统选择对应的版本，本文以 Window 64位系统为例： 下载后成功后，双击图标即可安装，安装过程中直接下一步即可，也可修改安装目录，本例安装目录为：D:\tools\jdk1.8.0_77。 二、环境变量配置 1、安装完成后，右击”我的电脑”，点击”属性”，选择”高级系统设置”； 2、选择”高级”选项卡，点击”环境变量”； 然后就会出现如下图所示的画面： 在”系统变量”中设置3项属性，JAVA_HOME，PATH，CLASSPATH(大小写无所谓),若已存在则点击”编辑”，不存在则点击”新建”。 变量设置参数如下： 变量名：JAVA_HOME 变量值：D:\tools\jdk1.8.0_77 // 要根据自己的实际路径配置 变量名：CLASSPATH 变量值：.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar; //记得前面有个”.” 变量名：Path 变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin; 至此，系统环境变量配置成功。 三、测试JDK是否安装成功 1、”开始”-&gt;”运行”，键入”cmd”（或快捷键win+R）； 2、键入命令: java -version、java、javac 几个命令，出现以下信息，说明环境变量配置成功； 四、IDEA配置JDK 1、点击File —&gt;Project Structure； 2、点击左侧标签页SDKs选项，再点击左上角“+”，选择JDK； 3、在弹出框选择JDK安装路径，点击OK即可配置成功。 下图可以看到JDK已经在IDEA中配置好了。 五.配置算法4相关环境 最近下定决心，要把《算法》第四版认认真真刷一遍，IDE我用的是Intellij Idea。但是，刚一上手写书上的第一个代码BinarySearch.java，就出现问题了。因为要导入作者编写的algs4.jar包，老是出错。因为BinarySearch.java的运行牵涉到文件的重定向，在Eclipse中可以通过Configure来指定输入输出文件，但是在Idea中，我找了好久，也没找到类似指定输入输出文件的功能，最后我决定还是用Idea的Terminal来进行编译运行。 首先导入algs4.jar包：File--&gt;Project Structure--&gt;Modules--&gt;Dependencies--&gt;+ 然后正确编写BinarySearch.java代码： import edu.princeton.cs.algs4.In;import edu.princeton.cs.algs4.StdIn;import edu.princeton.cs.algs4.StdOut; import java.util.Arrays; public class BinarySearch { private BinarySearch() { } 1234567891011121314151617181920212223242526272829303132public static int indexOf(int[] a, int key) &#123; int lo = 0; int hi = a.length - 1; while (lo &lt;= hi) &#123; // Key is in a[lo..hi] or not present. int mid = lo + (hi - lo) / 2; if (key &lt; a[mid]) hi = mid - 1; else if (key &gt; a[mid]) lo = mid + 1; else return mid; &#125; return -1;&#125;public static int rank(int key, int[] a) &#123; return indexOf(a, key);&#125; public static void main(String[] args) &#123; // read the integers from a file In in = new In(args[0]); int[] whitelist = in.readAllInts(); // sort the array Arrays.sort(whitelist); // read integer key from standard input; print if not in whitelist while (!StdIn.isEmpty()) &#123; int key = StdIn.readInt(); if (BinarySearch.indexOf(whitelist, key) == -1) StdOut.println(key); &#125;&#125; } 注意：tinyT.txt和tinyW.txt两个文件也放在和BinarySearch.java的同一目录下 但是我在Idea的Terminal下输入javac BinarySearch.java进行编译，发现出错，并打印如下信息： Leisure:src Leisure$ javac BinarySearch.javaBinarySearch.java:1: 错误: 程序包edu.princeton.cs.algs4不存在import edu.princeton.cs.algs4.In; ^BinarySearch.java:2: 错误: 程序包edu.princeton.cs.algs4不存在import edu.princeton.cs.algs4.StdIn; ^BinarySearch.java:3: 错误: 程序包edu.princeton.cs.algs4不存在import edu.princeton.cs.algs4.StdOut; ^BinarySearch.java:35: 错误: 找不到符号 In in = new In(args[0]); ^ 符号: 类 In 位置: 类 BinarySearchBinarySearch.java:35: 错误: 找不到符号 In in = new In(args[0]); ^ 符号: 类 In 位置: 类 BinarySearchBinarySearch.java:42: 错误: 找不到符号 while (!StdIn.isEmpty()) { ^ 符号: 变量 StdIn 位置: 类 BinarySearchBinarySearch.java:43: 错误: 找不到符号 int key = StdIn.readInt(); ^ 符号: 变量 StdIn 位置: 类 BinarySearchBinarySearch.java:45: 错误: 找不到符号 StdOut.println(key); ^ 符号: 变量 StdOut 位置: 类 BinarySearch8 个错误 我疑惑了好久，因为我已经正确导入了Jar包，在Idea的Project下的External Libraries目录，也显示了algs4.jar文件，但为什么出现错误”找不到程序包edu.princeton.cs.algs4”这个错误呢？ 我查了官网资料，发现algs4.jar的存放位置不对，那么正确的姿势是什么样子的呢？请看下面截图 原来，我没有将algs4.jar放入正确的目录，我的电脑是Windows，此时将algs4.jar放入安装JDK时Java目录的Extensions文件下，此时再导入到Idea中，就可以按正常的语法编译运行代码了。 配置好之后再导入jar包一次，即可在cmd和idea内部Terminal正常按书上使用]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>java环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上传本地图片至你的博客---系列第五篇]]></title>
    <url>%2F2018%2F12%2F06%2Fblog5%2F</url>
    <content type="text"><![CDATA[简单的说明 有时候我们并不想上传网络上的图片，而是想把自己本地的图片上传到博客上，但发现按照markdown语法上传后无法显示，这时候我们只需要下载一个github上的插件即可，具体步骤如下： 1.cd到博客根目录下 查看_config.yml文件 查找 post_asset_folder 字段确定post_asset_folder 设置为true -&gt; post_asset_folder:true 2.当您设置 post_asset_folder 参数后，在建立文件时，Hexo 会自动建立一个与文章同名的文件夹，您可以把与该文章相关的所有资源都放到此文件夹内，这样就可以更方便的使用资源。 3.到博客的根目录下执行 123456npm install hexo-asset-image --save//这里有个大坑，有可能执行不成功，主要原因是国内访问外网，下面给一个解决措施//使用cnpm,通过淘宝镜像源下载$ npm install cnpm -g --registry=https://registry.npm.taobao.org //安装模块cnpm install hexo-asset-image --save //完成后改为用cnpm即可 4.然后创建一文章 hexo new “test” 然后查看博客的 ../source/_posts 目录下的文件，会看到存在一个test 文件夹 和 test.md 文件 ​ filePath.png 5.将所需要的图片资源放到test 文件夹 内 目录结构如下： ​ fileImagePath.png 6.书写文章使用test文件内 的图片 ​ imagePath.png 7. 12hexo cleanhexo g -d //部署完成即可看到效果]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转载）博客功能进阶配置:每篇文章阅读量显示---系列第四篇]]></title>
    <url>%2F2018%2F12%2F05%2Fblog4%2F</url>
    <content type="text"><![CDATA[以下内容转自我友链哥们zuzu的原创博文，欢迎大家去支持学习一波，我就是按照他的教程做的 为什么要添加每篇文章的计数？ 每篇文章加入阅读量显示可以让你知道读者更喜欢哪一方面的内容 通过什么来计数？ Leancloud的存储功能。 前言 什么是leancloud 领先的 BaaS 提供商,提供数据存储、文件存储、云引擎、容器、即时通讯、消息推送、移动统计、短信、游戏云等多项服务,为移动开发提供强有力的后端支持。 建议先阅读我的这一篇文章——给自己的博客添加极简风评论系统 好吧不需要知道它是什么 注册并创建应用 如图（我创建过的所以有请求记录） 点击 储存-创建Class 名字必须必须必须为Counter！！ 无限制写入 最后点击创建 对主题配置文件进行修改 点击设置-应用Key 记录appid与appkey 填写到主题配置文件的对应位置 记得将security项改为false！！ 1234567891011# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors:enable: trueapp_id: 填写 #&lt;app_id&gt;app_key: 填写 #&lt;app_key&gt;# Dependencies: https://github.com/theme-next/hexo-leancloud-counter-security# If you don&apos;t care about security in lc counter and just want to use it directly# (without hexo-leancloud-counter-security plugin), set the `security` to `false`.security: falsebetterPerformance: false 部署等待即可]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转载）博客功能进阶配置:给自己的博客添加极简风评论系统---系列第三篇]]></title>
    <url>%2F2018%2F12%2F05%2Fblog3%2F</url>
    <content type="text"><![CDATA[以下内容转自我友链哥们zuzu的原创博文，欢迎大家去支持学习一波，我就是按照他的教程做的 客不能只有单方向的输出，需要通过评论系统来提升沟通交流的空间，但是许多评论系统都过于繁琐，账号登陆，手机号验证等等操作让人评论的欲望大大下降，这里介绍一款极简的可以匿名也可以留下信息的评论系统– Valine – 一款基于 Leancloud 的极简风评论系统 Valine是什么 Valine 诞生于2017年8月7日，是一款基于Leancloud的快速、简洁且高效的无后端评论系统。 Valine比起其他系统的优点 快速 安全 Emoji 😉 无后端实现 MarkDown 全语法支持 轻量易用(~15kb gzipped) 搭建步骤注册账号 在 LeanCloud官网 进行账号注册。 创建应用并获取Leancloud Key 点击创建应用，取名任意，其余设置默认即可。 点击设置-应用key，可以看到 App ID 和 App Key 保留页面，待会有用。 修改主题配置文件 为了保险起见，推荐你先备份下 /themes/next/_config.yml。以防出现不备。 在/themes/next/_config.yml中查找关键词valine完善代码如下 1234567891011121314# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true # 功能开关 appid: **************** # 你的 appid appkey: *************** # 你的 appkey notify: false # 新评论的邮件通知 verify: false # 评论是否需要验证码 placeholder: 说点什么 # 评论框提示语 avatar: hide # 游客头像 guest_info: nick,mail,link # 游客信息 pageSize: 10 # 分页数量 visitor: false # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors&apos; for counter compatibility. Article reading statistic https://valine.js.org/visitor.html 添加安全域名 Leancloud -&gt; 设置 -&gt; 安全中心 -&gt; Web 安全域名 ，把你的域名加进去 网站部署 记得进行 hexo clean 效果图 参考文档 Valine – 一款极简的评论系统 Valine - 一款快速、简洁且高效的无后端评论系统。]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转载）DIY属于你自己的博客——更改主题与美化------系列第二篇]]></title>
    <url>%2F2018%2F12%2F05%2Fblog2%2F</url>
    <content type="text"><![CDATA[简单的说明 这里直接给大家推荐一个简约好用的主题，hexo原生简约主题NexT，是目前hexo搭建博客中使用人数最多的主题.且自由度很高，可以自己DIY独属于自己的博客样式，本博客也是采用Next主题 话不多说，开搞 以下内容转自我友链哥们zuzu的原创博文，欢迎大家去支持学习一波，我就是按照他的教程做的 前言 Hexo静态博客本地配置中有两个重要的配置文件，分别是 位于根目录下的站点配置文件：/_config.yml 位于根目录/theme/你的主题下的主题配置文件：/theme/你的主题/_config.yml 两者的关系为前者优先于后者 本文介绍Next主题的基础配置。 安装 hexo 默认主题为landscape，新主题需要我们自己下载 最简单的是通过git（也可以去官网下载源码） 1$ git clone https://github.com/theme-next/hexo-theme-next themes/next 切换主题 通过更改 1站点配置文件 进行主题选择。 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 语言选择 该主题默认语言为英语，按照需要进行语言切换 找到 \themes\next\languages 下支持的文件列表。（例如汉化的zh-CN） 再对 站点配置文件 进行修改，找到关键词language并在后面做修改并保存退出 12345678# Site 站点配置title: # 网站标题subtitle: # 副标题description: # 网站描述keywords: # 关键字author: # 你的昵称language: zh-CN # 网站语言timezone: # 时区 大部分的博客教程都是将language改为zh-hans，但是主题文件更新后该名称已经失效，望注意！ 主题模板更换 next主题提供了四种风格的主题模板，可以自由切换，至于四种模板的不同，请移步官方GitHub文档 在主题配置文件 中查找关键字Schemes 想要保留的模板前去掉 1# 即可。 12345# Schemes#scheme: Muse（默认主题）scheme: Mist#scheme: Pisces#scheme: Gemini 目录自选 在主题配置文件中查找关键字menu 寻找想要的目录并去掉前面的 1# 123456789menu: home: / || home #about: /about/ || user #tags: /tags/ || tags #categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 在每篇文章末尾添加 “本文结束” 在路径\themes\next\layout\_macro中新建 passage-end-tag.swig文件,并添加以下内容： 12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 接着打开\themes\next\layout\_macro\post.swig文件， 查找post-footer并在该 div 之前添加如下代码 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;passage-end-tag.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 在\theme\_config.yml末尾添加以下代码 123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 删除网页底部 “由hexo强力驱动、主题版本” 文字 打开 1\themes\next\layout\_partials\footer.swig 文件,找到并删除以下部分。 1234567891011121314151617&#123;% if theme.footer.powered.enable %&#125; &lt;div class=&quot;powered-by&quot;&gt;&#123;# #&#125;&#123;&#123; __(&apos;footer.powered&apos;, next_url(&apos;https://hexo.io&apos;, &apos;Hexo&apos;, &#123;class: &apos;theme-link&apos;&#125;)) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.powered.version %&#125; v&#123;&#123; hexo_env(&apos;version&apos;) &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if theme.footer.powered.enable and theme.footer.theme.enable %&#125; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.theme.enable %&#125; &lt;div class=&quot;theme-info&quot;&gt;&#123;# #&#125;&#123;&#123; __(&apos;footer.theme&apos;) &#125;&#125; – &#123;&#123; next_url(&apos;https://theme-next.org&apos;, &apos;NexT.&apos; + theme.scheme, &#123;class: &apos;theme-link&apos;&#125;) &#125;&#125;&#123;# #&#125;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125; 在网站底部加上访问量 打开\themes\next\layout\_partials\footer.swig文件,在第一行前加上以下代码 1&lt;script async src=&quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 在上一步删除的代码位置添加以下代码： 12345&lt;div class=&quot;powered-by&quot;&gt;&lt;i class=&quot;fa fa-user-md&quot;&gt;&lt;/i&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站访客数:&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; 新建标签页面 在根目录下进行 1$ hexo new page tags 对生成的md文件进行编辑,系统将自动识别为标签页面 1234title: 标签date: 2018-11-28 17:53:24type: &quot;tags&quot;--- 修改主题配置文件，添加tags至menu中 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags #categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false 1234title: 标签date: 2018-11-28 17:53:24type: &quot;tags&quot;comments: false 设置头像 编辑 1主题配置文件 ，修改字段 1avatar ，值设置成头像的链接地址。其中，头像的链接地址可以是： 完整的互联网URI: http://example.com/avatar.png 站点内的地址:放置在 1主题文件夹 的 source/images/ 目录下 配置为：avatar: /images/avatar.png 123456789101112# Sidebar Avatar 头像avatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: #/images/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: false # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false 设置网页标签页左侧标记 编辑主题配置文件，修改字段 favicon，文件放在主题文件夹的相对地址 123456789# For example, you put your favicons into `hexo-site/source/images` directory.# Then need to rename &amp; redefine they on any other names, otherwise icons from Next will rewrite your custom icons in Hexo.favicon: small: /images/favicon-16x16-next.png medium: /images/favicon-32x32-next.png apple_touch_icon: /images/apple-touch-icon-next.png safari_pinned_tab: /images/logo.svg #android_manifest: /images/manifest.json #ms_browserconfig: /images/browserconfig.xml 效果图 以上的步骤结束后需要进行部署 根目录下执行(第二步本地预览可选) 123$ hexo clean //消除大部分的缓存引起的更新未显示问题$ //hexo s$ hexo g -d //hexo generate + hexo deploy]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10 20分钟免费搭建你的博客---系列第一篇]]></title>
    <url>%2F2018%2F12%2F05%2Fblog1%2F</url>
    <content type="text"><![CDATA[简单的说明 系统：windows 10 64位，编辑器：vscode，控制台：cmder 搭建博客使用hexo+Github 什么是hexo?官方文档 Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 有了基本认识，废话不多说，开搞~ 一、配置Github首先注册、登录 https://github.com/ 记住自己的Username（很重要），你的Username就是你博客的域名了，取一个喜欢的哦 然后右上角选择 Create a new repository https://github.com/new Repository name （填自己的名字） yourname.github.io(yourname与你的注册用户名一致,这个就是你博客的域名了) 具体流程如下图 二、环境安装（node、git）1、安装 Node.js https://nodejs.org/en/ 2、安装 Git https://github.com/waylau/git-for-win Git教程 https://github.com/waylau/git-for-win廖雪峰老师的教程，感兴趣可以学习一下，这里我们不用看 3、安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，名称和邮箱是Github上的 4、安装 Hexo。所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 1$ npm install -g hexo-cli 好了到这一步我们环境全部安装好了。 三、设置在电脑F盘（自己随意）目录下新建文件夹blog，进入blog，右键Git bash here输入 1hexo init blog 稍微等待下，速度有点慢。成功提示 1INFO Start blogging with Hexo! 因为你初始化hexo 之后source目录下自带一篇hello world文章, 所以直接执行下方命令 123456$ hexo generate# 启动本地服务器$ hexo server# 在浏览器输入 http://localhost:4000/就可以看见网页和模板了INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 访问http://localhost:4000/，便可以看到网站初步的模样，不要激动，我们还要把网页发布到Github上去。 重新打开CMD，输入： 1ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; 一路Enter过来就好，得到信息： 1Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 找到该文件，打开（sublime text），Ctrl + a复制里面的所有内容，然后进入Sign in to GitHub：https://github.com/settings/ssh New SSH key ——Title：blog —— Key：输入刚才复制的—— Add SSH key 四、配置博客在blog目录下，用vscode打开_config.yml文件，修改参数信息 特别提醒，在每个参数的：后都要加一个空格 修改网站相关信息 123456title: dragon-liu测试所用博客 # 网站标题subtitle: # 副标题description: #网页描述，类似座右铭author: dragin-liu #即站长信息language: zh-CN #网站语言timezone: Asia/Shanghai #网站时区 配置部署（我的是zhihuya，修改成自己的） 1234deploy: type: git repo: https://github.com/zhihuya/zhihuya.github.io.git branch: master 五、发表文章在CMD中输入 12$ hexo new &quot;dragon-liu测试文章&quot;INFO Created: F:\test\blog\source\_posts\dragon-liu测试文章.md 找到该文章，打开，使用Markdown语法，该语法介绍可以查看(这里我们可以跳过)https://zhangslob.github.io/2017/02/26/%E5%88%A9%E7%94%A8HEXO%E6%90%AD%E5%BB%BA%E7%9A%84%E5%8D%9A%E5%AE%A2/ 123456---title: dragon-liu测试文章date: 2018-12-6 13:03:44tags:---这是一篇测试文章，欢迎关注作者博客[1]: https://dragon-liu.github.io/ 保存，然后执行下列步骤： 12345678910111213141516171819F:\test\blog$ hexo clean#以下是成功的结果显示INFO Deleted database.INFO Deleted public folder.F:\test\blog$ hexo generate#以下是成功的结果显示INFO Start processingINFO Files loaded in 1.48 s#省略INFO 29 files generated in 4.27 sF:\test\blog$ hexo server#以下是成功的结果显示INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 这个时候，打开http://localhost:4000/，发现刚才的文章已经成功了 最后一步，发布到网上，执行： 12345678F:\test\blog$ hexo deploy#若这条命令提示失败可改用这一条：npm install hexo-deployer-git --save#以下是成功的结果显示INFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...#省略 其中会跳出Github登录，直接登录，如果没有问题，这时输入zhihuya（换成你的）.github.io/ 然后就可以看到已经发布了 六、总结发布文章的步骤： 1、hexo new 创建文章 2、Markdown语法编辑文章 3、部署（所有打开CMD都是在blog目录下） 1234567hexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo generate #生成hexo server #启动服务预览，非必要，可本地浏览网页hexo deploy #部署发布#可以简化为2步hexo clean //用于消除缓存引起的问题hexo g -d //生成并部署发布 简写Tips： hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo p == hexo publish hexo g == hexo generate#生成 hexo s == hexo server #启动服务预览 hexo d == hexo deploy#部署 hexo new example(文档名) //即创建一篇新博客 下一篇将着重谈谈如何美化我们的博客]]></content>
      <categories>
        <category>个人博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Let's get started in machine leaning!]]></title>
    <url>%2F2018%2F12%2F05%2FML%2F</url>
    <content type="text"><![CDATA[​ 这篇文章是结合最近学习的一次东哥的live和csdn上相关文章的总结，算是我机器学习的入门吧 目录 背景知识 什么是机器学习 首先给一个总体概括： 无监督学习 迁移学习 深度学习 机器学习库对比 如何快速入门机器学习 好文推荐计算机潜意识]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电信好导师.答疑("OJ平台解释")]]></title>
    <url>%2F2018%2F11%2F29%2F%E7%94%B5%E4%BF%A1%E5%A5%BD%E5%AF%BC%E5%B8%88.%E7%AD%94%E7%96%91(OJ%E5%B9%B3%E5%8F%B0%E8%A7%A3%E9%87%8A)%2F</url>
    <content type="text"><![CDATA[大家以前如果没有接触过OJ平台，对他的输入输出可能就会有一些懵逼，这次答疑就解释一下OJ平台的运作原理。 首先看一下一套例题 “A+B” 的答案 123456789#include&lt;stdio.h&gt;int main()&#123; int a, b; while(scanf("%d%d",&amp;a,&amp;b)!=EOF)&#123; printf("%d\n", a+b); &#125; return 0;&#125; 1.EOF是啥EOF—&gt;End of File;是文件结束符，和’a’,’0’,’\n’,’\t’一样，都是char型的字符，可以在ASCII码表里面找到它。在键盘中用Ctrl+Z的方法来键入这个字符。作用是什么呢？首先大家来新建一个文本文档（后缀为.txt），然后打开它，输入123，回车，456，保存，关闭。那么这个文本文档的内容是什么呢？ 答案是：”123\n456”吗？，实际上不对。排除Windows记事本自己的小九九的话，在456这三个字符之后还会有一个EOF字符（同理在123之前也会有一个文件开始字符）。 2.scanf的返回值简单来讲，scanf的返回值有如下两种情况 正确按指定格式输入变量的个数；也即能正确接收到值的变量个数。 读取到文件结束字符时，返回-1。 那我的代码里面为什么会有一个scanf(“%d%d”,&amp;a,&amp;b)!=EOF，实际上，因为读取文件结束字符是很常见的，所以为了方便理解，c语言的宏定义里面把 EOF 定义为 -1。综上，在可以用scanf(…)!=EOF的方法来判断是否已经读取完整个文件 3.为什么要使用scanf(…)!=EOF实际上，在OJ平台上，自动把输入输出重定向了。跟着如下操作来理解一下输入输出重定向吧。 把上述代码编译链接成可执行文件.exe。我这里为了便于理解，把exe文件重命名为了A+B.exe。接下了来，在桌面新建一个名为test的文件夹。把exe文件复制进去。再新建一个input.txt的文件，里面写下如下内容 12341 12 25 658 90 好了，现在我们这个文件夹里面已经有了两个文件，一个是处理A+B的可执行程序，一个是A+B的输入值。那么如何让A+B.exe拿到input里面的内容呢？ 按下快捷键 Win+R（Win就是键盘上Windows图标的那个键）。在打开的文本框里面输入cmd并回车。可以看到已经打开了控制台。 在控制台中依次输入以下命令12cd desktopcd test 这样控制台的运行环境就进入到了test文件夹目录下。输入命令dir可以看到当前文件夹下面的文件，这里能看到exe文件和我们的txt文件。 接下来运行如下命令，注意看文件夹内部文件的变化。1A+B &lt; input.txt &gt; output.txt 可以惊奇的发现文件夹中多出了 output.txt 的文件。打开output.txt可以看到如下内容1234247098 好了，这就完成了一次输入输出重定向。（也可以在C语言源代码中完成这一步，这里我是借助了win的&lt; &gt;这两个命令符）。 接下来解释一下A+B &lt; input.txt &gt; output.txt的含义 A+B：我们可执行程序的名字，注意没有后缀.exe &lt;：用于输入重定向，把&lt;后面的文件重定向为stdin >：用于输出重定向，把&gt;后面的文件重定向为stdout 4.解释stdin和stdout默认情况下，c语言所有的scanf会从stdin中读取数据，printf会把所有打印数据输出到stdout。你们以前写的代码运行的时候，在黑窗窗里面输入的数据都会导入stdin，黑窗窗自动显示的文本全部来源于stdout。这样就有了1.键盘输入—&gt;stdin文件—&gt;scanf读取—&gt;给int、double等等数据赋值2.printf生成输出字符串—&gt;输出字符串进入stdin—&gt;打印到黑窗窗 5.回到A+B &lt; input.txt &gt; output.txt使用&lt; 和 &gt; 之后，4中的流程变成了1.键盘输入—&gt;input.txt文件—&gt;scanf读取—&gt;给int、double等等数据赋值2.printf生成输出字符串—&gt;输出字符串进入output.txt（不存在则新建该文件）—&gt;打印到黑窗窗（这一步就没有了，因为output.txt并不是stdout）。 现在就可以明白，这个程序运行的流程为 从input中scanf到1,1并赋值给a和b，scanf返回值为2不等于EOF，进入循环体内。 输出a+b的值，这里为2，和一个换行符到output.txt中。 从input中scanf到2,2并赋值给a和b，scanf返回值为2不等于EOF，进入循环体内。 输出a+b的值，这里为4，和一个换行符到output.txt中。 从input中scanf到5,65并赋值给a和b，scanf返回值为2不等于EOF，进入循环体内。 输出a+b的值，这里为70，和一个换行符到output.txt中。 从input中scanf到8,90并赋值给a和b，scanf返回值为2不等于EOF，进入循环体内。 输出a+b的值，这里为98，和一个换行符到output.txt中。 从input中scanf到文件结束字符，scanf返回值为-1等于EOF，跳出循环体。 main函数结束。 6.回到OJ平台解释OJ相当于完成了上述步骤。提交代码之后，会编译你的代码，并完成输入输出重定向。代码跑完之后就生成了output文件。把output文件和标准答案文件对比，如果完全符合，就accept。若不符合，就wrong answer。所以不要在OJ上printf与题目无关的内容！！！ 7.智超组的任务在周六晚上之前AC掉这几个题目。（AC就是accepted，也即通过）。]]></content>
      <categories>
        <category>好导师答疑</category>
      </categories>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rapidjson 使用]]></title>
    <url>%2F2018%2F07%2F21%2FRapidjson%20%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Rapidjson 使用git主页 中文文档 导入工程rapidjson是只依赖于头文件的库，没有动态库、静态库的导入过程。 将头文件加入工程 方法一：将include复制到系统头文件地址，与系统include文件夹合并 方法二：将include复制添加进入工程文件，通常还需设定头文件搜索目录 代码引用头文件 1#include "rapidjson/&lt;对应模块&gt;" 基本使用Document对象1234567891011121314151617// Document作为一个JSON的可操作对象// 1. 将JSON Cstr解析至Documentconst char* json = "&#123;\"project\": \"rapidjson&#125;"Document d;d.Parse(json);// 2. 修改Document对象的键值Value&amp; s = d["stars"];s.SetInt(s.GetInt() + 1);// 3. 将Document转换成stringStringBuffer buffer;Writer&lt;StringBuffer&gt; writer(buffer);d.Accept(writer);// 输出 &#123;"project":"rapidjson","stars":11&#125;std::cout &lt;&lt; buffer.GetString() &lt;&lt; std::endl; Document对象的分析 Writer对象12345678910111213141516171819202122232425262728// 主要作用是将数据分步写入StringBufferStringBuffer s;Writer&lt;StringBuffer&gt; writer(s);writer.StartObject(); // Between StartObject()/EndObject(), writer.Key("hello"); // output a key,writer.String("world"); // follow by a value.writer.Key("t");writer.Bool(true);writer.Key("f");writer.Bool(false);writer.Key("n");writer.Null();writer.Key("i");writer.Uint(123);writer.Key("pi");writer.Double(3.1416);writer.Key("a");writer.StartArray(); // Between StartArray()/EndArray(),for (unsigned i = 0; i &lt; 4; i++) writer.Uint(i); // all values are elements of the array.writer.EndArray();writer.EndObject();// &#123;"hello":"world","t":true,"f":false,"n":null,"i":123,"pi":3.1416,"a":[0,1,2,3]&#125;cout &lt;&lt; s.GetString() &lt;&lt; endl; Writer 成员函数列表 Reader对象 和 数据Validation分别见/example中的schemavalidator和simplewritter]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深信服组质量收集模块代码总结]]></title>
    <url>%2F2018%2F07%2F16%2FQC%2F</url>
    <content type="text"><![CDATA[Quality collection这是暑假在深信服项目组时摸🐟写的一段代码，很惭愧也很后悔，白白浪费了锻炼的机会，但还算有点输出，就贴出来纪念一下。 主要就是判断QualityCollection.json这个文件是否存在，若不存在则创建，存在则更新。先用map读取json文件，再将json数据写入map,最后是根据传入类型修改map相应数据，将修改好的数据再次写入json保存，具体看代码吧，注释写的很详细 注意这段代码调用了rapidjson库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187//// Created by lzc on 18-7-08..//#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;#include &lt;map&gt;#include &lt;cstdio&gt;#include &lt;cstdlib&gt;#include "Quality.hpp"#include "log/Log.hpp"#include "vender/rapidjson/include/rapidjson/document.h"#include "vender/rapidjson/include/rapidjson/prettywriter.h"#include "vender/rapidjson/include/rapidjson/filewritestream.h"#include "vender/rapidjson/include/rapidjson/memorybuffer.h"using namespace std;using namespace cb;Quality::Quality()&#123;&#125;Quality::~Quality()&#123;&#125;void Quality::StatisticsWithCollectionType(CollectionType collection)&#123; ifstream json; json.open("/sdcard/QualityCollection.json", ios::in); //判断json文件是否已存在 if (!json.is_open()) &#123; writeFileJson("/sdcard/QualityCollection.json"); //json文件不存在则初始化json文件 &#125; else &#123; json.close(); //若已存在json文件则关闭 &#125; //用来读取json文件的map Map quality_collection; //将json中数据写入map Jsontomap("/sdcard/QualityCollection.json", quality_collection); //根据传入类型修改map相应数据 (quality_collection[qualitystr[collection]])++; //将修改好的数据再次写入json保存 maptoJson("/sdcard/QualityCollection.json", quality_collection);&#125;//将信息保存为JSON格式void Quality:: writeFileJson(const char* file)&#123; //生成Json串 rapidjson::Document jsonDoc; //生成一个dom元素Document rapidjson::Document::AllocatorType &amp;allocator = jsonDoc.GetAllocator(); //获取分配器 jsonDoc.SetObject(); //将当前的Document设置为一个object，也就是说，整个Document是一个Object类型的dom元素 //创建模板json文件 jsonDoc.AddMember("CollectionTypeLogin", 0, allocator); jsonDoc.AddMember("CollectionTypeUserDelete", 0, allocator); jsonDoc.AddMember("CollectionTypeSimpleReply", 0, allocator); jsonDoc.AddMember("CollectionTypeReply", 0, allocator); jsonDoc.AddMember("CollectionTypeRecall", 0, allocator); jsonDoc.AddMember("CollectionTypeFetchMessage", 0, allocator); jsonDoc.AddMember("CollectionTypeCrash", 0, allocator); jsonDoc.AddMember("CollectionTypeDatabaseCorruption", 0, allocator); jsonDoc.AddMember("CollectionTypeDirtyData", 0, allocator); //输出到文件 rapidjson::StringBuffer buffer; rapidjson::PrettyWriter&lt;rapidjson::StringBuffer&gt; pretty_writer(buffer); //PrettyWriter是格式化的json，如果是Writer则是换行空格压缩后的json jsonDoc.Accept(pretty_writer); ofstream fout; fout.open(file); if ( !fout.is_open() ) //判断新建收集json文件是否失败 &#123; SE_LOG_ERROR(kQualityModule, "%s", "open file failed."); &#125; fout&lt;&lt;buffer.GetString(); fout.close(); //检验是否创建成功 ifstream os; os.open(file, ios::in); if (!os.is_open()) //判断新建收集json文件是否失败 &#123; SE_LOG_ERROR(kQualityModule, "%s", "create collection_file failed."); &#125; else &#123; SE_LOG_INFO(kQualityModule, "%s", "create collection_file successful."); os.close(); &#125; /* //json文件内容如下： &#123; "CollectionTypeLogin":0, allocator, 登录 "CollectionTypeUserDelete": 0, allocator, 删除用户 "CollectionTypeSimpleReply": 0, allocator, 快捷回复 "CollectionTypeReply": 0, allocator, 回复 "CollectionTypeRecall": 0, allocator, 撤销 "CollectionTypeFetchMessage": 0, allocator , 获取邮件 "CollectionTypeCrash": 0, allocator, 程序崩溃 "CollectionTypeDatabaseCorruption": 0, allocator, 数据库损坏 "CollectionTypeDirtyData": 0, allocator 数据库脏数据 &#125; */&#125;void Quality::Jsontomap(const char* file, Map&amp;quality_collection)&#123; ifstream fin; fin.open(file, ios::in); if (!fin.is_open()) //判断新建收集json文件是否失败 &#123; SE_LOG_ERROR(kQualityModule, "%s", "open file failed."); &#125; string str; string str_in=""; while(getline(fin,str)) //一行一行地读到字符串str_in中 &#123; str_in=str_in+str+'\n'; &#125; //解析并打印出来 rapidjson::Document doc; doc.Parse&lt;0&gt;(str_in.c_str()); //判断读取成功与否 if (doc.HasParseError()) &#123; SE_LOG_ERROR(kQualityModule, "%s", "Parse file failed."); return; &#125; //遍历Json文件写入map for (auto&amp;m: doc.GetObject()) &#123; //逐个提取数组元素（声明的变量必须为引用) quality_collection[m.name.GetString()] = m.value.GetInt(); &#125;&#125;void Quality::maptoJson(const char* file, Map&amp; quality_collection)&#123; rapidjson::Document document; rapidjson::Document::AllocatorType&amp; allocator = document.GetAllocator(); rapidjson::Value root(rapidjson::kObjectType); rapidjson::Value key(rapidjson::kStringType); map&lt;string, int&gt;::const_iterator it; for (it = quality_collection.begin(); it != quality_collection.end(); ++it) // 注意这里要用const_iterator &#123; key.SetString(it-&gt;first.c_str(), allocator); root.AddMember(key, it-&gt;second, allocator); &#125; //输出到文件 rapidjson::StringBuffer buffer; rapidjson::PrettyWriter&lt;rapidjson::StringBuffer&gt; pretty_writer(buffer); //PrettyWriter是格式化的json root.Accept(pretty_writer); ofstream fout; fout.open(file); if (!fout.is_open()) //判断新建收集json文件是否失败 &#123; SE_LOG_ERROR(kQualityModule, "%s", "open file failed."); &#125; fout &lt;&lt; buffer.GetString(); fout.close(); //检验是否创建成功 ifstream os; os.open(file, ios::in); if (!os.is_open()) //新建收集json文件失败 &#123; SE_LOG_ERROR(kQualityModule, "%s", "create collection_file failed."); &#125; else &#123; SE_LOG_INFO(kQualityModule, "%s", "create collection_file successful."); os.close(); &#125;&#125;]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10 archlinux Vmware虚拟机安装]]></title>
    <url>%2F2018%2F05%2F29%2Farch%20Linux%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. 首先就是我一开始在纠结怎么截图1ctrl+alt+a //可启用qq截图 2. 正式创建虚拟机 注意几个选择把，首先是选择自定义配置 选择稍后安装操作系统 客户机操作系统选择Linux，版本为其他Linux 4.x内核x86 之后一路下一步默认选项即可 选中新建的虚拟机，右键弹出菜单，点击“设置’—”硬件“—-CD/DVD（IDE） ，在又侧，选“使用ISO镜像文件”，将下载好的Linux镜像文件载入进来 3.正式安装开始 首先运行虚拟机，选择最上方x86 之后开始使用命令行安装 首先检查是否已联网，这很重要，因为arch要下载很多东西 1# Ping -c 4 www.baidu.com //检查并没有问题 如果ping的结果是找不到该主机，可能是由于dhcp服务没有开启，使用 1# systemctl dhcpcd.service 确保联网后进行下一步 4. 测试系统时间1# timedatectl status 检测发现时间不对于是输入 1# timedatectl set-ntp true 开启ntp服务，它会每隔11分钟进行一次网络对时。然后在查看一下系统时间状态，正常 5. 测试存储设备1# lsblk 查看存储设备状态。在sda节点下（准确的说是sdx，有几个设备就有几个sdx）看看有没有sda1这样的，每一个带数字的都是该物理硬盘分出来的区。依据刚才的设置，这里只有sda一个20G的硬盘。 6. 磁盘分区1# cfdisk 这里就是我踩过的最大一个坑，因为这里的失误，导致后面grub总是安装不了。 关键在于，不管你怎么分区，GPT，无论用 BIOS 还是 UEFI。开头都分一个小的分区，最小 1M，我给的是 100M，类型为 21686148-6449-6E6F-744E-656564454649，fdisk 中为 04 BIOS boot。而且最后一个分区不要用完磁盘，末尾 1M 空出不要用，我空 100M。开头的空间是bios保留分区，防止只认mbr的程序把gpt写坏的最末尾的空间是用来备份分区表的 安装 GRUB 前，在一个没有文件系统的磁盘上，用 fdisk 或 gdisk 创建一个 +1M 分区，设置为 BIOS boot 类型，在 fdisk 中的类型号是 4, 在 gdisk 中的类型是 ef02，在 parted 中是 bios_grub。此分区可以在磁盘前 2TB 的任何位置。 上述就是症结所在，引用wiki中的说明 1# lsblk //检查发现节点已创建 我这里是创建了sda2,sda3,sda4。其中sda2为home，sda3为扩展分区，sda4为交换分区 7. 格式化分区对于交换分区，格式化命令 1# mkswap /dev/sda3 开启使用交换分区 1# swapon /dev/sda3 格式化根分区和家目录分区 12mkfs.ext4 /dev/sda1mkfs.ext4 /dev/sda2 8. 挂载各个分区注意一点，最开始分的sda1与sda5不用进行任何操作，甚至不用格式化，不需要挂载，这是留着以后用的 使用命令 1# mount /dev/sda1 /mnt 把根分区挂载在/mnt目录下再使用命令 1# mkdir /mnt/home 在/mnt目录下创建home目录，注意，如果你分了多个分区需要挂载boot，usr，opt目录等，均要在挂载完根目录后在/mnt目录下创建各个分区的目录。再进行挂载。 使用命令1# mount /dev/sda2 /mnt/home 挂载家目录 9.安装系统挂载好分区之后，就可以安装arch了。第一件事情是先修改一下arch的镜像源，使用国内的镜像速度更快，亲测更快！！！。 修改镜像源需要编辑/etc/pacman.d/mirrorlist文件。可以先用nano查看一下该文件，查看完成之后使用Ctrl+X退出。 1# nano /etc/pacman.d/mirrorlist 利用正则表达式工具grep工具来将中国的源取出来。首先，先切换到软件源所在目录。 1# cd /etc/pacman.d 然后用grep取出中国的源。用-A参数的会用–分隔符分隔每一条匹配，因此再次使用grep工具去掉前一次结果产生的分隔符。 1# grep -A 1 &apos;##.*China&apos; mirrorlist|grep -v &apos;\-\-&apos;&gt; mirrorlist2 最后将中国的源放到镜像源列表的最前面。这里先反过来，将原来的镜像源追加到中国的源后面，然后将追加完成之后的文件替换掉原来的镜像源。 12# cat mirrorlist&gt;&gt;mirrorlist2# mv mirrorlist2 mirrorlist 镜像源修改完成之后需要刷新一下pacman的缓存。 1# pacman -Syy 修改完镜像源，开始安装基本系统了。pacman会从网络上下载最新的软件包开始安装。-i参数是在安装前进行确认，出现确认提示的话直接全选即可（一路enter）。 1# pacstrap -i /mnt base base-devel 配置新系统安装完成系统之后需要生成fstab文件。生成之后需要查看一下是否生成成功。如果不成功需要重新生成。 12# genfstab -U -p /mnt &gt;&gt; /mnt/etc/fstab# nano /mnt/etc/fstab 进入新系统然后就可以进入新系统进行配置了。先进入新系统。1# arch-chroot /mnt /bin/bash 进入新系统之后会发现命令提示符也发生了相应的变化。 区域和时间设置区域设置需要设置两个文件：locale.gen和locale.conf文件。 先使用nano打开/etc/locale.gen文件，然后取消en_US.UTF-8、zh_CN.UTF-8、zh_TW.UTF-8三行的注释。然后运行locale-gen命令生成locale信息。12# nano /etc/locale.gen# locale-gen 然后生成一个locale.conf文件。ps:这里最好使用英文，使用别的语言可能会导致终端乱码。(亲自踩坑，一把辛酸泪)1# echo LANG=en_US.UTF-8 &gt; /etc/locale.conf 首先先选择时区：1# tzselect 先按4选择Asia，在按9选择China，再按1选择北京时间，按1选择yes。然后将时区链接到自己的时区。1# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 将时间标准设置为UTC，有的虚拟机有这项设置则不用设置（例如VirtualBox就有设置硬件事件为UTC的选项，lj vmware不行~）1# hwclock --systohc --utc 设置root密码和创建新用户使用不带参数的passwd可以设置root密码。这里我还没搞明白，顺序问题应该是，我设置不了密码，无法输入，目前系统装好了，再试试 1# passwd 安装启动加载器先安装grub 1# pacman -S grub 10.安装完成后，再使用1# grub-install --recheck /dev/sda 注意这里的分区不需要指定分区数字，不要写成/dev/sda1这样的。 踩得大坑。。。主要是分区要搞好 如果没有错误提示的话，就说明安装成功。应该会显示Is=installation finished，成功的话然后自动生成一个配置文件，默认的配置文件可以适应大部分情况。 1# grub-mkconfig -o /boot/grub/grub.cfg 配置网络并在 /etc/hosts 添加同样的主机名：12345678910111213141516# nano /etc/hosts## /etc/hosts: static lookup table for host names##&lt;ip-address&gt; &lt;hostname.domain.org&gt; &lt;hostname&gt;127.0.0.1 localhost.localdomain localhost 主机名 ::1 localhost.localdomain localhost 主机名# End of file 启动有线网服务如果虚拟机重启没有网的话需要让它开机自启动。1# systemctl enable dhcpcd.service 11.退出并重启系统123# exit# umount -R /mnt# reboot 目前到了这一步，正在装新东西顺带一提，一开始登陆时默认输入root登陆，不知道为什么应该贴一张图，但断网了。。。用户名是draongliu很霸气有木有！ 12.图形界面安装下面开始安装图形界面 首先是装Xorg1# pacman -S xorg-server xorg-xinit 显卡驱动下面安装显卡驱动1# pacman -S xf86-video-vesa GNOME桌面gnome桌面只要安装gnome包即可，还有一个gnome-extra包可以提供额外的常用软件和几个游戏1# pacman -S gnome gnome-extra 然后安装gdm登录管理器1# pacman -S gnome gdm 将gdm设置为开机自启动，这样开机时会自动载入桌面1# systemctl enable gdm 目前是新建了用户，但是打不开Terminal，图形界面装的gnome。。。又踩到坑了，我忘记在reboot前安装中文字体了，现在进不了terminal。。。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>
